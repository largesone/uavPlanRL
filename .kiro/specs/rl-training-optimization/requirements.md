# 强化学习训练流程优化需求文档

## 介绍

本项目旨在优化无人机任务分配项目的强化学习智能体训练流程。当前系统使用 `GraphRLSolver` 配合 `ZeroShotGNN` 策略网络，在复杂场景中存在动作空间过大、智能体频繁执行无效动作（Action=0）导致训练停滞的问题。本优化方案通过实现动作空间剪枝、增强奖励塑形和提前终止机制来提升探索效率，防止策略停滞，并提供更有效的奖励引导。

## 需求

### 需求 1：动作空间剪枝优化

**用户故事：** 作为强化学习训练系统，我希望能够智能地剪枝动作空间，以便在复杂场景中减少无效动作的探索，提高训练效率。

#### 验收标准

1. WHEN 环境生成动作掩码时 THEN 系统 SHALL 基于距离实现动作空间剪枝
2. WHEN 计算有效动作时 THEN 系统 SHALL 只考虑每个目标距离最近的K架无人机（K可配置，默认为5）
3. WHEN 剪枝动作空间时 THEN 系统 SHALL 保持原有的 `_is_valid_action` 和 `_has_actual_contribution` 检查逻辑
4. WHEN 无人机没有所需资源时 THEN 系统 SHALL 排除该无人机与目标的动作组合
5. WHEN 生成最终动作掩码时 THEN 系统 SHALL 确保只有经过距离筛选和有效性验证的动作被标记为可用

### 需求 2：奖励塑形增强

**用户故事：** 作为强化学习智能体，我希望能够获得更细粒度的奖励反馈，以便更好地学习接近目标的行为，即使在无法立即完成任务的情况下也能获得正向引导。

#### 验收标准

1. WHEN 无人机执行动作后 THEN 系统 SHALL 计算与目标之间缩短的距离
2. WHEN 无人机拥有目标所需资源且向目标移动时 THEN 系统 SHALL 给予基于距离缩短的正奖励
3. WHEN 计算接近奖励时 THEN 系统 SHALL 使用公式：奖励 = 0.001 * 缩短的距离
4. WHEN 无人机不拥有目标所需资源时 THEN 系统 SHALL NOT 给予接近奖励
5. WHEN 生成奖励分解时 THEN 系统 SHALL 将接近奖励以 '接近奖励' 为键添加到breakdown字典中
6. WHEN 计算总奖励时 THEN 系统 SHALL 将接近奖励计入layer2_total的总和

### 需求 3：训练停滞检测与提前终止

**用户故事：** 作为训练监控系统，我希望能够检测到智能体的策略停滞情况，以便及时终止无效的训练轮次，避免浪费计算资源。

#### 验收标准

1. WHEN 开始新的训练轮次时 THEN 系统 SHALL 初始化连续零动作计数器
2. WHEN 智能体选择动作为0时 THEN 系统 SHALL 增加连续零动作计数器
3. WHEN 智能体选择非零动作时 THEN 系统 SHALL 重置连续零动作计数器为0
4. WHEN 连续零动作计数达到15次时 THEN 系统 SHALL 设置truncated=True并终止当前轮次
5. WHEN 提前终止轮次时 THEN 系统 SHALL 打印包含轮次编号的警告日志
6. WHEN 提前终止后 THEN 系统 SHALL 继续进行下一轮训练而不是完全停止训练过程

### 需求 4：系统兼容性保持

**用户故事：** 作为系统维护者，我希望优化后的系统能够保持与现有代码的兼容性，以便不影响其他功能模块的正常运行。

#### 验收标准

1. WHEN 实现优化功能时 THEN 系统 SHALL 保持现有API接口不变
2. WHEN 修改核心函数时 THEN 系统 SHALL 确保函数签名和返回值格式保持兼容
3. WHEN 添加新参数时 THEN 系统 SHALL 提供合理的默认值以保持向后兼容
4. WHEN 优化实现后 THEN 系统 SHALL 确保现有的测试用例仍能通过
5. WHEN 系统运行时 THEN 系统 SHALL 保持原有的日志格式和错误处理机制