# 需求文档

## 介绍

本功能旨在通过引入基于势函数（Potential Function）的奖励塑形（Potential-Based Reward Shaping, PBRS）来改进现有的强化学习奖励机制。核心思想是在当前奖励函数基础上，进行一次精细化的、理论上安全的增强。通过定义场景的"势函数"来衡量状态的"优劣"，并将势函数的"变化"作为额外的塑形奖励，既能提供稠密的过程引导信号以加速学习，又能从理论上保证不会改变问题的最优策略，从而完美地避免"投机取巧"和"优先级模糊"的问题。

## 需求

### 需求 1

**用户故事：** 作为强化学习研究者，我希望能够定义一个势函数来衡量当前状态的"优劣"，以便为智能体提供更好的学习引导信号。

#### 验收标准

1. WHEN 环境初始化时 THEN 系统 SHALL 创建一个势函数计算方法 `_calculate_potential()`
2. WHEN 调用势函数时 THEN 系统 SHALL 基于当前状态返回一个标量值，表示状态的"好坏"程度
3. WHEN 状态越接近"所有任务完成"时 THEN 势函数值 SHALL 越高
4. WHEN 计算势函数时 THEN 系统 SHALL 确保函数是平滑且连续的，避免奖励的"悬崖"效应

### 需求 2

**用户故事：** 作为强化学习系统，我希望势函数能够综合考虑任务完成度和空间效率，以便准确反映状态质量。

#### 验收标准

1. WHEN 计算势函数时 THEN 系统 SHALL 考虑所有目标的剩余资源量，剩余越少势能越高
2. WHEN 计算势函数时 THEN 系统 SHALL 考虑所有无人机到最近未完成目标的距离，距离越近势能越高
3. WHEN 目标完全完成时 THEN 该目标 SHALL 不再影响距离势能的计算
4. WHEN 所有目标都完成时 THEN 势函数 SHALL 达到最大值

### 需求 3

**用户故事：** 作为强化学习训练过程，我希望使用PBRS理论来重构奖励函数，以便在不改变最优策略的前提下提供更好的学习信号。

#### 验收标准

1. WHEN 执行动作前 THEN 系统 SHALL 记录当前状态的势能值 `potential_before`
2. WHEN 执行动作后 THEN 系统 SHALL 计算新状态的势能值 `potential_after`
3. WHEN 计算塑形奖励时 THEN 系统 SHALL 使用公式 `shaping_reward = gamma * potential_after - potential_before`
4. WHEN 计算总奖励时 THEN 系统 SHALL 将塑形奖励与基础奖励相加

### 需求 4

**用户故事：** 作为强化学习系统，我希望基础奖励变得更加稀疏但更有意义，以便让塑形奖励发挥主要的引导作用。

#### 验收标准

1. WHEN 所有目标未完成时 THEN 基础奖励 SHALL 为0
2. WHEN 所有目标完成且episode结束时 THEN 基础奖励 SHALL 为100.0
3. WHEN 执行无效动作时 THEN 系统 SHALL 给予-5.0的惩罚，覆盖所有其他奖励
4. WHEN 动作对任务没有贡献时 THEN 系统 SHALL 给予-5.0的惩罚，覆盖所有其他奖励

### 需求 5

**用户故事：** 作为系统维护者，我希望新的奖励机制能够与现有系统兼容，以便平滑地集成到现有的训练流程中。

#### 验收标准

1. WHEN 实现新奖励机制时 THEN 系统 SHALL 保持现有的step()方法接口不变
2. WHEN 使用新奖励机制时 THEN 系统 SHALL 能够与现有的观测空间和动作空间兼容
3. WHEN 启用势函数奖励时 THEN 系统 SHALL 提供配置选项来启用或禁用该功能
4. WHEN 调试时 THEN 系统 SHALL 能够记录和输出势函数值的变化过程

### 需求 6

**用户故事：** 作为研究者，我希望能够监控和分析势函数的行为，以便验证其有效性和调试潜在问题。

#### 验收标准

1. WHEN 计算势函数时 THEN 系统 SHALL 记录完成度势能和距离势能的分别贡献
2. WHEN 执行step时 THEN 系统 SHALL 在info字典中包含势函数相关的调试信息
3. WHEN 启用详细日志时 THEN 系统 SHALL 输出势函数值的变化和各组成部分
4. WHEN 训练过程中 THEN 系统 SHALL 能够追踪势函数值的统计分布