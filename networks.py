# -*- coding: utf-8 -*-
# æ–‡ä»¶å: networks.py
# æè¿°: ç»Ÿä¸€çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…å«æ‰€æœ‰ç½‘ç»œç»“æ„å®šä¹‰

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Optional, Dict, Any
import math

class LowRankSelfAttention(nn.Module):
    """
    ç®€åŒ–çš„ä½ç§©è¿‘ä¼¼è‡ªæ³¨æ„åŠ›æœºåˆ¶
    
    æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨çº¿æ€§æŠ•å½±å‡å°‘æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦
    """
    
    def __init__(self, d_model: int, low_rank_dim: int, nhead: int, dropout: float = 0.1):
        super(LowRankSelfAttention, self).__init__()
        
        self.d_model = d_model
        self.low_rank_dim = low_rank_dim
        self.nhead = nhead
        
        # ç®€åŒ–çš„çº¿æ€§æŠ•å½±
        self.query_proj = nn.Linear(d_model, low_rank_dim, bias=False)
        self.key_proj = nn.Linear(d_model, low_rank_dim, bias=False)
        self.value_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = 1.0 / math.sqrt(low_rank_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.query_proj, self.key_proj, self.value_proj, self.out_proj]:
            nn.init.xavier_uniform_(module.weight)
    
    def forward(self, x, mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
            mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len, seq_len]
        
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.size()
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = x.device
        x = x.to(device)
        
        # 1. è®¡ç®—Q, K, V
        q = self.query_proj(x)  # [batch_size, seq_len, low_rank_dim]
        k = self.key_proj(x)    # [batch_size, seq_len, low_rank_dim]
        v = self.value_proj(x)  # [batch_size, seq_len, d_model]
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆåœ¨ä½ç»´ç©ºé—´ï¼‰
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [batch_size, seq_len, seq_len]
        
        # 3. åº”ç”¨æ©ç 
        if mask is not None:
            # ç¡®ä¿æ©ç åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            mask = mask.to(device)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 5. åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attn_output = torch.matmul(attn_weights, v)  # [batch_size, seq_len, d_model]
        
        # 6. è¾“å‡ºæŠ•å½±
        output = self.out_proj(attn_output)
        
        return output


class LowRankCrossAttention(nn.Module):
    """
    ç®€åŒ–çš„ä½ç§©è¿‘ä¼¼äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
    
    ç”¨äºUAV-ç›®æ ‡é—´çš„äº¤äº’ï¼Œé€šè¿‡ä½ç§©æŠ•å½±å‡å°‘è®¡ç®—å¤æ‚åº¦
    """
    
    def __init__(self, d_model: int, low_rank_dim: int, nhead: int, dropout: float = 0.1):
        super(LowRankCrossAttention, self).__init__()
        
        self.d_model = d_model
        self.low_rank_dim = low_rank_dim
        self.nhead = nhead
        
        # ç®€åŒ–çš„çº¿æ€§æŠ•å½±
        self.query_proj = nn.Linear(d_model, low_rank_dim, bias=False)
        self.key_proj = nn.Linear(d_model, low_rank_dim, bias=False)
        self.value_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = 1.0 / math.sqrt(low_rank_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.query_proj, self.key_proj, self.value_proj, self.out_proj]:
            nn.init.xavier_uniform_(module.weight)
    
    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len, d_model]
            memory: è®°å¿†åºåˆ— [batch_size, memory_len, d_model]
            tgt_mask: ç›®æ ‡æ©ç 
            memory_mask: è®°å¿†æ©ç 
        
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ [batch_size, tgt_len, d_model]
        """
        batch_size, tgt_len, d_model = tgt.size()
        memory_len = memory.size(1)
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = tgt.device
        tgt = tgt.to(device)
        memory = memory.to(device)
        
        # 1. è®¡ç®—Q, K, V
        q = self.query_proj(tgt)      # [batch_size, tgt_len, low_rank_dim]
        k = self.key_proj(memory)     # [batch_size, memory_len, low_rank_dim]
        v = self.value_proj(memory)   # [batch_size, memory_len, d_model]
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [batch_size, tgt_len, memory_len]
        
        # 3. åº”ç”¨æ©ç 
        if memory_mask is not None:
            # ç¡®ä¿æ©ç åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            memory_mask = memory_mask.to(device)
            scores = scores.masked_fill(memory_mask == 0, -1e9)
        
        # 4. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 5. åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attn_output = torch.matmul(attn_weights, v)  # [batch_size, tgt_len, d_model]
        
        # 6. è¾“å‡ºæŠ•å½±
        output = self.out_proj(attn_output)
        
        return output


class SimpleNetwork(nn.Module):
    """ç®€åŒ–çš„ç½‘ç»œç»“æ„ - åŸºç¡€ç‰ˆæœ¬"""
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, dropout: float = 0.1):
        super(SimpleNetwork, self).__init__()
        
        layers = []
        current_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.BatchNorm1d(hidden_dim)
            ])
            current_dim = hidden_dim
        
        layers.extend([
            nn.Linear(current_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(64),
            nn.Linear(64, output_dim)
        ])
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.3)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.network(x)

class DeepFCN(nn.Module):
    """æ·±åº¦å…¨è¿æ¥ç½‘ç»œ"""
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, dropout: float = 0.1):
        super(DeepFCN, self).__init__()
        
        layers = []
        current_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.BatchNorm1d(hidden_dim)
            ])
            current_dim = hidden_dim
        
        layers.extend([
            nn.Linear(current_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(64),
            nn.Linear(64, output_dim)
        ])
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.3)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.network(x)

# GATç½‘ç»œå·²ç§»é™¤ - ä½¿ç”¨ZeroShotGNNæ›¿ä»£

class DeepFCNResidual(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„æ·±åº¦å…¨è¿æ¥ç½‘ç»œ - ä¼˜åŒ–ç‰ˆæœ¬"""
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, dropout: float = 0.2):
        super(DeepFCNResidual, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims if hidden_dims else [256, 128, 64]  # é»˜è®¤å±‚æ¬¡ç»“æ„
        self.output_dim = output_dim
        self.dropout = dropout
        
        # è¾“å…¥å±‚ - æ·»åŠ BatchNorm
        self.input_layer = nn.Sequential(
            nn.Linear(input_dim, self.hidden_dims[0]),
            nn.BatchNorm1d(self.hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5)  # è¾“å…¥å±‚ä½¿ç”¨è¾ƒå°çš„dropout
        )
        
        # æ®‹å·®å—
        self.residual_blocks = nn.ModuleList()
        for i in range(len(self.hidden_dims) - 1):
            block = ResidualBlock(self.hidden_dims[i], self.hidden_dims[i+1], dropout)
            self.residual_blocks.append(block)
        
        # æ³¨æ„åŠ›æœºåˆ¶ - ç®€åŒ–ç‰ˆæœ¬
        self.attention = nn.Sequential(
            nn.Linear(self.hidden_dims[-1], self.hidden_dims[-1] // 4),
            nn.ReLU(),
            nn.Linear(self.hidden_dims[-1] // 4, self.hidden_dims[-1]),
            nn.Sigmoid()
        )
        
        # è¾“å‡ºå±‚ - ä¼˜åŒ–ç»“æ„
        self.output_layer = nn.Sequential(
            nn.Linear(self.hidden_dims[-1], 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(64, output_dim)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Heåˆå§‹åŒ–ï¼Œé€‚åˆReLUæ¿€æ´»å‡½æ•°
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)  # å°çš„æ­£åç½®
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶"""
        # è¾“å…¥å±‚
        x = self.input_layer(x)
        
        # æ®‹å·®å—
        for block in self.residual_blocks:
            x = block(x)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(x)
        x = x * attention_weights
        
        # è¾“å‡ºå±‚
        return self.output_layer(x)

class ResidualBlock(nn.Module):
    """ä¼˜åŒ–çš„æ®‹å·®å— - æ”¹è¿›çš„ç»“æ„å’Œæ­£åˆ™åŒ–"""
    def __init__(self, in_dim: int, out_dim: int, dropout: float = 0.2):
        super(ResidualBlock, self).__init__()
        
        # ä¸»è·¯å¾„ - ä½¿ç”¨é¢„æ¿€æ´»ç»“æ„
        self.layers = nn.Sequential(
            nn.BatchNorm1d(in_dim),
            nn.ReLU(),
            nn.Linear(in_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(out_dim, out_dim),
        )
        
        # è·³è·ƒè¿æ¥
        if in_dim != out_dim:
            self.shortcut = nn.Sequential(
                nn.Linear(in_dim, out_dim),
                nn.BatchNorm1d(out_dim)
            )
        else:
            self.shortcut = nn.Identity()
        
        # æ·»åŠ SEæ³¨æ„åŠ›æ¨¡å—
        self.se_attention = SEBlock(out_dim, reduction=4)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - é¢„æ¿€æ´»æ®‹å·®è¿æ¥"""
        residual = self.shortcut(x)
        out = self.layers(x)
        
        # åº”ç”¨SEæ³¨æ„åŠ›
        out = self.se_attention(out)
        
        # æ®‹å·®è¿æ¥
        return out + residual

class SEBlock(nn.Module):
    """Squeeze-and-Excitationæ³¨æ„åŠ›å—"""
    def __init__(self, channels: int, reduction: int = 16):
        super(SEBlock, self).__init__()
        self.squeeze = nn.AdaptiveAvgPool1d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # å…¨å±€å¹³å‡æ± åŒ–
        b, c = x.size()
        y = x.mean(dim=0, keepdim=True)  # ç®€åŒ–çš„å…¨å±€æ± åŒ–
        
        # æ¿€åŠ±æ“ä½œ
        y = self.excitation(y)
        
        # é‡æ–°åŠ æƒ
        return x * y.expand_as(x)

def create_network(network_type: str, input_dim: int, hidden_dims: List[int], output_dim: int, config=None) -> nn.Module:
    """
    åˆ›å»ºæŒ‡å®šç±»å‹çš„ç½‘ç»œ
    
    Args:
        network_type: ç½‘ç»œç±»å‹ ("SimpleNetwork", "DeepFCN", "GAT", "DeepFCNResidual", "ZeroShotGNN")
        input_dim: è¾“å…¥ç»´åº¦
        hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
        output_dim: è¾“å‡ºç»´åº¦
        
    Returns:
        ç½‘ç»œæ¨¡å‹å®ä¾‹
    """
    if network_type == "SimpleNetwork":
        return SimpleNetwork(input_dim, hidden_dims, output_dim)
    elif network_type == "DeepFCN":
        return DeepFCN(input_dim, hidden_dims, output_dim)
    # GATç½‘ç»œå·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ZeroShotGNN
    elif network_type == "DeepFCNResidual":
        return DeepFCNResidual(input_dim, hidden_dims, output_dim)
    elif network_type == "ZeroShotGNN":
        return ZeroShotGNN(input_dim, hidden_dims, output_dim, config=config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç½‘ç»œç±»å‹: {network_type}")

class ZeroShotGNN(nn.Module):
    """
    çœŸæ­£çš„é›¶æ ·æœ¬å›¾ç¥ç»ç½‘ç»œ - åŸºäºTransformerçš„æ¶æ„
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å‚æ•°å…±äº«çš„å®ä½“ç¼–ç å™¨ï¼Œæ”¯æŒå¯å˜æ•°é‡çš„UAVå’Œç›®æ ‡
    2. è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åŒç±»å®ä½“é—´çš„å†…éƒ¨å…³ç³»
    3. äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ UAV-ç›®æ ‡é—´çš„äº¤äº’å…³ç³»
    4. æ”¯æŒæ©ç æœºåˆ¶ï¼Œå¿½ç•¥å¡«å……çš„æ— æ•ˆæ•°æ®
    5. é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œé€‚åº”ä¸åŒè§„æ¨¡çš„åœºæ™¯
    """
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, dropout: float = 0.1, config=None):
        super(ZeroShotGNN, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims if hidden_dims else [256, 128]
        self.output_dim = output_dim
        self.dropout = dropout
        self.config = config  # ä¿å­˜configå¼•ç”¨
        
        # åµŒå…¥ç»´åº¦
        self.embedding_dim = 128
        
        # === 1. å‚æ•°å…±äº«çš„å®ä½“ç¼–ç å™¨ ===
        # UAVç‰¹å¾ç»´åº¦ï¼šposition(2) + heading(1) + resources_ratio(2) + max_distance_norm(1) + 
        #              velocity_norm(2) + is_alive(1) + is_idle(1) = 10
        # æ–°å¢ï¼šis_idleç‰¹å¾ï¼Œæ˜ç¡®æ ‡è¯†UAVæ˜¯å¦å¤„äºç©ºé—²çŠ¶æ€]
        # æ–°å¢ï¼šscarcity_metric_res1(1) + scarcity_metric_res2(1) = 2
        self.uav_feature_dim = 12
        
        # ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼šposition(2) + resources_ratio(2) + value_norm(1) + 
        #              remaining_ratio(2) + is_visible(1) = 8
        self.target_feature_dim = 8
        
        # UAVç¼–ç å™¨
        self.uav_encoder = nn.Sequential(
            nn.Linear(self.uav_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, self.embedding_dim),
            nn.LayerNorm(self.embedding_dim)
        )
        
        # ç›®æ ‡ç¼–ç å™¨
        self.target_encoder = nn.Sequential(
            nn.Linear(self.target_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, self.embedding_dim),
            nn.LayerNorm(self.embedding_dim)
        )
        
        # === 2. æ³¨æ„åŠ›å±‚ï¼ˆæ”¯æŒæ–°æ—§æ¨¡å‹å…¼å®¹ï¼‰ ===
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä½ç§©è¿‘ä¼¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰æˆ–æ ‡å‡†Transformerï¼ˆæ—§ç‰ˆæœ¬ï¼‰
        self.use_low_rank_attention = getattr(config, 'USE_LOW_RANK_ATTENTION', True) if config else True
        
        # å¼ºåˆ¶å¯ç”¨ä½ç§©è¿‘ä¼¼æ³¨æ„åŠ›ä»¥æµ‹è¯•æ€§èƒ½
        if config and hasattr(config, 'FORCE_LOW_RANK_ATTENTION') and config.FORCE_LOW_RANK_ATTENTION:
            self.use_low_rank_attention = True
        
        if self.use_low_rank_attention:
            # ä»configè·å–æ³¨æ„åŠ›å¤´æ•°
            nhead = getattr(config, 'num_heads', 8) if config else 8
            
            # UAVå†…éƒ¨è‡ªæ³¨æ„åŠ› - ä½¿ç”¨ä½ç§©è¿‘ä¼¼
            self.uav_self_attention = LowRankSelfAttention(
                d_model=self.embedding_dim,
                low_rank_dim=32,  # ä½ç§©ç»´åº¦ï¼Œè¿œå°äºembedding_dim
                nhead=nhead,
                dropout=dropout
            )
            
            # ç›®æ ‡å†…éƒ¨è‡ªæ³¨æ„åŠ› - ä½¿ç”¨ä½ç§©è¿‘ä¼¼
            self.target_self_attention = LowRankSelfAttention(
                d_model=self.embedding_dim,
                low_rank_dim=32,
                nhead=nhead,
                dropout=dropout
            )
            
            # UAV-ç›®æ ‡äº¤å‰æ³¨æ„åŠ› - ä½¿ç”¨ä½ç§©è¿‘ä¼¼
            self.cross_attention = LowRankCrossAttention(
                d_model=self.embedding_dim,
                low_rank_dim=32,
                nhead=nhead,
                dropout=dropout
            )
        else:
            # ä»configè·å–æ³¨æ„åŠ›å¤´æ•°
            nhead = getattr(config, 'num_heads', 8) if config else 8
            
            # æ ‡å‡†Transformerå±‚ï¼ˆå…¼å®¹æ—§æ¨¡å‹ï¼‰
            self.uav_self_attention = nn.TransformerEncoderLayer(
                d_model=self.embedding_dim,
                nhead=nhead,
                dim_feedforward=256,
                dropout=dropout,
                activation='relu',
                batch_first=True
            )
            
            self.target_self_attention = nn.TransformerEncoderLayer(
                d_model=self.embedding_dim,
                nhead=nhead,
                dim_feedforward=256,
                dropout=dropout,
                activation='relu',
                batch_first=True
            )
            
            self.cross_attention = nn.TransformerDecoderLayer(
                d_model=self.embedding_dim,
                nhead=nhead,
                dim_feedforward=256,
                dropout=dropout,
                activation='relu',
                batch_first=True
            )
        
        # === 4. ä½ç½®ç¼–ç  ===
        self.position_encoder = PositionalEncoding(self.embedding_dim, dropout)
        
        # === 5. Qå€¼è§£ç å™¨ ===
        # ä¸ºæ¯ä¸ªUAV-ç›®æ ‡å¯¹è¾“å‡ºæ‰€æœ‰å¯èƒ½æ¥è¿‘è§’åº¦çš„Qå€¼
        # è¾“å‡ºç»´åº¦ä¸ºconfig.GRAPH_N_PHIï¼ˆè§’åº¦æ•°é‡Pï¼‰
        n_phi = getattr(config, 'GRAPH_N_PHI', 6) if hasattr(config, 'GRAPH_N_PHI') else 6
        self.n_phi = n_phi
        
        self.q_decoder = nn.Sequential(
            nn.Linear(self.embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, n_phi)  # æ¯ä¸ªUAV-ç›®æ ‡å¯¹è¾“å‡ºn_phiä¸ªQå€¼
        )
        
        # === 6. ç©ºé—´ç¼–ç å™¨ ===
        # é¢„å…ˆå®šä¹‰ç©ºé—´ç¼–ç å™¨ï¼Œé¿å…åŠ¨æ€åˆ›å»ºå¯¼è‡´çš„state_dictä¸åŒ¹é…
        self.spatial_encoder = nn.Sequential(
            nn.Linear(3, 32),  # ç›¸å¯¹ä½ç½®(2) + è·ç¦»(1)
            nn.ReLU(),
            nn.Linear(32, self.embedding_dim // 4)
        )
        
        # === 7. å…¨å±€èšåˆå±‚ ===
        # å°†æ‰€æœ‰UAVçš„è¡¨ç¤ºèšåˆä¸ºæœ€ç»ˆçš„åŠ¨ä½œQå€¼
        self.global_aggregator = nn.Sequential(
            nn.Linear(self.embedding_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        self._init_weights()
        self._register_gradient_hooks()
    
    def load_state_dict(self, state_dict, strict=True):
        """
        é‡å†™load_state_dictæ–¹æ³•ï¼Œæ”¯æŒæ–°æ—§æ¨¡å‹å…¼å®¹
        """
        # æ£€æµ‹æ¨¡å‹ç‰ˆæœ¬
        has_old_attention = any('self_attn.in_proj_weight' in key for key in state_dict.keys())
        
        if has_old_attention:
            # æ—§æ¨¡å‹ï¼Œæ ¹æ®configå†³å®šæ˜¯å¦ä½¿ç”¨ä½ç§©è¿‘ä¼¼
            if getattr(self.config, 'USE_LOW_RANK_ATTENTION', False):
                print("ğŸ” æ£€æµ‹åˆ°æ—§æ¨¡å‹æ ¼å¼ï¼Œä½†æ ¹æ®é…ç½®ä½¿ç”¨ä½ç§©è¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶")
                self.use_low_rank_attention = True
                # é‡æ–°åˆå§‹åŒ–ä½ç§©æ³¨æ„åŠ›å±‚
                self._init_low_rank_attention_layers()
            else:
                print("ğŸ” æ£€æµ‹åˆ°æ—§æ¨¡å‹æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†Transformeræ³¨æ„åŠ›æœºåˆ¶")
                self.use_low_rank_attention = False
                # é‡æ–°åˆå§‹åŒ–æ ‡å‡†æ³¨æ„åŠ›å±‚
                self._init_old_attention_layers()
        else:
            # æ–°æ¨¡å‹ï¼Œä½¿ç”¨ä½ç§©è¿‘ä¼¼
            print("ğŸ” æ£€æµ‹åˆ°æ–°æ¨¡å‹æ ¼å¼ï¼Œä½¿ç”¨ä½ç§©è¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶")
            self.use_low_rank_attention = True
        
        # è°ƒç”¨çˆ¶ç±»çš„load_state_dictï¼Œå¿½ç•¥ä¸åŒ¹é…çš„æƒé‡
        try:
            return super().load_state_dict(state_dict, strict=False)
        except Exception as e:
            print(f"âš ï¸ æƒé‡åŠ è½½éƒ¨åˆ†å¤±è´¥ï¼Œä½†ç»§ç»­ä½¿ç”¨æ–°æ¶æ„: {e}")
            # å³ä½¿æƒé‡åŠ è½½å¤±è´¥ï¼Œä¹Ÿç»§ç»­ä½¿ç”¨æ–°çš„ä½ç§©æ³¨æ„åŠ›æ¶æ„
            return None
    
    def _init_old_attention_layers(self):
        """åˆå§‹åŒ–æ—§ç‰ˆæœ¬çš„æ³¨æ„åŠ›å±‚"""
        # ä»configè·å–æ³¨æ„åŠ›å¤´æ•°
        nhead = getattr(self.config, 'num_heads', 8) if hasattr(self, 'config') and self.config else 8
        
        self.uav_self_attention = nn.TransformerEncoderLayer(
            d_model=self.embedding_dim,
            nhead=nhead,
            dim_feedforward=256,
            dropout=self.dropout,
            activation='relu',
            batch_first=True
        )
        
        self.target_self_attention = nn.TransformerEncoderLayer(
            d_model=self.embedding_dim,
            nhead=nhead,
            dim_feedforward=256,
            dropout=self.dropout,
            activation='relu',
            batch_first=True
        )
        
        self.cross_attention = nn.TransformerDecoderLayer(
            d_model=self.embedding_dim,
            nhead=nhead,
            dim_feedforward=256,
            dropout=self.dropout,
            activation='relu',
            batch_first=True
        )
    
    def _init_low_rank_attention_layers(self):
        """åˆå§‹åŒ–ä½ç§©è¿‘ä¼¼æ³¨æ„åŠ›å±‚"""
        # ä»configè·å–æ³¨æ„åŠ›å¤´æ•°
        nhead = getattr(self.config, 'num_heads', 8) if hasattr(self, 'config') and self.config else 8
        
        self.uav_self_attention = LowRankSelfAttention(
            d_model=self.embedding_dim,
            low_rank_dim=32,
            nhead=nhead,
            dropout=self.dropout
        )
        
        self.target_self_attention = LowRankSelfAttention(
            d_model=self.embedding_dim,
            low_rank_dim=32,
            nhead=nhead,
            dropout=self.dropout
        )
        
        self.cross_attention = LowRankCrossAttention(
            d_model=self.embedding_dim,
            low_rank_dim=32,
            nhead=nhead,
            dropout=self.dropout
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ - æ•°å€¼ç¨³å®šç‰ˆæœ¬"""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨æ›´ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight, gain=0.1)  # è¿›ä¸€æ­¥å‡å°gain
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1.0)
                nn.init.constant_(module.bias, 0.0)
            elif isinstance(module, nn.MultiheadAttention):
                # ç‰¹åˆ«å¤„ç†æ³¨æ„åŠ›å±‚çš„æƒé‡
                for param_name, param in module.named_parameters():
                    if 'weight' in param_name:
                        nn.init.xavier_uniform_(param, gain=0.05)  # éå¸¸å°çš„gain
                    elif 'bias' in param_name:
                        nn.init.zeros_(param)
    
    def _register_gradient_hooks(self):
        """æ³¨å†Œæ¢¯åº¦è£å‰ªhook"""
        def gradient_hook(grad):
            # è£å‰ªæ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            if torch.isnan(grad).any() or torch.isinf(grad).any():
                return torch.zeros_like(grad)
            return torch.clamp(grad, -1.0, 1.0)
        
        for param in self.parameters():
            if param.requires_grad:
                param.register_hook(gradient_hook)
    
    def _create_attention_mask(self, padding_mask):
        """
        åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        
        Args:
            padding_mask: [batch_size, seq_len] å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºéœ€è¦å¿½ç•¥çš„ä½ç½®
        
        Returns:
            torch.Tensor: [batch_size, seq_len, seq_len] æ³¨æ„åŠ›æ©ç 
        """
        batch_size, seq_len = padding_mask.shape
        # ç¡®ä¿æ©ç åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = padding_mask.device
        # åˆ›å»º2Dæ©ç ï¼ŒTrueè¡¨ç¤ºéœ€è¦å¿½ç•¥çš„ä½ç½®
        mask = padding_mask.unsqueeze(1) | padding_mask.unsqueeze(2)  # [batch_size, seq_len, seq_len]
        return mask.to(device)
    
    def forward(self, graph_obs):
        """
        å‰å‘ä¼ æ’­ - å¤„ç†å›¾ç»“æ„è§‚æµ‹
        
        Args:
            graph_obs (dict): å›¾ç»“æ„è§‚æµ‹å­—å…¸ï¼ŒåŒ…å«ï¼š
                - uav_features: [batch_size, N_uav, uav_feature_dim]
                - target_features: [batch_size, N_target, target_feature_dim]
                - relative_positions: [batch_size, N_uav, N_target, 2]
                - distances: [batch_size, N_uav, N_target]
                - masks: æ©ç å­—å…¸
        
        Returns:
            torch.Tensor: Qå€¼ [batch_size, N_actions]
        """
        # æå–è¾“å…¥å¹¶ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
        uav_features = graph_obs["uav_features"]  # [batch_size, N_uav, uav_feat_dim]
        target_features = graph_obs["target_features"]  # [batch_size, N_target, target_feat_dim]
        relative_positions = graph_obs["relative_positions"]  # [batch_size, N_uav, N_target, 2]
        distances = graph_obs["distances"]  # [batch_size, N_uav, N_target]
        uav_mask = graph_obs["masks"]["uav_mask"]  # [batch_size, N_uav]
        target_mask = graph_obs["masks"]["target_mask"]  # [batch_size, N_target]
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = next(self.parameters()).device
        uav_features = uav_features.to(device)
        target_features = target_features.to(device)
        relative_positions = relative_positions.to(device)
        distances = distances.to(device)
        uav_mask = uav_mask.to(device)
        target_mask = target_mask.to(device)
        
        # å½¢çŠ¶æ£€æŸ¥å’Œä¿®å¤ - å¤„ç†å„ç§ç»´åº¦æƒ…å†µ
        try:
            # å¤„ç†uav_featuresçš„å„ç§å½¢çŠ¶
            if len(uav_features.shape) == 4:
                # å¦‚æœæ˜¯4ç»´ [1, 1, N, features]ï¼Œå‹ç¼©ä¸º3ç»´ [1, N, features]
                uav_features = uav_features.squeeze(1)
            elif len(uav_features.shape) == 2:
                # å¦‚æœæ˜¯2ç»´ï¼Œæ·»åŠ batchç»´åº¦
                uav_features = uav_features.unsqueeze(0)
            elif len(uav_features.shape) == 1:
                # å¦‚æœæ˜¯1ç»´ï¼Œéœ€è¦é‡å¡‘ä¸º[1, 1, features]
                uav_features = uav_features.unsqueeze(0).unsqueeze(0)
                
            # å¤„ç†target_featuresçš„å„ç§å½¢çŠ¶
            if len(target_features.shape) == 4:
                # å¦‚æœæ˜¯4ç»´ [1, 1, N, features]ï¼Œå‹ç¼©ä¸º3ç»´ [1, N, features]
                target_features = target_features.squeeze(1)
            elif len(target_features.shape) == 2:
                # å¦‚æœæ˜¯2ç»´ï¼Œæ·»åŠ batchç»´åº¦
                target_features = target_features.unsqueeze(0)
            elif len(target_features.shape) == 1:
                # å¦‚æœæ˜¯1ç»´ï¼Œéœ€è¦é‡å¡‘ä¸º[1, 1, features]
                target_features = target_features.unsqueeze(0).unsqueeze(0)
            
            # å¤„ç†å…¶ä»–å¼ é‡çš„ç»´åº¦
            if len(relative_positions.shape) == 5:
                # å¦‚æœæ˜¯5ç»´ï¼Œå‹ç¼©ä¸º4ç»´
                relative_positions = relative_positions.squeeze(1)
            elif len(relative_positions.shape) == 3:
                relative_positions = relative_positions.unsqueeze(0)
            elif len(relative_positions.shape) == 2:
                relative_positions = relative_positions.unsqueeze(0).unsqueeze(0)
                
            if len(distances.shape) == 4:
                # å¦‚æœæ˜¯4ç»´ï¼Œå‹ç¼©ä¸º3ç»´
                distances = distances.squeeze(1)
            elif len(distances.shape) == 2:
                distances = distances.unsqueeze(0)
            elif len(distances.shape) == 1:
                distances = distances.unsqueeze(0).unsqueeze(0)
                
            if len(uav_mask.shape) == 3:
                # å¦‚æœæ˜¯3ç»´ï¼Œå‹ç¼©ä¸º2ç»´
                uav_mask = uav_mask.squeeze(1)
            elif len(uav_mask.shape) == 1:
                uav_mask = uav_mask.unsqueeze(0)
                
            if len(target_mask.shape) == 3:
                # å¦‚æœæ˜¯3ç»´ï¼Œå‹ç¼©ä¸º2ç»´
                target_mask = target_mask.squeeze(1)
            elif len(target_mask.shape) == 1:
                target_mask = target_mask.unsqueeze(0)
                
            # ç¡®ä¿å½¢çŠ¶æ­£ç¡®åå†è§£åŒ…
            if len(uav_features.shape) != 3:
                raise ValueError(f"uav_featureså½¢çŠ¶å¼‚å¸¸: {uav_features.shape}, æœŸæœ›3ç»´")
            if len(target_features.shape) != 3:
                raise ValueError(f"target_featureså½¢çŠ¶å¼‚å¸¸: {target_features.shape}, æœŸæœ›3ç»´")
                
            batch_size, n_uavs, _ = uav_features.shape
            _, n_targets, _ = target_features.shape
            
        except Exception as e:
            print(f"[ERROR] ZeroShotGNNå‰å‘ä¼ æ’­å½¢çŠ¶é”™è¯¯: {e}")
            print(f"[ERROR] uav_featureså½¢çŠ¶: {uav_features.shape}")
            print(f"[ERROR] target_featureså½¢çŠ¶: {target_features.shape}")
            raise e
        
        # === 1. å¢å¼ºçš„å®ä½“ç¼–ç ï¼ˆèåˆç©ºé—´ä¿¡æ¯ï¼‰===
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ - è¾“å…¥éªŒè¯
        uav_features = torch.clamp(uav_features, -1e6, 1e6)
        target_features = torch.clamp(target_features, -1e6, 1e6)
        
        # ç¼–ç UAVç‰¹å¾
        uav_embeddings = self.uav_encoder(uav_features)  # [batch_size, N_uav, embedding_dim]
        uav_embeddings = torch.clamp(uav_embeddings, -1e6, 1e6)  # é˜²æ­¢ç¼–ç å™¨è¾“å‡ºå¼‚å¸¸å€¼
        
        # ç¼–ç ç›®æ ‡ç‰¹å¾
        target_embeddings = self.target_encoder(target_features)  # [batch_size, N_target, embedding_dim]
        target_embeddings = torch.clamp(target_embeddings, -1e6, 1e6)  # é˜²æ­¢ç¼–ç å™¨è¾“å‡ºå¼‚å¸¸å€¼
        
        # èåˆç©ºé—´å…³ç³»ä¿¡æ¯åˆ°UAVåµŒå…¥ä¸­
        uav_embeddings_enhanced = self._enhance_uav_embeddings_with_spatial_info(
            uav_embeddings, target_embeddings, relative_positions, distances
        )
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        uav_embeddings_enhanced = self.position_encoder(uav_embeddings_enhanced)
        target_embeddings = self.position_encoder(target_embeddings)
        
        # === 2. è‡ªæ³¨æ„åŠ› ===
        # å®‰å…¨çš„æ©ç å¤„ç†ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
        try:
            # UAVå†…éƒ¨è‡ªæ³¨æ„åŠ› - å­¦ä¹ UAVé—´çš„åä½œå…³ç³»
            uav_mask_bool = (uav_mask == 0)  # è½¬æ¢ä¸ºå¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºéœ€è¦å¿½ç•¥çš„ä½ç½®
            
            # ç¡®ä¿æ©ç ç»´åº¦æ­£ç¡® [batch_size, N_uav]
            if uav_mask_bool.dim() == 1:
                uav_mask_bool = uav_mask_bool.unsqueeze(0)
            
            if self.use_low_rank_attention:
                # ä¸ºä½ç§©æ³¨æ„åŠ›åˆ›å»ºæ©ç 
                uav_attention_mask = self._create_attention_mask(uav_mask_bool)
                
                uav_contextualized = self.uav_self_attention(
                    uav_embeddings_enhanced,
                    mask=uav_attention_mask
                )  # [batch_size, N_uav, embedding_dim]
                
                # ç›®æ ‡å†…éƒ¨è‡ªæ³¨æ„åŠ› - å­¦ä¹ ç›®æ ‡é—´çš„ä¾èµ–å…³ç³»
                target_mask_bool = (target_mask == 0)
                
                # ç¡®ä¿æ©ç ç»´åº¦æ­£ç¡® [batch_size, N_target]
                if target_mask_bool.dim() == 1:
                    target_mask_bool = target_mask_bool.unsqueeze(0)
                
                # ä¸ºä½ç§©æ³¨æ„åŠ›åˆ›å»ºæ©ç 
                target_attention_mask = self._create_attention_mask(target_mask_bool)
                
                target_contextualized = self.target_self_attention(
                    target_embeddings,
                    mask=target_attention_mask
                )  # [batch_size, N_target, embedding_dim]
            else:
                # æ ‡å‡†Transformeræ³¨æ„åŠ›
                uav_contextualized = self.uav_self_attention(
                    uav_embeddings_enhanced,
                    src_key_padding_mask=uav_mask_bool
                )  # [batch_size, N_uav, embedding_dim]
                
                # ç›®æ ‡å†…éƒ¨è‡ªæ³¨æ„åŠ› - å­¦ä¹ ç›®æ ‡é—´çš„ä¾èµ–å…³ç³»
                target_mask_bool = (target_mask == 0)
                
                # ç¡®ä¿æ©ç ç»´åº¦æ­£ç¡® [batch_size, N_target]
                if target_mask_bool.dim() == 1:
                    target_mask_bool = target_mask_bool.unsqueeze(0)
                
                target_contextualized = self.target_self_attention(
                    target_embeddings,
                    src_key_padding_mask=target_mask_bool
                )  # [batch_size, N_target, embedding_dim]
            
            # === 3. ä¼˜åŒ–çš„é€æ— äººæœºäº¤å‰æ³¨æ„åŠ› ===
            # ä½¿ç”¨é€æ— äººæœºçš„æ–¹å¼è®¡ç®—äº¤å‰æ³¨æ„åŠ›ï¼Œé¿å…åˆ›å»ºå·¨å¤§å¼ é‡
            uav_target_aware = torch.zeros_like(uav_contextualized)
            
            # é€ä¸ªå¤„ç†æ¯æ¶æ— äººæœºï¼Œé¿å…æ˜¾å­˜ç“¶é¢ˆ
            for uav_idx in range(n_uavs):
                # æ£€æŸ¥å½“å‰UAVæ˜¯å¦æœ‰æ•ˆ
                if uav_idx < uav_mask_bool.shape[1] and uav_mask_bool[0, uav_idx]:
                    continue  # è·³è¿‡æ— æ•ˆçš„UAV
                
                # æå–å•ä¸ªUAVçš„è¡¨ç¤º [batch_size, 1, embedding_dim]
                single_uav = uav_contextualized[:, uav_idx:uav_idx+1, :]
                
                # ä¸ºå•ä¸ªUAVè®¡ç®—å¯¹æ‰€æœ‰ç›®æ ‡çš„äº¤å‰æ³¨æ„åŠ›
                try:
                    if self.use_low_rank_attention:
                        # ä¸ºäº¤å‰æ³¨æ„åŠ›åˆ›å»ºæ©ç 
                        cross_attention_mask = self._create_attention_mask(target_mask_bool)
                        
                        single_uav_aware = self.cross_attention(
                            tgt=single_uav,  # query: å•ä¸ªUAVè¡¨ç¤º
                            memory=target_contextualized,  # key & value: æ‰€æœ‰ç›®æ ‡è¡¨ç¤º
                            tgt_mask=None,  # å•ä¸ªUAVä¸éœ€è¦æ©ç 
                            memory_mask=cross_attention_mask
                        )  # [batch_size, 1, embedding_dim]
                    else:
                        # æ ‡å‡†Transformeräº¤å‰æ³¨æ„åŠ›
                        single_uav_aware = self.cross_attention(
                            tgt=single_uav,  # query: å•ä¸ªUAVè¡¨ç¤º
                            memory=target_contextualized,  # key & value: æ‰€æœ‰ç›®æ ‡è¡¨ç¤º
                            tgt_key_padding_mask=None,  # å•ä¸ªUAVä¸éœ€è¦æ©ç 
                            memory_key_padding_mask=target_mask_bool
                        )  # [batch_size, 1, embedding_dim]
                    
                    # å°†ç»“æœå­˜å‚¨å›åŸä½ç½®
                    uav_target_aware[:, uav_idx, :] = single_uav_aware.squeeze(1)
                    
                except Exception as e:
                    # å¦‚æœå•ä¸ªUAVçš„äº¤å‰æ³¨æ„åŠ›å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¡¨ç¤º
                    uav_target_aware[:, uav_idx, :] = uav_contextualized[:, uav_idx, :]
            
        except Exception as e:
            # å¦‚æœæ³¨æ„åŠ›æœºåˆ¶å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„å¤„ç†æ–¹å¼
            print(f"æ³¨æ„åŠ›æœºåˆ¶å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†: {str(e)[:100]}...")
            uav_target_aware = uav_embeddings_enhanced
            target_contextualized = target_embeddings
        
        # === 4. å®Œæ•´çš„Qå€¼è§£ç  - è¾“å‡ºå››ç»´å¼ é‡ ===
        batch_size, n_uavs, embed_dim = uav_target_aware.shape
        _, n_targets, _ = target_contextualized.shape
        
        # åˆå§‹åŒ–å››ç»´Qå€¼å¼ é‡ [batch_size, n_uavs, n_targets, n_phi]
        # ä½¿ç”¨å®é™…çš„UAVå’Œç›®æ ‡æ•°é‡ï¼Œè€Œä¸æ˜¯å›ºå®šçš„æœ€å¤§å€¼
        q_values_4d = torch.full((batch_size, n_uavs, n_targets, self.n_phi), 
                                float('-inf'), device=uav_target_aware.device)
        
        # ä¸ºæ¯ä¸ªæœ‰æ•ˆçš„UAV-ç›®æ ‡å¯¹è®¡ç®—Qå€¼
        for i in range(n_uavs):
            for j in range(n_targets):
                # è®¡ç®—UAV-ç›®æ ‡äº¤äº’ç‰¹å¾
                uav_emb = uav_target_aware[:, i, :]  # [batch_size, embed_dim]
                target_emb = target_contextualized[:, j, :]  # [batch_size, embed_dim]
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                uav_emb = torch.clamp(uav_emb, -1e6, 1e6)
                target_emb = torch.clamp(target_emb, -1e6, 1e6)
                
                # ç®€å•çš„åŠ æ³•èåˆï¼ˆå¯ä»¥æ”¹ä¸ºæ›´å¤æ‚çš„èåˆæ–¹å¼ï¼‰
                interaction_emb = uav_emb + target_emb  # [batch_size, embed_dim]
                interaction_emb = torch.clamp(interaction_emb, -1e6, 1e6)
                
                # é€šè¿‡Qå€¼è§£ç å™¨å¾—åˆ°æ‰€æœ‰è§’åº¦çš„Qå€¼
                q_values_phi = self.q_decoder(interaction_emb)  # [batch_size, n_phi]
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥Qå€¼è¾“å‡º
                if torch.isnan(q_values_phi).any() or torch.isinf(q_values_phi).any():
                    q_values_phi = torch.full_like(q_values_phi, -1e-3)
                else:
                    q_values_phi = torch.clamp(q_values_phi, -1e6, 1e6)
                
                # å­˜å‚¨åˆ°å››ç»´å¼ é‡ä¸­
                q_values_4d[:, i, j, :] = q_values_phi
        
        # === 5. åº”ç”¨æ©ç æ“ä½œ ===
        # å¯¹æ— æ•ˆçš„UAVå’Œç›®æ ‡è®¾ç½®æå°è´Ÿæ•°
        for i in range(n_uavs):
            for j in range(n_targets):
                # æ£€æŸ¥UAVæ˜¯å¦æœ‰æ•ˆ
                if i < uav_mask.shape[1] and uav_mask[0, i] == 0:
                    q_values_4d[:, i, j, :] = -1e9
                
                # æ£€æŸ¥ç›®æ ‡æ˜¯å¦æœ‰æ•ˆ
                if j < target_mask.shape[1] and target_mask[0, j] == 0:
                    q_values_4d[:, i, j, :] = -1e9
        
        # === 6. å±•å¹³ä¸ºä¸€ç»´å‘é‡ ===
        # å°†å››ç»´å¼ é‡å±•å¹³ä¸º [batch_size, n_uavs * n_targets * n_phi]
        q_values_final = q_values_4d.view(batch_size, -1)
        
        # å¦‚æœè¾“å‡ºç»´åº¦ä¸æœŸæœ›çš„output_dimä¸åŒ¹é…ï¼Œè¿›è¡Œè°ƒæ•´
        if q_values_final.shape[1] != self.output_dim:
            if q_values_final.shape[1] > self.output_dim:
                # å¦‚æœå®é™…è¾“å‡ºå¤§äºæœŸæœ›ï¼Œæˆªæ–­
                q_values_final = q_values_final[:, :self.output_dim]
            else:
                # å¦‚æœå®é™…è¾“å‡ºå°äºæœŸæœ›ï¼Œå¡«å……
                padding = torch.full((batch_size, self.output_dim - q_values_final.shape[1]), 
                                   -1e9, device=q_values_final.device)
                q_values_final = torch.cat([q_values_final, padding], dim=1)
        
        # ã€NaNä¿®å¤ã€‘è¾“å‡ºéªŒè¯å’Œä¿®å¤
        if torch.isnan(q_values_final).any() or torch.isinf(q_values_final).any():
            print("âš ï¸ è­¦å‘Š: ZeroShotGNNç½‘ç»œè¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨å®‰å…¨é»˜è®¤å€¼")
            # ä½¿ç”¨å°çš„è´Ÿå€¼è€Œä¸æ˜¯é›¶ï¼Œä¿æŒQ-learningçš„æ¢ç´¢æ€§
            q_values_final = torch.full_like(q_values_final, -1e-3)
        
        return q_values_final
    
    def _enhance_uav_embeddings_with_spatial_info(self, uav_embeddings, target_embeddings, 
                                                  relative_positions, distances):
        """
        ä½¿ç”¨ç©ºé—´ä¿¡æ¯å¢å¼ºUAVåµŒå…¥
        
        Args:
            uav_embeddings: UAVåµŒå…¥ [batch_size, N_uav, embedding_dim]
            target_embeddings: ç›®æ ‡åµŒå…¥ [batch_size, N_target, embedding_dim]
            relative_positions: ç›¸å¯¹ä½ç½® [batch_size, N_uav, N_target, 2]
            distances: è·ç¦»çŸ©é˜µ [batch_size, N_uav, N_target]
        
        Returns:
            torch.Tensor: å¢å¼ºçš„UAVåµŒå…¥
        """
        batch_size, n_uavs, embedding_dim = uav_embeddings.shape
        _, n_targets, _ = target_embeddings.shape
        
        # ä½¿ç”¨é¢„å®šä¹‰çš„ç©ºé—´ç¼–ç å™¨
        
        # ä¸ºæ¯ä¸ªUAVè®¡ç®—ç©ºé—´ä¸Šä¸‹æ–‡
        enhanced_embeddings = []
        
        for uav_idx in range(n_uavs):
            # è·å–è¯¥UAVåˆ°æ‰€æœ‰ç›®æ ‡çš„ç©ºé—´ä¿¡æ¯
            # æ£€æŸ¥relative_positionsçš„ç»´åº¦
            if relative_positions.dim() == 4:
                uav_rel_pos = relative_positions[:, uav_idx, :, :]  # [batch_size, N_target, 2]
            elif relative_positions.dim() == 3:
                # å¦‚æœæ˜¯3ç»´ï¼Œå‡è®¾æ˜¯[batch_size, N_uav*N_target, 2]ï¼Œéœ€è¦é‡æ–°æ•´å½¢
                n_targets = relative_positions.shape[1] // n_uavs
                uav_rel_pos = relative_positions[:, uav_idx*n_targets:(uav_idx+1)*n_targets, :]  # [batch_size, N_target, 2]
            else:
                # é™çº§å¤„ç†ï¼šä½¿ç”¨é›¶å¼ é‡
                uav_rel_pos = torch.zeros(batch_size, n_targets, 2, device=relative_positions.device)
            
            # æ£€æŸ¥distancesçš„ç»´åº¦
            if distances.dim() == 3:
                uav_distances = distances[:, uav_idx, :].unsqueeze(-1)  # [batch_size, N_target, 1]
            elif distances.dim() == 2:
                # å¦‚æœæ˜¯2ç»´ï¼Œå‡è®¾æ˜¯[batch_size, N_uav*N_target]ï¼Œéœ€è¦é‡æ–°æ•´å½¢
                n_targets = distances.shape[1] // n_uavs
                uav_distances = distances[:, uav_idx*n_targets:(uav_idx+1)*n_targets].unsqueeze(-1)  # [batch_size, N_target, 1]
            else:
                # é™çº§å¤„ç†ï¼šä½¿ç”¨é›¶å¼ é‡
                uav_distances = torch.zeros(batch_size, n_targets, 1, device=distances.device)
            
            # ç»„åˆç©ºé—´ç‰¹å¾
            # ç¡®ä¿ä¸¤ä¸ªå¼ é‡çš„å‰ä¸¤ä¸ªç»´åº¦åŒ¹é…
            if uav_rel_pos.shape[:2] != uav_distances.shape[:2]:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨è¾ƒå°çš„ç»´åº¦
                min_batch = min(uav_rel_pos.shape[0], uav_distances.shape[0])
                min_targets = min(uav_rel_pos.shape[1], uav_distances.shape[1])
                uav_rel_pos = uav_rel_pos[:min_batch, :min_targets, :]
                uav_distances = uav_distances[:min_batch, :min_targets, :]
            
            spatial_features = torch.cat([uav_rel_pos, uav_distances], dim=-1)  # [batch_size, N_target, 3]
            
            # ç¼–ç ç©ºé—´ç‰¹å¾
            spatial_encoded = self.spatial_encoder(spatial_features)  # [batch_size, N_target, embedding_dim//4]
            
            # èšåˆç©ºé—´ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ³¨æ„åŠ›æƒé‡ï¼‰
            spatial_context = spatial_encoded.mean(dim=1)  # [batch_size, embedding_dim//4]
            
            # å°†ç©ºé—´ä¸Šä¸‹æ–‡èåˆåˆ°UAVåµŒå…¥ä¸­
            uav_emb = uav_embeddings[:, uav_idx, :]  # [batch_size, embedding_dim]
            
            # ç®€å•çš„æ‹¼æ¥èåˆï¼ˆå¯ä»¥æ”¹ä¸ºæ›´å¤æ‚çš„èåˆæ–¹å¼ï¼‰
            if spatial_context.shape[-1] + uav_emb.shape[-1] <= embedding_dim:
                # å¦‚æœç»´åº¦å…è®¸ï¼Œç›´æ¥æ‹¼æ¥
                padding_size = embedding_dim - spatial_context.shape[-1] - uav_emb.shape[-1]
                if padding_size > 0:
                    padding = torch.zeros(batch_size, padding_size, device=uav_emb.device)
                    enhanced_emb = torch.cat([uav_emb[:, :embedding_dim-spatial_context.shape[-1]], 
                                            spatial_context, padding], dim=-1)
                else:
                    enhanced_emb = torch.cat([uav_emb[:, :embedding_dim-spatial_context.shape[-1]], 
                                            spatial_context], dim=-1)
            else:
                # ä½¿ç”¨åŠ æƒèåˆ
                spatial_weight = 0.2
                enhanced_emb = (1 - spatial_weight) * uav_emb + spatial_weight * torch.cat([
                    spatial_context, torch.zeros(batch_size, embedding_dim - spatial_context.shape[-1], 
                                                device=spatial_context.device)
                ], dim=-1)
            
            enhanced_embeddings.append(enhanced_emb.unsqueeze(1))
        
        return torch.cat(enhanced_embeddings, dim=1)  # [batch_size, N_uav, embedding_dim]
    
    def _compute_q_values_vectorized(self, uav_target_aware, target_contextualized, uav_mask, target_mask):
        """
        å‘é‡åŒ–è®¡ç®—Qå€¼ï¼Œæé«˜æ•ˆç‡
        
        Args:
            uav_target_aware: UAVç›®æ ‡æ„ŸçŸ¥è¡¨ç¤º [batch_size, N_uav, embedding_dim]
            target_contextualized: ç›®æ ‡ä¸Šä¸‹æ–‡è¡¨ç¤º [batch_size, N_target, embedding_dim]
            uav_mask: UAVæ©ç  [batch_size, N_uav]
            target_mask: ç›®æ ‡æ©ç  [batch_size, N_target]
        
        Returns:
            torch.Tensor: Qå€¼çŸ©é˜µ [batch_size, N_uav * N_target]
        """
        batch_size, n_uavs, embedding_dim = uav_target_aware.shape
        _, n_targets, _ = target_contextualized.shape
        
        # æ‰©å±•ç»´åº¦ä»¥è¿›è¡Œå¹¿æ’­
        uav_expanded = uav_target_aware.unsqueeze(2)  # [batch_size, N_uav, 1, embedding_dim]
        target_expanded = target_contextualized.unsqueeze(1)  # [batch_size, 1, N_target, embedding_dim]
        
        # è®¡ç®—UAV-ç›®æ ‡äº¤äº’ç‰¹å¾
        interaction_features = uav_expanded + target_expanded  # [batch_size, N_uav, N_target, embedding_dim]
        
        # é‡å¡‘ä¸ºæ‰¹æ¬¡å¤„ç†
        interaction_flat = interaction_features.view(batch_size * n_uavs * n_targets, embedding_dim)
        
        # é€šè¿‡Qå€¼è§£ç å™¨
        q_values_flat = self.q_decoder(interaction_flat)  # [batch_size * N_uav * N_target, 1]
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        q_values_matrix = q_values_flat.view(batch_size, n_uavs * n_targets)
        
        return q_values_matrix
    

    
    def _create_action_mask(self, uav_mask, target_mask, n_phi):
        """
        åˆ›å»ºåŠ¨ä½œæ©ç ï¼Œå±è”½æ— æ•ˆçš„UAV-ç›®æ ‡-phiç»„åˆ
        
        Args:
            uav_mask: UAVæ©ç  [batch_size, N_uav]
            target_mask: ç›®æ ‡æ©ç  [batch_size, N_target]
            n_phi: phiç»´åº¦æ•°é‡
        
        Returns:
            torch.Tensor: åŠ¨ä½œæ©ç  [batch_size, N_actions]
        """
        batch_size, n_uavs = uav_mask.shape
        _, n_targets = target_mask.shape
        
        # åˆ›å»ºUAV-ç›®æ ‡å¯¹æ©ç 
        uav_mask_expanded = uav_mask.unsqueeze(2)  # [batch_size, N_uav, 1]
        target_mask_expanded = target_mask.unsqueeze(1)  # [batch_size, 1, N_target]
        
        # æ— æ•ˆçš„UAV-ç›®æ ‡å¯¹ï¼šä»»ä¸€å®ä½“æ— æ•ˆ
        pair_mask = (uav_mask_expanded == 0) | (target_mask_expanded == 0)  # [batch_size, N_uav, N_target]
        
        # æ‰©å±•åˆ°åŒ…å«phiç»´åº¦
        pair_mask_expanded = pair_mask.unsqueeze(-1).repeat(1, 1, 1, n_phi)  # [batch_size, N_uav, N_target, n_phi]
        action_mask = pair_mask_expanded.view(batch_size, -1)  # [batch_size, N_actions]
        
        return action_mask

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç æ¨¡å— - ä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯
    """
    
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: Tensor [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:x.size(1), :].transpose(0, 1)
        return self.dropout(x)

def get_network_info(network_type: str) -> dict:
    """
    è·å–ç½‘ç»œä¿¡æ¯
    
    Args:
        network_type: ç½‘ç»œç±»å‹
        
    Returns:
        ç½‘ç»œä¿¡æ¯å­—å…¸
    """
    network_info = {
        "SimpleNetwork": {
            "description": "åŸºç¡€å…¨è¿æ¥ç½‘ç»œ",
            "features": ["BatchNorm", "Dropout", "Xavieråˆå§‹åŒ–"],
            "complexity": "ä½"
        },
        "DeepFCN": {
            "description": "æ·±åº¦å…¨è¿æ¥ç½‘ç»œ",
            "features": ["å¤šå±‚ç»“æ„", "BatchNorm", "Dropout"],
            "complexity": "ä¸­"
        },
        # GATç½‘ç»œå·²ç§»é™¤
        "DeepFCNResidual": {
            "description": "å¸¦æ®‹å·®è¿æ¥çš„æ·±åº¦ç½‘ç»œ",
            "features": ["æ®‹å·®è¿æ¥", "BatchNorm", "Dropout"],
            "complexity": "ä¸­"
        },
        "ZeroShotGNN": {
            "description": "é›¶æ ·æœ¬å›¾ç¥ç»ç½‘ç»œ",
            "features": ["Transformeræ¶æ„", "è‡ªæ³¨æ„åŠ›", "äº¤å‰æ³¨æ„åŠ›", "å‚æ•°å…±äº«", "é›¶æ ·æœ¬è¿ç§»"],
            "complexity": "é«˜"
        }
    }
    
    return network_info.get(network_type, {"description": "æœªçŸ¥ç½‘ç»œ", "features": [], "complexity": "æœªçŸ¥"}) 