#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¾ç¨‹å­¦ä¹ è®­ç»ƒè„šæœ¬
ä»æ˜“åˆ°éš¾çš„åœºæ™¯åºåˆ—è®­ç»ƒï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import os
import time
import pickle
from datetime import datetime
from typing import List, Dict, Tuple

from main import GraphRLSolver, set_chinese_font
from config import Config
from scenarios import get_curriculum_scenarios
from environment import DirectedGraph
from comprehensive_debug_analysis import ComprehensiveDebugAnalyzer
from comprehensive_debug_analysis import ComprehensiveDebugAnalyzer
from scenarios import test_large_curriculum_scenarios

class CurriculumLearningTrainer:
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config or Config()
        self.config.ENABLE_REWARD_DEBUG = False  # å‡å°‘è¾“å‡º
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_history = {
            'levels': [],
            'performance': [],
            'convergence_data': [],
            'training_config': {
                'episodes_per_level': None,
                'success_threshold': None,
                'early_stopping': True,
                'patience': getattr(self.config, 'PATIENCE', 100),
                'learning_rate': getattr(self.config, 'LEARNING_RATE', 1e-5),
                'batch_size': getattr(self.config, 'BATCH_SIZE', 16),
                'memory_size': getattr(self.config, 'MEMORY_SIZE', 15000),
                'network_type': 'ZeroShotGNN',
                'optimizer': 'AdamW',
                'gamma': getattr(self.config, 'GAMMA', 0.99),
                'epsilon_start': getattr(self.config, 'EPSILON_START', 0.9),
                'epsilon_end': getattr(self.config, 'EPSILON_END', 0.1),
                'epsilon_decay': getattr(self.config, 'EPSILON_DECAY', 0.9995),
                'target_update_freq': getattr(self.config, 'TARGET_UPDATE_FREQ', 20),
                'use_prioritized_replay': getattr(self.config.training_config, 'use_prioritized_replay', True),
                'per_alpha': getattr(self.config.training_config, 'per_alpha', 0.6),
                'per_beta_start': getattr(self.config.training_config, 'per_beta_start', 0.4),
                'gradient_clipping': getattr(self.config.training_config, 'use_gradient_clipping', True),
                'max_grad_norm': getattr(self.config.training_config, 'max_grad_norm', 1.0),
                'reward_normalization': getattr(self.config, 'REWARD_NORMALIZATION', True),
                'reward_scale': getattr(self.config, 'REWARD_SCALE', 0.3)
            }
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = f"output/curriculum_learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"è®­ç»ƒé…ç½®: å­¦ä¹ ç‡={self.training_history['training_config']['learning_rate']}, "
              f"æ‰¹æ¬¡å¤§å°={self.training_history['training_config']['batch_size']}")
    
    def train_curriculum(self, episodes_per_level=200, success_threshold=0.8):
        """
        æ‰§è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        
        Args:
            episodes_per_level: æ¯ä¸ªéš¾åº¦çº§åˆ«çš„è®­ç»ƒè½®æ¬¡
            success_threshold: è¿›å…¥ä¸‹ä¸€çº§åˆ«çš„æˆåŠŸç‡é˜ˆå€¼
        """
        print("=" * 80)
        print("å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
        print("=" * 80)
        
        # æ›´æ–°è®­ç»ƒé…ç½®è®°å½•
        self.training_history['training_config']['episodes_per_level'] = episodes_per_level
        self.training_history['training_config']['success_threshold'] = success_threshold
        
        # è®°å½•å®é™…ä½¿ç”¨çš„é…ç½®å‚æ•°
        if hasattr(self, 'config'):
            self.training_history['training_config'].update({
                'actual_learning_rate': getattr(self.config, 'LEARNING_RATE', 1e-5),
                'actual_batch_size': getattr(self.config, 'BATCH_SIZE', 16),
                'actual_memory_size': getattr(self.config, 'MEMORY_SIZE', 15000),
                'network_architecture': getattr(self.config, 'NETWORK_TYPE', 'ZeroShotGNN'),
                'training_mode': getattr(self.config, 'TRAINING_MODE', 'zero_shot_train'),
                'use_phrrt': getattr(self.config, 'USE_PHRRT_DURING_TRAINING', True),
                'obstacle_tolerance': getattr(self.config, 'OBSTACLE_TOLERANCE', 50.0),
                'map_size': getattr(self.config, 'MAP_SIZE', 1000.0)
            })
        
        # è®¾ç½®å­—ä½“
        set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
        
        # è·å–è¯¾ç¨‹åœºæ™¯
        curriculum_scenarios = get_curriculum_scenarios()
        
        # åˆå§‹åŒ–æ±‚è§£å™¨ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªåœºæ™¯ï¼‰
        solver = None
        
        for level_idx, (scenario_func, level_name, description) in enumerate(curriculum_scenarios):
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒçº§åˆ« {level_idx + 1}: {level_name}")
            print(f"åœºæ™¯æè¿°: {description}")
            print(f"{'='*60}")
            
            # è·å–å½“å‰çº§åˆ«çš„åœºæ™¯
            uavs, targets, obstacles = scenario_func(self.config.OBSTACLE_TOLERANCE)
            
            # åˆ›å»ºæˆ–æ›´æ–°æ±‚è§£å™¨
            if solver is None:
                # é¦–æ¬¡åˆ›å»ºæ±‚è§£å™¨
                graph = DirectedGraph(uavs, targets, len(uavs[0].resources), obstacles, self.config)
                i_dim = len(uavs) * len(targets) * len(uavs[0].resources)
                h_dim = [256, 128]
                o_dim = len(uavs) * len(targets) * len(uavs[0].resources)
                
                solver = GraphRLSolver(
                    uavs=uavs, targets=targets, graph=graph, obstacles=obstacles,
                    i_dim=i_dim, h_dim=h_dim, o_dim=o_dim, config=self.config,
                    obs_mode="graph", network_type="ZeroShotGNN"
                )
                print(f"æ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆ")
            else:
                # æ›´æ–°æ±‚è§£å™¨çš„ç¯å¢ƒ
                solver.env.uavs = uavs
                solver.env.targets = targets
                solver.env.obstacles = obstacles
                solver.env.graph = DirectedGraph(uavs, targets, len(uavs[0].resources), obstacles, self.config)
                print(f"æ±‚è§£å™¨ç¯å¢ƒå·²æ›´æ–°")
            
            # è®­ç»ƒå½“å‰çº§åˆ«
            level_performance = self._train_level(
                solver, level_name, episodes_per_level, success_threshold
            )
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['levels'].append({
                'level': level_idx + 1,
                'name': level_name,
                'description': description,
                'episodes': episodes_per_level,
                'performance': level_performance
            })
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æˆåŠŸé˜ˆå€¼
            final_success_rate = level_performance.get('final_success_rate', 0)
            if final_success_rate < success_threshold:
                print(f"âš ï¸ çº§åˆ« {level_idx + 1} æœªè¾¾åˆ°æˆåŠŸé˜ˆå€¼ ({final_success_rate:.2%} < {success_threshold:.2%})")
                print(f"å»ºè®®å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´ç½‘ç»œå‚æ•°")
            else:
                print(f"âœ… çº§åˆ« {level_idx + 1} è®­ç»ƒæˆåŠŸ ({final_success_rate:.2%} >= {success_threshold:.2%})")
            
            # ä¿å­˜å½“å‰çº§åˆ«çš„æœ€ä¼˜æ¨¡å‹
            best_model_path = os.path.join(self.output_dir, f"best_model_level_{level_idx + 1}_{level_name}.pth")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
            if hasattr(solver, '_last_saved_model_path') and solver._last_saved_model_path:
                # å¤åˆ¶æœ€ä½³æ¨¡å‹åˆ°è¯¾ç¨‹å­¦ä¹ ç›®å½•
                import shutil
                try:
                    shutil.copy2(solver._last_saved_model_path, best_model_path)
                    print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
                except Exception as e:
                    # å¦‚æœå¤åˆ¶å¤±è´¥ï¼Œä¿å­˜å½“å‰æ¨¡å‹
                    torch.save(solver.policy_net.state_dict(), best_model_path)
                    print(f"âš ï¸ å¤åˆ¶æœ€ä¼˜æ¨¡å‹å¤±è´¥ï¼Œä¿å­˜å½“å‰æ¨¡å‹: {best_model_path}")
            else:
                # ä¿å­˜å½“å‰æ¨¡å‹
                torch.save(solver.policy_net.state_dict(), best_model_path)
                print(f"ğŸ“ å½“å‰æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
            
            # è®°å½•æ¨¡å‹è·¯å¾„åˆ°æ€§èƒ½æ•°æ®ä¸­
            level_performance['best_model_path'] = best_model_path
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report()
        
        print(f"\n{'='*80}")
        print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        print(f"è¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {self.output_dir}")
        print(f"{'='*80}")
        
        return solver
    
    def _train_level(self, solver, level_name, episodes, success_threshold):
        """è®­ç»ƒå•ä¸ªéš¾åº¦çº§åˆ« - å¢å¼ºç‰ˆï¼Œè¾“å‡ºæ¯è½®å¥–åŠ±å˜åŒ–è¯¦æƒ…"""
        print(f"\nå¼€å§‹è®­ç»ƒ {level_name}...")
        print(f"ç›®æ ‡è½®æ¬¡: {episodes}, æˆåŠŸé˜ˆå€¼: {success_threshold:.2%}")
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = ComprehensiveDebugAnalyzer(
            os.path.join(self.output_dir, f"debug_{level_name}")
        )
        
        # è®­ç»ƒç»Ÿè®¡
        episode_rewards = []
        success_episodes = []
        completion_rates = []
        losses = []  # è®°å½•æŸå¤±å€¼
        exploration_rates = []  # è®°å½•æ¢ç´¢ç‡
        
        # æ–°å¢ï¼šè¯¦ç»†å¥–åŠ±åˆ†è§£è®°å½•
        detailed_reward_history = []  # è®°å½•æ¯ä¸ªepisodeçš„è¯¦ç»†å¥–åŠ±åˆ†è§£
        step_reward_history = []      # è®°å½•æ¯æ­¥çš„å¥–åŠ±åˆ†è§£
        
        # æ—©åœæœºåˆ¶
        best_reward = float('-inf')
        patience_counter = 0
        patience = 100  # æ—©åœè€å¿ƒå€¼
        
        start_time = time.time()
        
        # è¿›åº¦æ¡åˆå§‹åŒ–
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {level_name}")
        print(f"è®­ç»ƒè¿›åº¦: [{'':50}] 0/{episodes} (0.0%)")
        last_progress_update = 0
        
        # è®­ç»ƒé˜¶æ®µæ ‡è®°
        training_phases = {
            int(episodes * 0.25): "æ¢ç´¢é˜¶æ®µ",
            int(episodes * 0.50): "å­¦ä¹ é˜¶æ®µ", 
            int(episodes * 0.75): "ä¼˜åŒ–é˜¶æ®µ",
            int(episodes * 0.90): "æ”¶æ•›é˜¶æ®µ"
        }
        
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            state = solver.env.reset()
            episode_reward = 0
            step_count = 0
            max_steps = 50  # é™åˆ¶æœ€å¤§æ­¥æ•°
            
            # å½“å‰episodeçš„å¥–åŠ±åˆ†è§£è®°å½•
            episode_reward_breakdown = {
                'total_reward': 0.0,
                'step_rewards': [],
                'step_breakdowns': [],
                'pbrs_info': [],
                'final_success': False,
                'completion_rate': 0.0
            }
            
            while step_count < max_steps:
                # é€‰æ‹©åŠ¨ä½œ
                state_tensor = solver._prepare_state_tensor(state)
                action = solver.select_action(state_tensor)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, truncated, info = solver.env.step(action.item())
                
                episode_reward += reward
                step_count += 1
                
                # è®°å½•å½“å‰æ­¥éª¤çš„è¯¦ç»†å¥–åŠ±ä¿¡æ¯
                step_reward_info = {
                    'step': step_count,
                    'action': action.item(),
                    'reward': reward,
                    'base_reward': info.get('base_reward', 0.0),
                    'shaping_reward': info.get('shaping_reward', 0.0),
                    'reward_breakdown': info.get('reward_breakdown', {}),
                    'target_id': info.get('target_id', -1),
                    'uav_id': info.get('uav_id', -1),
                    'contribution': info.get('actual_contribution', 0.0),
                    'path_length': info.get('path_length', 0.0),
                    'done': done
                }
                
                episode_reward_breakdown['step_rewards'].append(reward)
                episode_reward_breakdown['step_breakdowns'].append(step_reward_info)
                
                # è®°å½•PBRSä¿¡æ¯
                if info.get('pbrs_enabled', False):
                    pbrs_info = {
                        'potential_before': info.get('potential_before', 0.0),
                        'potential_after': info.get('potential_after', 0.0),
                        'shaping_reward': info.get('shaping_reward', 0.0)
                    }
                    episode_reward_breakdown['pbrs_info'].append(pbrs_info)
                
                # è®°å½•æ•°æ®åˆ°åˆ†æå™¨
                if episode % 20 == 0:  # æ¯20ä¸ªepisodeè®°å½•ä¸€æ¬¡è¯¦ç»†æ•°æ®
                    analyzer.log_step_data(episode, step_count, {
                        'action': action.item(),
                        'reward': reward,
                        'is_valid': not info.get('invalid_action', False),
                        'reward_breakdown': info.get('reward_breakdown', {})
                    })
                
                state = next_state
                
                if done or truncated:
                    break
            
            # å®Œå–„episodeå¥–åŠ±åˆ†è§£è®°å½•
            episode_reward_breakdown['total_reward'] = episode_reward
            episode_reward_breakdown['final_success'] = episode_reward >= 1000
            
            # è®¡ç®—å®Œæˆç‡
            total_remaining = sum(np.sum(t.remaining_resources) for t in solver.env.targets)
            total_original = sum(np.sum(t.resources) for t in solver.env.targets)
            completion_rate = 1.0 - (total_remaining / (total_original + 1e-6))
            episode_reward_breakdown['completion_rate'] = completion_rate
            
            # è®°å½•episodeæ•°æ®
            episode_rewards.append(episode_reward)
            completion_rates.append(completion_rate)
            detailed_reward_history.append(episode_reward_breakdown)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆè·å¾—1000åˆ†å¥–åŠ±ï¼‰
            if episode_reward >= 1000:
                success_episodes.append(episode)
            
            # è¾“å‡ºæ¯è½®çš„è¯¦ç»†å¥–åŠ±å˜åŒ–ï¼ˆæ¯10è½®è¾“å‡ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯ï¼‰
            if episode % 10 == 0 or episode_reward >= 1000:
                self._print_episode_reward_details(episode, episode_reward_breakdown, level_name)
            
            # è®°å½•åˆ°åˆ†æå™¨
            if episode % 20 == 0:
                analyzer.log_episode_data(episode, {
                    'total_reward': episode_reward,
                    'completion_rate': completion_rate,
                    'final_success': episode_reward >= 1000,
                    'step_count': step_count,
                    'detailed_breakdown': episode_reward_breakdown
                })
                
                analyzer.log_resource_allocation(episode, solver.env.uavs, solver.env.targets)
            
            # è®­ç»ƒç½‘ç»œå¹¶è®°å½•æŸå¤±
            if len(solver.memory) > solver.batch_size:
                loss = solver.optimize_model()
                if loss is not None:
                    losses.append(loss)
            
            # è®°å½•æ¢ç´¢ç‡
            exploration_rates.append(solver.epsilon)
            
            # æ—©åœæ£€æŸ¥
            if episode_reward > best_reward:
                best_reward = episode_reward
                patience_counter = 0
            else:
                patience_counter += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress = (episode + 1) / episodes
            if progress - last_progress_update >= 0.02 or episode == episodes - 1:  # æ¯2%æ›´æ–°ä¸€æ¬¡
                filled = int(50 * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (50 - filled)
                print(f"\rè®­ç»ƒè¿›åº¦: [{bar}] {episode + 1}/{episodes} ({progress:.1%})", end='', flush=True)
                last_progress_update = progress
            
            # è®­ç»ƒé˜¶æ®µæç¤º
            if (episode + 1) in training_phases:
                phase_name = training_phases[episode + 1]
                print(f"\nğŸ¯ è¿›å…¥{phase_name} (Episode {episode + 1})")
            
            # å®šæœŸè¾“å‡ºè¯¦ç»†è¿›åº¦
            if (episode + 1) % 50 == 0:
                recent_rewards = episode_rewards[-50:]
                recent_success_rate = len([r for r in recent_rewards if r >= 1000]) / len(recent_rewards)
                recent_completion = np.mean(completion_rates[-50:])
                avg_loss = np.mean(losses[-50:]) if losses else 0
                
                # è®¡ç®—è®­ç»ƒè¶‹åŠ¿
                if len(episode_rewards) >= 100:
                    trend = "ğŸ“ˆ" if np.mean(episode_rewards[-50:]) > np.mean(episode_rewards[-100:-50]) else "ğŸ“‰"
                else:
                    trend = "ğŸ“Š"
                
                print(f"\n{trend} Episode {episode + 1}/{episodes}: "
                      f"å¹³å‡å¥–åŠ±={np.mean(recent_rewards):.1f}, "
                      f"æˆåŠŸç‡={recent_success_rate:.2%}, "
                      f"å®Œæˆç‡={recent_completion:.2%}, "
                      f"æŸå¤±={avg_loss:.4f}, "
                      f"æ¢ç´¢ç‡={solver.epsilon:.3f}")
                
                # GPUå†…å­˜ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(solver, 'get_gpu_memory_info'):
                    print(f"   {solver.get_gpu_memory_info()}")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience and episode > episodes * 0.3:
                print(f"\næ—©åœè§¦å‘äºç¬¬ {episode + 1} è½® (è¿ç»­{patience}è½®æ— æ”¹è¿›)")
                break
        
        training_time = time.time() - start_time
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        final_success_rate = len(success_episodes) / episodes
        final_avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)
        final_completion_rate = np.mean(completion_rates[-50:]) if len(completion_rates) >= 50 else np.mean(completion_rates)
        
        performance = {
            'episodes': episode + 1,  # å®é™…è®­ç»ƒè½®æ¬¡
            'training_time': training_time,
            'total_successes': len(success_episodes),
            'final_success_rate': final_success_rate,
            'final_avg_reward': final_avg_reward,
            'final_completion_rate': final_completion_rate,
            'episode_rewards': episode_rewards,
            'completion_rates': completion_rates,
            'losses': losses,
            'exploration_rates': exploration_rates,
            'best_reward': best_reward,
            'early_stopped': patience_counter >= patience,
            'detailed_reward_history': detailed_reward_history,  # æ–°å¢ï¼šè¯¦ç»†å¥–åŠ±åˆ†è§£å†å²
            'convergence_data': {
                'reward_trend': np.polyfit(range(len(episode_rewards)), episode_rewards, 1)[0] if len(episode_rewards) > 1 else 0,
                'reward_stability': np.std(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.std(episode_rewards),
                'final_exploration_rate': solver.epsilon
            }
        }
        
        print(f"\n{level_name} è®­ç»ƒå®Œæˆ:")
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        print(f"  æˆåŠŸæ¬¡æ•°: {len(success_episodes)}/{episodes}")
        print(f"  æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
        print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.1f}")
        print(f"  æœ€ç»ˆå®Œæˆç‡: {final_completion_rate:.2%}")
        
        # ç”Ÿæˆçº§åˆ«åˆ†ææŠ¥å‘Šå’Œæ”¶æ•›æ›²çº¿
        if episode % 20 == 0:  # åªæœ‰è®°å½•äº†æ•°æ®æ‰ç”ŸæˆæŠ¥å‘Š
            analyzer.generate_comprehensive_plots()
            analyzer.generate_detailed_report()
        
        # ç”Ÿæˆè¯¦ç»†å¥–åŠ±åˆ†ææŠ¥å‘Š
        self._generate_reward_analysis_report(level_name, detailed_reward_history)
        
        # ä¿å­˜è®­ç»ƒæ”¶æ•›æ›²çº¿
        self._save_convergence_curves(level_name, performance)
        
        # ä¿å­˜è®­ç»ƒå†å²æ•°æ®ï¼ˆä¸main.pyæ ¼å¼ä¸€è‡´ï¼‰
        training_data = {
            'episode_rewards': episode_rewards,
            'completion_rates': completion_rates,
            'losses': losses,
            'exploration_rates': exploration_rates,
            'training_time': training_time,
            'detailed_reward_history': detailed_reward_history,  # æ–°å¢ï¼šè¯¦ç»†å¥–åŠ±åˆ†è§£å†å²
            'config': {
                'learning_rate': getattr(solver, 'learning_rate', self.config.LEARNING_RATE),
                'batch_size': getattr(solver, 'batch_size', self.config.BATCH_SIZE),
                'memory_size': len(solver.memory),
                'network_type': getattr(solver, 'network_type', 'ZeroShotGNN'),
                'episodes': episode + 1,
                'early_stopping': getattr(self.config, 'PATIENCE', 100),
                'target_update_freq': getattr(self.config, 'TARGET_UPDATE_FREQ', 20),
                'gamma': getattr(self.config, 'GAMMA', 0.99),
                'epsilon_start': getattr(self.config, 'EPSILON_START', 0.9),
                'epsilon_end': getattr(self.config, 'EPSILON_END', 0.1),
                'epsilon_decay': getattr(self.config, 'EPSILON_DECAY', 0.995),
                'use_prioritized_replay': getattr(self.config.training_config, 'use_prioritized_replay', True),
                'optimizer': 'AdamW' if getattr(solver, 'network_type', '') == 'ZeroShotGNN' else 'Adam'
            },
            'performance_metrics': {
                'final_success_rate': len([r for r in episode_rewards if r >= 1000]) / len(episode_rewards),
                'final_avg_reward': np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards),
                'final_completion_rate': np.mean(completion_rates[-50:]) if len(completion_rates) >= 50 else np.mean(completion_rates),
                'convergence_episode': self._find_convergence_point(episode_rewards),
                'training_stability': np.std(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.std(episode_rewards)
            }
        }
        
        # ä¿å­˜ä¸ºpickleæ–‡ä»¶ï¼ˆä¸main.pyä¸€è‡´ï¼‰
        history_path = os.path.join(self.output_dir, f"training_history_{level_name}.pkl")
        with open(history_path, 'wb') as f:
            pickle.dump(training_data, f)
        
        print(f"\nè®­ç»ƒæ•°æ®å·²ä¿å­˜: {history_path}")
        
        return performance
    
    def _find_convergence_point(self, rewards):
        """
        æ£€æµ‹è®­ç»ƒæ”¶æ•›ç‚¹
        
        Args:
            rewards: å¥–åŠ±åºåˆ—
            
        Returns:
            int: æ”¶æ•›çš„episodeç¼–å·ï¼Œå¦‚æœæœªæ”¶æ•›è¿”å›-1
        """
        if len(rewards) < 100:
            return -1
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æµ‹æ”¶æ•›
        window_size = 50
        threshold = 0.1  # å˜åŒ–é˜ˆå€¼
        
        for i in range(window_size, len(rewards) - window_size):
            # è®¡ç®—å‰åçª—å£çš„å¹³å‡å€¼
            before_avg = np.mean(rewards[i-window_size:i])
            after_avg = np.mean(rewards[i:i+window_size])
            
            # å¦‚æœå˜åŒ–å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ”¶æ•›
            if abs(after_avg - before_avg) / (abs(before_avg) + 1e-6) < threshold:
                return i
        
        return -1
    
    def _print_episode_reward_details(self, episode, reward_breakdown, level_name):
        """
        æ‰“å°æ¯è½®çš„è¯¦ç»†å¥–åŠ±åˆ†è§£ä¿¡æ¯
        
        Args:
            episode: å½“å‰è½®æ¬¡
            reward_breakdown: å¥–åŠ±åˆ†è§£å­—å…¸
            level_name: çº§åˆ«åç§°
        """
        print(f"\nğŸ“Š [{level_name}] Episode {episode + 1} å¥–åŠ±è¯¦æƒ…:")
        print(f"   æ€»å¥–åŠ±: {reward_breakdown['total_reward']:.2f}")
        print(f"   å®Œæˆç‡: {reward_breakdown['completion_rate']:.2%}")
        print(f"   æœ€ç»ˆæˆåŠŸ: {'âœ…' if reward_breakdown['final_success'] else 'âŒ'}")
        print(f"   æ€»æ­¥æ•°: {len(reward_breakdown['step_rewards'])}")
        
        # ç»Ÿè®¡æ­¥éª¤å¥–åŠ±åˆ†å¸ƒ
        step_rewards = reward_breakdown['step_rewards']
        if step_rewards:
            print(f"   æ­¥éª¤å¥–åŠ±ç»Ÿè®¡:")
            print(f"     - å¹³å‡æ­¥éª¤å¥–åŠ±: {np.mean(step_rewards):.2f}")
            print(f"     - æœ€å¤§æ­¥éª¤å¥–åŠ±: {np.max(step_rewards):.2f}")
            print(f"     - æœ€å°æ­¥éª¤å¥–åŠ±: {np.min(step_rewards):.2f}")
            print(f"     - æ­£å¥–åŠ±æ­¥æ•°: {len([r for r in step_rewards if r > 0])}")
            print(f"     - è´Ÿå¥–åŠ±æ­¥æ•°: {len([r for r in step_rewards if r < 0])}")
        
        # æ˜¾ç¤ºå…³é”®æ­¥éª¤çš„å¥–åŠ±åˆ†è§£
        step_breakdowns = reward_breakdown['step_breakdowns']
        if step_breakdowns:
            # æ‰¾å‡ºå¥–åŠ±æœ€é«˜çš„æ­¥éª¤
            max_reward_step = max(step_breakdowns, key=lambda x: x['reward'])
            print(f"   ğŸ† æœ€é«˜å¥–åŠ±æ­¥éª¤ (Step {max_reward_step['step']}):")
            print(f"     - å¥–åŠ±: {max_reward_step['reward']:.2f}")
            print(f"     - åŸºç¡€å¥–åŠ±: {max_reward_step['base_reward']:.2f}")
            print(f"     - å¡‘å½¢å¥–åŠ±: {max_reward_step['shaping_reward']:.2f}")
            print(f"     - ç›®æ ‡ID: {max_reward_step['target_id']}, UAV ID: {max_reward_step['uav_id']}")
            print(f"     - è´¡çŒ®é‡: {max_reward_step['contribution']:.2f}")
            print(f"     - è·¯å¾„é•¿åº¦: {max_reward_step['path_length']:.2f}")
            
            # æ˜¾ç¤ºè¯¦ç»†çš„å¥–åŠ±åˆ†è§£ï¼ˆå¦‚æœæœ‰ï¼‰
            if max_reward_step['reward_breakdown']:
                breakdown = max_reward_step['reward_breakdown']
                print(f"     - è¯¦ç»†åˆ†è§£:")
                
                # å¤„ç†ä¸åŒç±»å‹çš„å¥–åŠ±åˆ†è§£
                if 'simple_breakdown' in breakdown:
                    # ç®€å•å¥–åŠ±åˆ†è§£
                    simple_bd = breakdown['simple_breakdown']
                    for key, value in simple_bd.items():
                        if value != 0:
                            print(f"       * {key}: {value:.2f}")
                
                elif 'layer1_breakdown' in breakdown and 'layer2_breakdown' in breakdown:
                    # åŒå±‚å¥–åŠ±åˆ†è§£
                    print(f"       * ç¬¬ä¸€å±‚å¥–åŠ±: {breakdown['layer1_total']:.2f}")
                    layer1_bd = breakdown['layer1_breakdown']
                    for key, value in layer1_bd.items():
                        if value != 0:
                            print(f"         - {key}: {value:.2f}")
                    
                    print(f"       * ç¬¬äºŒå±‚å¥–åŠ±: {breakdown['layer2_total']:.2f}")
                    layer2_bd = breakdown['layer2_breakdown']
                    for key, value in layer2_bd.items():
                        if value != 0:
                            print(f"         - {key}: {value:.2f}")
        
        # PBRSä¿¡æ¯ç»Ÿè®¡
        pbrs_info = reward_breakdown['pbrs_info']
        if pbrs_info:
            total_shaping = sum(info['shaping_reward'] for info in pbrs_info)
            avg_potential_change = np.mean([info['potential_after'] - info['potential_before'] for info in pbrs_info])
            print(f"   ğŸ”„ PBRSä¿¡æ¯:")
            print(f"     - æ€»å¡‘å½¢å¥–åŠ±: {total_shaping:.2f}")
            print(f"     - å¹³å‡åŠ¿èƒ½å˜åŒ–: {avg_potential_change:.2f}")
        
        print("   " + "="*50)
    
    def _generate_reward_analysis_report(self, level_name, detailed_reward_history):
        """
        ç”Ÿæˆè¯¦ç»†çš„å¥–åŠ±åˆ†ææŠ¥å‘Š
        
        Args:
            level_name: çº§åˆ«åç§°
            detailed_reward_history: è¯¦ç»†å¥–åŠ±å†å²æ•°æ®
        """
        if not detailed_reward_history:
            return
        
        print(f"\nğŸ“ˆ [{level_name}] å¥–åŠ±åˆ†ææŠ¥å‘Š:")
        
        # ç»Ÿè®¡æ€»ä½“å¥–åŠ±åˆ†å¸ƒ
        total_rewards = [episode['total_reward'] for episode in detailed_reward_history]
        completion_rates = [episode['completion_rate'] for episode in detailed_reward_history]
        success_episodes = [i for i, episode in enumerate(detailed_reward_history) if episode['final_success']]
        
        print(f"   æ€»ä½“ç»Ÿè®¡:")
        print(f"     - å¹³å‡æ€»å¥–åŠ±: {np.mean(total_rewards):.2f}")
        print(f"     - æœ€é«˜æ€»å¥–åŠ±: {np.max(total_rewards):.2f}")
        print(f"     - æœ€ä½æ€»å¥–åŠ±: {np.min(total_rewards):.2f}")
        print(f"     - å¥–åŠ±æ ‡å‡†å·®: {np.std(total_rewards):.2f}")
        print(f"     - æˆåŠŸè½®æ¬¡: {len(success_episodes)}/{len(detailed_reward_history)} ({len(success_episodes)/len(detailed_reward_history):.2%})")
        print(f"     - å¹³å‡å®Œæˆç‡: {np.mean(completion_rates):.2%}")
        
        # åˆ†æå¥–åŠ±ç»„æˆ
        if detailed_reward_history:
            # ç»Ÿè®¡PBRSä½¿ç”¨æƒ…å†µ
            pbrs_episodes = [episode for episode in detailed_reward_history if episode['pbrs_info']]
            if pbrs_episodes:
                total_shaping_rewards = []
                for episode in pbrs_episodes:
                    episode_shaping = sum(info['shaping_reward'] for info in episode['pbrs_info'])
                    total_shaping_rewards.append(episode_shaping)
                
                print(f"   PBRSç»Ÿè®¡:")
                print(f"     - ä½¿ç”¨PBRSçš„è½®æ¬¡: {len(pbrs_episodes)}/{len(detailed_reward_history)}")
                print(f"     - å¹³å‡å¡‘å½¢å¥–åŠ±: {np.mean(total_shaping_rewards):.2f}")
                print(f"     - å¡‘å½¢å¥–åŠ±èŒƒå›´: [{np.min(total_shaping_rewards):.2f}, {np.max(total_shaping_rewards):.2f}]")
            
            # åˆ†ææ­¥éª¤å¥–åŠ±æ¨¡å¼
            all_step_rewards = []
            for episode in detailed_reward_history:
                all_step_rewards.extend(episode['step_rewards'])
            
            if all_step_rewards:
                positive_rewards = [r for r in all_step_rewards if r > 0]
                negative_rewards = [r for r in all_step_rewards if r < 0]
                zero_rewards = [r for r in all_step_rewards if r == 0]
                
                print(f"   æ­¥éª¤å¥–åŠ±åˆ†æ:")
                print(f"     - æ€»æ­¥æ•°: {len(all_step_rewards)}")
                print(f"     - æ­£å¥–åŠ±æ­¥æ•°: {len(positive_rewards)} ({len(positive_rewards)/len(all_step_rewards):.2%})")
                print(f"     - è´Ÿå¥–åŠ±æ­¥æ•°: {len(negative_rewards)} ({len(negative_rewards)/len(all_step_rewards):.2%})")
                print(f"     - é›¶å¥–åŠ±æ­¥æ•°: {len(zero_rewards)} ({len(zero_rewards)/len(all_step_rewards):.2%})")
                
                if positive_rewards:
                    print(f"     - å¹³å‡æ­£å¥–åŠ±: {np.mean(positive_rewards):.2f}")
                if negative_rewards:
                    print(f"     - å¹³å‡è´Ÿå¥–åŠ±: {np.mean(negative_rewards):.2f}")
        
        # ä¿å­˜è¯¦ç»†åˆ†æåˆ°æ–‡ä»¶
        report_path = os.path.join(self.output_dir, f"reward_analysis_{level_name}.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"å¥–åŠ±åˆ†ææŠ¥å‘Š - {level_name}\n")
            f.write("="*50 + "\n\n")
            
            f.write("æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡æ€»å¥–åŠ±: {np.mean(total_rewards):.2f}\n")
            f.write(f"  æœ€é«˜æ€»å¥–åŠ±: {np.max(total_rewards):.2f}\n")
            f.write(f"  æœ€ä½æ€»å¥–åŠ±: {np.min(total_rewards):.2f}\n")
            f.write(f"  å¥–åŠ±æ ‡å‡†å·®: {np.std(total_rewards):.2f}\n")
            f.write(f"  æˆåŠŸè½®æ¬¡: {len(success_episodes)}/{len(detailed_reward_history)} ({len(success_episodes)/len(detailed_reward_history):.2%})\n")
            f.write(f"  å¹³å‡å®Œæˆç‡: {np.mean(completion_rates):.2%}\n\n")
            
            # è¯¦ç»†çš„æ¯è½®å¥–åŠ±åˆ†è§£
            f.write("è¯¦ç»†è½®æ¬¡åˆ†è§£:\n")
            for i, episode in enumerate(detailed_reward_history):
                if i % 10 == 0 or episode['final_success']:  # æ¯10è½®æˆ–æˆåŠŸè½®æ¬¡è®°å½•è¯¦ç»†ä¿¡æ¯
                    f.write(f"\nEpisode {i+1}:\n")
                    f.write(f"  æ€»å¥–åŠ±: {episode['total_reward']:.2f}\n")
                    f.write(f"  å®Œæˆç‡: {episode['completion_rate']:.2%}\n")
                    f.write(f"  æœ€ç»ˆæˆåŠŸ: {episode['final_success']}\n")
                    f.write(f"  æ­¥æ•°: {len(episode['step_rewards'])}\n")
                    
                    if episode['step_rewards']:
                        f.write(f"  æ­¥éª¤å¥–åŠ±ç»Ÿè®¡: å¹³å‡={np.mean(episode['step_rewards']):.2f}, "
                               f"æœ€å¤§={np.max(episode['step_rewards']):.2f}, "
                               f"æœ€å°={np.min(episode['step_rewards']):.2f}\n")
        
        print(f"   è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆçš„è¯¾ç¨‹å­¦ä¹ æŠ¥å‘Š"""
        print(f"\nç”Ÿæˆæœ€ç»ˆè¯¾ç¨‹å­¦ä¹ æŠ¥å‘Š...")
        
        # è®¾ç½®å­—ä½“ - å¢å¼ºç‰ˆæœ¬
        import matplotlib
        matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
        
        # åˆ›å»ºç»¼åˆæ€§èƒ½å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        levels = [level['level'] for level in self.training_history['levels']]
        level_names = [level['name'] for level in self.training_history['levels']]
        success_rates = [level['performance']['final_success_rate'] for level in self.training_history['levels']]
        avg_rewards = [level['performance']['final_avg_reward'] for level in self.training_history['levels']]
        completion_rates = [level['performance']['final_completion_rate'] for level in self.training_history['levels']]
        training_times = [level['performance']['training_time'] for level in self.training_history['levels']]
        
        # 1. æˆåŠŸç‡å˜åŒ–
        ax1.bar(levels, success_rates, alpha=0.7, color='green')
        ax1.set_title('å„çº§åˆ«æœ€ç»ˆæˆåŠŸç‡')
        ax1.set_xlabel('è®­ç»ƒçº§åˆ«')
        ax1.set_ylabel('æˆåŠŸç‡')
        ax1.set_xticks(levels)
        ax1.set_xticklabels([f"L{i}" for i in levels])
        ax1.grid(True, alpha=0.3)
        
        # 2. å¹³å‡å¥–åŠ±å˜åŒ–
        ax2.plot(levels, avg_rewards, 'o-', color='blue', linewidth=2, markersize=8)
        ax2.set_title('å„çº§åˆ«æœ€ç»ˆå¹³å‡å¥–åŠ±')
        ax2.set_xlabel('è®­ç»ƒçº§åˆ«')
        ax2.set_ylabel('å¹³å‡å¥–åŠ±')
        ax2.set_xticks(levels)
        ax2.set_xticklabels([f"L{i}" for i in levels])
        ax2.grid(True, alpha=0.3)
        
        # 3. å®Œæˆç‡å˜åŒ–
        ax3.bar(levels, completion_rates, alpha=0.7, color='orange')
        ax3.set_title('å„çº§åˆ«æœ€ç»ˆå®Œæˆç‡')
        ax3.set_xlabel('è®­ç»ƒçº§åˆ«')
        ax3.set_ylabel('å®Œæˆç‡')
        ax3.set_xticks(levels)
        ax3.set_xticklabels([f"L{i}" for i in levels])
        ax3.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒæ—¶é—´
        ax4.bar(levels, training_times, alpha=0.7, color='red')
        ax4.set_title('å„çº§åˆ«è®­ç»ƒæ—¶é—´')
        ax4.set_xlabel('è®­ç»ƒçº§åˆ«')
        ax4.set_ylabel('è®­ç»ƒæ—¶é—´(ç§’)')
        ax4.set_xticks(levels)
        ax4.set_xticklabels([f"L{i}" for i in levels])
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = os.path.join(self.output_dir, "curriculum_learning_summary.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ€»ç»“æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"è®­ç»ƒçº§åˆ«æ•°: {len(self.training_history['levels'])}")
        report_lines.append("")
        
        # å„çº§åˆ«è¯¦ç»†ä¿¡æ¯
        for level in self.training_history['levels']:
            perf = level['performance']
            report_lines.append(f"çº§åˆ« {level['level']}: {level['name']}")
            report_lines.append(f"  åœºæ™¯æè¿°: {level['description']}")
            report_lines.append(f"  è®­ç»ƒè½®æ¬¡: {perf['episodes']}")
            report_lines.append(f"  è®­ç»ƒæ—¶é—´: {perf['training_time']:.1f}ç§’")
            report_lines.append(f"  æˆåŠŸæ¬¡æ•°: {perf['total_successes']}")
            report_lines.append(f"  æœ€ç»ˆæˆåŠŸç‡: {perf['final_success_rate']:.2%}")
            report_lines.append(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {perf['final_avg_reward']:.1f}")
            report_lines.append(f"  æœ€ç»ˆå®Œæˆç‡: {perf['final_completion_rate']:.2%}")
            report_lines.append("")
        
        # æ€»ä½“åˆ†æ
        report_lines.append("æ€»ä½“åˆ†æ:")
        report_lines.append("-" * 40)
        
        total_time = sum(training_times)
        avg_success_rate = np.mean(success_rates)
        final_level_success = success_rates[-1] if success_rates else 0
        
        report_lines.append(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’")
        report_lines.append(f"å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2%}")
        report_lines.append(f"æœ€ç»ˆçº§åˆ«æˆåŠŸç‡: {final_level_success:.2%}")
        
        if final_level_success >= 0.8:
            report_lines.append("âœ… è¯¾ç¨‹å­¦ä¹ æˆåŠŸï¼æ¨¡å‹å·²å…·å¤‡å¤„ç†å¤æ‚åœºæ™¯çš„èƒ½åŠ›")
        else:
            report_lines.append("âš ï¸ è¯¾ç¨‹å­¦ä¹ éœ€è¦æ”¹è¿›ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´å‚æ•°")
        
        report_lines.append("")
        report_lines.append("=" * 80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, "curriculum_learning_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(self.output_dir, "training_history.pkl")
        with open(history_path, 'wb') as f:
            pickle.dump(self.training_history, f)
        
        print(f"æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  å›¾è¡¨: {chart_path}")
        print(f"  æŠ¥å‘Š: {report_path}")
        print(f"  å†å²: {history_path}")
    def train_large_scale_curriculum(self, episodes_per_level=300, success_threshold=0.7):
        """
        æ‰§è¡Œå¤§è§„æ¨¡è¯¾ç¨‹å­¦ä¹ è®­ç»ƒï¼ˆ20UAV-15Targetï¼‰
        
        Args:
            episodes_per_level: æ¯ä¸ªéš¾åº¦çº§åˆ«çš„è®­ç»ƒè½®æ¬¡ï¼ˆå¤§è§„æ¨¡åœºæ™¯éœ€è¦æ›´å¤šè½®æ¬¡ï¼‰
            success_threshold: è¿›å…¥ä¸‹ä¸€çº§åˆ«çš„æˆåŠŸç‡é˜ˆå€¼
        """
        print("=" * 80)
        print("å¼€å§‹å¤§è§„æ¨¡è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ (20UAV-15Target)")
        print("=" * 80)
        
        # è®¾ç½®å­—ä½“
        set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
        
        # è·å–å¤§è§„æ¨¡è¯¾ç¨‹åœºæ™¯ - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
        try:
            from large_curriculum_scenarios_optimized import get_large_curriculum_scenarios_optimized
            curriculum_scenarios = get_large_curriculum_scenarios_optimized()
            print("âœ… ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„å¤§è§„æ¨¡è¯¾ç¨‹åœºæ™¯")
        except ImportError:
            from scenarios import get_large_curriculum_scenarios
            curriculum_scenarios = get_large_curriculum_scenarios()
            print("âš ï¸ ä½¿ç”¨åŸç‰ˆå¤§è§„æ¨¡è¯¾ç¨‹åœºæ™¯")
        
        # è°ƒæ•´é…ç½®ä»¥é€‚åº”å¤§è§„æ¨¡åœºæ™¯
        self.config.BATCH_SIZE = 32  # å¢åŠ æ‰¹æ¬¡å¤§å°
        self.config.MEMORY_SIZE = 20000  # å¢åŠ ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.config.TARGET_UPDATE = 200  # è°ƒæ•´ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        
        print(f"å¤§è§„æ¨¡è®­ç»ƒé…ç½®:")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.config.BATCH_SIZE}")
        print(f"  ç»éªŒç¼“å†²åŒº: {self.config.MEMORY_SIZE}")
        print(f"  æ¯çº§åˆ«è½®æ¬¡: {episodes_per_level}")
        
        # ä½¿ç”¨åŸæœ‰çš„è®­ç»ƒé€»è¾‘ï¼Œä½†ä¼ å…¥å¤§è§„æ¨¡åœºæ™¯
        return self._train_curriculum_with_scenarios(
            curriculum_scenarios, episodes_per_level, success_threshold, "Large_Scale"
        )

    def _train_curriculum_with_scenarios(self, scenarios, episodes_per_level, success_threshold, prefix=""):
        """
        ä½¿ç”¨æŒ‡å®šåœºæ™¯è¿›è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        """
        solver = None
        
        for level_idx, (scenario_func, level_name, description) in enumerate(scenarios):
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒçº§åˆ« {level_idx + 1}: {level_name}")
            print(f"åœºæ™¯æè¿°: {description}")
            print(f"{'='*60}")
            
            # è·å–å½“å‰çº§åˆ«çš„åœºæ™¯
            uavs, targets, obstacles = scenario_func(self.config.OBSTACLE_TOLERANCE)
            
            # åˆ›å»ºæˆ–æ›´æ–°æ±‚è§£å™¨
            if solver is None:
                # é¦–æ¬¡åˆ›å»ºæ±‚è§£å™¨
                graph = DirectedGraph(uavs, targets, len(uavs[0].resources), obstacles, self.config)
                i_dim = len(uavs) * len(targets) * len(uavs[0].resources)
                h_dim = [512, 256, 128]  # å¤§è§„æ¨¡åœºæ™¯ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ
                o_dim = len(uavs) * len(targets) * len(uavs[0].resources)
                
                solver = GraphRLSolver(
                    uavs=uavs, targets=targets, graph=graph, obstacles=obstacles,
                    i_dim=i_dim, h_dim=h_dim, o_dim=o_dim, config=self.config,
                    obs_mode="graph", network_type="ZeroShotGNN"
                )
                print(f"å¤§è§„æ¨¡æ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆ (åŠ¨ä½œç©ºé—´: {o_dim})")
            else:
                # æ›´æ–°æ±‚è§£å™¨çš„ç¯å¢ƒ
                solver.env.uavs = uavs
                solver.env.targets = targets
                solver.env.obstacles = obstacles
                solver.env.graph = DirectedGraph(uavs, targets, len(uavs[0].resources), obstacles, self.config)
                print(f"æ±‚è§£å™¨ç¯å¢ƒå·²æ›´æ–°")
            
            # è®­ç»ƒå½“å‰çº§åˆ«
            level_performance = self._train_level(
                solver, f"{prefix}_{level_name}" if prefix else level_name, 
                episodes_per_level, success_threshold
            )
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['levels'].append({
                'level': level_idx + 1,
                'name': level_name,
                'description': description,
                'episodes': episodes_per_level,
                'performance': level_performance,
                'scale': prefix or "Standard"
            })
            
            # æ£€æŸ¥æˆåŠŸé˜ˆå€¼
            final_success_rate = level_performance.get('final_success_rate', 0)
            if final_success_rate < success_threshold:
                print(f"âš ï¸ çº§åˆ« {level_idx + 1} æœªè¾¾åˆ°æˆåŠŸé˜ˆå€¼ ({final_success_rate:.2%} < {success_threshold:.2%})")
            else:
                print(f"âœ… çº§åˆ« {level_idx + 1} è®­ç»ƒæˆåŠŸ ({final_success_rate:.2%} >= {success_threshold:.2%})")
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(self.output_dir, f"model_{prefix}_level_{level_idx + 1}_{level_name}.pth")
            torch.save(solver.policy_net.state_dict(), model_path)
            print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report()
        
        return solver


def main():
    """ä¸»å‡½æ•°"""
    print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CurriculumLearningTrainer(config)
    
    # å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
    # final_solver = trainer.train_curriculum(
    #     episodes_per_level=200,  # æ¯çº§åˆ«200è½®
    #     success_threshold=0.6    # 60%æˆåŠŸç‡é˜ˆå€¼
    # )
    final_solver = trainer.train_large_scale_curriculum(episodes_per_level=300, success_threshold=0.7)

    print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
    
    return final_solver

    def _save_convergence_curves(self, level_name, performance):
        """ä¿å­˜è®­ç»ƒæ”¶æ•›æ›²çº¿ï¼ˆä¸main.pyæ ¼å¼ä¸€è‡´ï¼‰"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“ - å¢å¼ºç‰ˆæœ¬
            import matplotlib
            matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            matplotlib.rcParams['axes.unicode_minus'] = False
            set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            episodes = range(1, len(performance['episode_rewards']) + 1)
            
            # 1. å¥–åŠ±æ”¶æ•›æ›²çº¿
            ax1.plot(episodes, performance['episode_rewards'], alpha=0.6, linewidth=1, label='åŸå§‹å¥–åŠ±')
            if len(performance['episode_rewards']) > 20:
                window = min(50, len(performance['episode_rewards']) // 5)
                moving_avg = np.convolve(performance['episode_rewards'], np.ones(window)/window, mode='valid')
                ax1.plot(episodes[window-1:], moving_avg, 'r-', linewidth=2, label=f'{window}è½®ç§»åŠ¨å¹³å‡')
            ax1.set_title(f'{level_name} - å¥–åŠ±æ”¶æ•›æ›²çº¿')
            ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
            ax1.set_ylabel('å¥–åŠ±å€¼')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. å®Œæˆç‡æ›²çº¿
            ax2.plot(episodes, performance['completion_rates'], 'g-', alpha=0.7, label='å®Œæˆç‡')
            ax2.set_title(f'{level_name} - ä»»åŠ¡å®Œæˆç‡')
            ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
            ax2.set_ylabel('å®Œæˆç‡')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # 3. æŸå¤±æ›²çº¿
            if performance['losses']:
                loss_episodes = np.linspace(1, len(episodes), len(performance['losses']))
                ax3.plot(loss_episodes, performance['losses'], 'orange', alpha=0.7, label='è®­ç»ƒæŸå¤±')
                ax3.set_title(f'{level_name} - è®­ç»ƒæŸå¤±')
                ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
                ax3.set_ylabel('æŸå¤±å€¼')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            else:
                ax3.text(0.5, 0.5, 'æ— æŸå¤±æ•°æ®', ha='center', va='center', transform=ax3.transAxes)
                ax3.set_title(f'{level_name} - è®­ç»ƒæŸå¤±')
            
            # 4. æ¢ç´¢ç‡è¡°å‡
            if performance['exploration_rates']:
                ax4.plot(episodes, performance['exploration_rates'], 'm-', alpha=0.7, label='æ¢ç´¢ç‡')
                ax4.set_title(f'{level_name} - æ¢ç´¢ç‡è¡°å‡')
                ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
                ax4.set_ylabel('æ¢ç´¢ç‡')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
            else:
                ax4.text(0.5, 0.5, 'æ— æ¢ç´¢ç‡æ•°æ®', ha='center', va='center', transform=ax4.transAxes)
                ax4.set_title(f'{level_name} - æ¢ç´¢ç‡è¡°å‡')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            curve_path = os.path.join(self.output_dir, f"convergence_curves_{level_name}.png")
            plt.savefig(curve_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"æ”¶æ•›æ›²çº¿å·²ä¿å­˜: {curve_path}")
            
        except Exception as e:
            print(f"ä¿å­˜æ”¶æ•›æ›²çº¿æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CurriculumLearningTrainer(config)
    
    # å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
    # final_solver = trainer.train_curriculum(
    #     episodes_per_level=200,  # æ¯çº§åˆ«200è½®
    #     success_threshold=0.6    # 60%æˆåŠŸç‡é˜ˆå€¼
    # )
    final_solver = trainer.train_large_scale_curriculum(episodes_per_level=300, success_threshold=0.7)

    print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
    
    return final_solver


if __name__ == "__main__":
    main()