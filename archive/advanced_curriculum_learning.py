#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆé«˜çº§è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨
ä¿®å¤æ··åˆç²¾åº¦è­¦å‘Šã€ä¼˜åŒ–æ§åˆ¶å°è¾“å‡ºã€æ”¹è¿›æ—©åœæœºåˆ¶ç­‰
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import os
import time
import pickle
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from collections import deque, defaultdict

from main import GraphRLSolver, set_chinese_font
from config import Config
from scenarios import get_curriculum_scenarios
from environment import DirectedGraph
from comprehensive_debug_analysis import ComprehensiveDebugAnalyzer

class OptimizedAdvancedCurriculumTrainer:
    """ä¼˜åŒ–ç‰ˆé«˜çº§è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config or Config()
        self.config.ENABLE_REWARD_DEBUG = False
        
        # åŠ¨æ€è®­ç»ƒé…ç½®
        self.dynamic_config = {
            'min_episodes_per_level': 50,
            'max_episodes_per_level': 500,
            'success_window_size': 100,
            'convergence_window_size': 50,
            'early_promotion_threshold': 0.85,
            'stability_threshold': 0.05,
        }
        
        # ä¼˜åŒ–çš„æ—©åœé…ç½®
        self.early_stopping_config = {
            'monitor_interval': 5000,          # æ¯5000ç¯å¢ƒæ­¥ç›‘æ§ä¸€æ¬¡
            'completion_window': 10,           # å®Œæˆç‡æ»‘åŠ¨çª—å£å¤§å°
            'patience_points': 20,             # è¿ç»­20ä¸ªç›‘æ§ç‚¹
            'min_improvement': 0.001,          # æœ€å°æ”¹è¿›0.1%
            'base_patience': 50,
            'patience_multiplier': {
                'easy': 0.8, 'simple': 1.0, 'medium': 1.5, 'hard': 2.0, 'expert': 2.5
            }
        }
        
        # å¥–åŠ±ç»Ÿè®¡é…ç½®
        self.reward_tracking = {
            'collaboration_threshold': 0.3,    # ååŒå¥–åŠ±å æ¯”é˜ˆå€¼
            'reward_clip_range': (-100, 100),  # å¥–åŠ±è£å‰ªèŒƒå›´
            'enable_reward_analysis': True,    # å¯ç”¨å¥–åŠ±åˆ†æ
        }
        
        # çŠ¶æ€å½’ä¸€åŒ–é…ç½®
        self.normalization_config = {
            'position_scale': 1000.0,          # ä½ç½®å½’ä¸€åŒ–å°ºåº¦
            'resource_scale': 100.0,           # èµ„æºå½’ä¸€åŒ–å°ºåº¦
            'distance_scale': 2000.0,          # è·ç¦»å½’ä¸€åŒ–å°ºåº¦
            'enable_input_validation': True,   # å¯ç”¨è¾“å…¥éªŒè¯
        }
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'total_env_steps': 0,
            'monitor_points': [],
            'reward_distributions': [],
            'collaboration_ratios': [],
            'input_validation_errors': 0,
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = f"output/optimized_curriculum_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ä¼˜åŒ–ç‰ˆé«˜çº§è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ—©åœç›‘æ§: æ¯{self.early_stopping_config['monitor_interval']}æ­¥")
        print(f"å¥–åŠ±åˆ†æ: {'å¯ç”¨' if self.reward_tracking['enable_reward_analysis'] else 'ç¦ç”¨'}")
        print(f"è¾“å…¥éªŒè¯: {'å¯ç”¨' if self.normalization_config['enable_input_validation'] else 'ç¦ç”¨'}")
    
    def _fix_mixed_precision_warnings(self, solver):
        """ä¿®å¤1ï¼šä¿®å¤æ··åˆç²¾åº¦è®­ç»ƒçš„FutureWarning"""
        if hasattr(solver, 'use_mixed_precision') and solver.use_mixed_precision:
            # æ£€æŸ¥PyTorchç‰ˆæœ¬å¹¶ä½¿ç”¨æ­£ç¡®çš„API
            if hasattr(torch, 'amp') and hasattr(torch.amp, 'autocast'):
                solver._autocast_context = lambda: torch.amp.autocast('cuda')
                print("âœ… ä½¿ç”¨æ–°ç‰ˆtorch.amp.autocast API")
            else:
                solver._autocast_context = lambda: torch.cuda.amp.autocast()
                print("âš ï¸ ä½¿ç”¨æ—§ç‰ˆtorch.cuda.amp.autocast API")
        else:
            solver._autocast_context = lambda: torch.no_grad()
    
    def _analyze_reward_distribution(self, episode_rewards_detail, episode):
        """åˆ†æ2ï¼šåˆ†æå¥–åŠ±åˆ†å¸ƒå’ŒååŒå¥–åŠ±å æ¯”"""
        if not self.reward_tracking['enable_reward_analysis']:
            return {}
        
        reward_stats = {
            'total_reward': 0,
            'reward_types': defaultdict(float),
            'reward_counts': defaultdict(int),
            'collaboration_ratio': 0,
            'penalty_ratio': 0,
            'clipped_rewards': 0
        }
        
        for reward_info in episode_rewards_detail:
            reward_value = reward_info.get('value', 0)
            reward_type = reward_info.get('type', 'unknown')
            
            # ç»Ÿè®¡å¥–åŠ±ç±»å‹å’Œæ•°é‡
            reward_stats['reward_types'][reward_type] += reward_value
            reward_stats['reward_counts'][reward_type] += 1
            reward_stats['total_reward'] += reward_value
            
            # æ£€æŸ¥å¥–åŠ±è£å‰ª
            clip_min, clip_max = self.reward_tracking['reward_clip_range']
            if reward_value < clip_min or reward_value > clip_max:
                reward_stats['clipped_rewards'] += 1
        
        # è®¡ç®—ååŒå¥–åŠ±å æ¯”
        collaboration_reward = reward_stats['reward_types'].get('collaboration', 0)
        if reward_stats['total_reward'] != 0:
            reward_stats['collaboration_ratio'] = abs(collaboration_reward) / abs(reward_stats['total_reward'])
        
        # è®¡ç®—æƒ©ç½šå æ¯”
        total_penalties = sum(v for v in reward_stats['reward_types'].values() if v < 0)
        if reward_stats['total_reward'] != 0:
            reward_stats['penalty_ratio'] = abs(total_penalties) / abs(reward_stats['total_reward'])
        
        # è®°å½•ååŒå¥–åŠ±å æ¯”å†å²
        self.training_stats['collaboration_ratios'].append(reward_stats['collaboration_ratio'])
        
        return reward_stats
    
    def _optimized_early_stopping_monitor(self, completion_rates, episode, env_steps):
        """ä¼˜åŒ–3ï¼šæ”¹è¿›çš„æ—©åœç›‘æ§æœºåˆ¶"""
        self.training_stats['total_env_steps'] = env_steps
        
        # æ¯5000æ­¥ç›‘æ§ä¸€æ¬¡
        if env_steps % self.early_stopping_config['monitor_interval'] == 0 and len(completion_rates) >= self.early_stopping_config['completion_window']:
            
            # è®¡ç®—æ»‘åŠ¨å¹³å‡å®Œæˆç‡
            window_size = self.early_stopping_config['completion_window']
            recent_completion = np.mean(completion_rates[-window_size:])
            
            # è®°å½•ç›‘æ§ç‚¹
            monitor_point = {
                'episode': episode,
                'env_steps': env_steps,
                'completion_rate': recent_completion,
                'timestamp': time.time()
            }
            self.training_stats['monitor_points'].append(monitor_point)
            
            # æ£€æŸ¥æ—©åœæ¡ä»¶
            if len(self.training_stats['monitor_points']) >= self.early_stopping_config['patience_points']:
                # è®¡ç®—æœ€è¿‘20ä¸ªç›‘æ§ç‚¹çš„æ”¹è¿›
                recent_points = self.training_stats['monitor_points'][-self.early_stopping_config['patience_points']:]
                first_completion = recent_points[0]['completion_rate']
                last_completion = recent_points[-1]['completion_rate']
                improvement = last_completion - first_completion
                
                if improvement < self.early_stopping_config['min_improvement']:
                    return True, {
                        'reason': 'completion_rate_plateau',
                        'improvement': improvement,
                        'required_improvement': self.early_stopping_config['min_improvement'],
                        'monitor_points': len(recent_points),
                        'env_steps': env_steps
                    }
        
        return False, {}
    
    def _validate_and_normalize_input(self, state):
        """æ£€æŸ¥6ï¼šéªŒè¯å’Œå½’ä¸€åŒ–ç½‘ç»œè¾“å…¥"""
        if not self.normalization_config['enable_input_validation']:
            return state
        
        try:
            if isinstance(state, dict):  # å›¾æ¨¡å¼
                normalized_state = {}
                for key, value in state.items():
                    if key == 'uav_features':
                        # å½’ä¸€åŒ–UAVç‰¹å¾ï¼šä½ç½®ã€èµ„æºç­‰
                        normalized_value = value.clone()
                        # ä½ç½®å½’ä¸€åŒ– (å‡è®¾å‰2åˆ—æ˜¯ä½ç½®)
                        if normalized_value.shape[-1] >= 2:
                            normalized_value[..., :2] /= self.normalization_config['position_scale']
                        # èµ„æºå½’ä¸€åŒ– (å‡è®¾åç»­åˆ—æ˜¯èµ„æº)
                        if normalized_value.shape[-1] > 2:
                            normalized_value[..., 2:] /= self.normalization_config['resource_scale']
                        normalized_state[key] = normalized_value
                        
                    elif key == 'target_features':
                        # å½’ä¸€åŒ–ç›®æ ‡ç‰¹å¾
                        normalized_value = value.clone()
                        if normalized_value.shape[-1] >= 2:
                            normalized_value[..., :2] /= self.normalization_config['position_scale']
                        if normalized_value.shape[-1] > 2:
                            normalized_value[..., 2:] /= self.normalization_config['resource_scale']
                        normalized_state[key] = normalized_value
                        
                    elif key == 'edge_features':
                        # å½’ä¸€åŒ–è¾¹ç‰¹å¾ (è·ç¦»ç­‰)
                        normalized_value = value.clone()
                        normalized_value /= self.normalization_config['distance_scale']
                        normalized_state[key] = normalized_value
                        
                    else:
                        normalized_state[key] = value
                
                # éªŒè¯æ•°å€¼èŒƒå›´
                for key, value in normalized_state.items():
                    if torch.isnan(value).any() or torch.isinf(value).any():
                        self.training_stats['input_validation_errors'] += 1
                        print(f"âš ï¸ è¾“å…¥éªŒè¯é”™è¯¯: {key} åŒ…å«NaN/Inf")
                        # ç”¨é›¶æ›¿æ¢å¼‚å¸¸å€¼
                        normalized_state[key] = torch.nan_to_num(value, nan=0.0, posinf=1.0, neginf=-1.0)
                
                return normalized_state
                
            else:  # æ‰å¹³æ¨¡å¼
                normalized_state = state.clone()
                # ç®€å•çš„å½’ä¸€åŒ–å¤„ç†
                normalized_state = torch.clamp(normalized_state, -10.0, 10.0)  # è£å‰ªæç«¯å€¼
                
                if torch.isnan(normalized_state).any() or torch.isinf(normalized_state).any():
                    self.training_stats['input_validation_errors'] += 1
                    normalized_state = torch.nan_to_num(normalized_state, nan=0.0, posinf=1.0, neginf=-1.0)
                
                return normalized_state
                
        except Exception as e:
            self.training_stats['input_validation_errors'] += 1
            print(f"âš ï¸ è¾“å…¥å½’ä¸€åŒ–å¤±è´¥: {e}")
            return state
    
    def _clip_and_validate_rewards(self, reward, reward_info=None):
        """æ£€æŸ¥5ï¼šå¥–åŠ±è£å‰ªå’ŒéªŒè¯"""
        clip_min, clip_max = self.reward_tracking['reward_clip_range']
        
        # è£å‰ªå¥–åŠ±
        original_reward = reward
        clipped_reward = np.clip(reward, clip_min, clip_max)
        
        # éªŒè¯å¥–åŠ±æ•°å€¼
        if np.isnan(clipped_reward) or np.isinf(clipped_reward):
            print(f"âš ï¸ å¼‚å¸¸å¥–åŠ±å€¼: {original_reward} -> 0")
            clipped_reward = 0.0
        
        # è®°å½•è£å‰ªä¿¡æ¯
        if abs(original_reward - clipped_reward) > 1e-6:
            if reward_info:
                reward_info['clipped'] = True
                reward_info['original_value'] = original_reward
        
        return clipped_reward
    
    def _train_level_optimized(self, solver, level_name, dynamic_episodes, success_threshold, early_stopping_config):
        """ä¼˜åŒ–ç‰ˆè®­ç»ƒæ–¹æ³•"""
        print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ {level_name}...")
        
        # ä¿®å¤æ··åˆç²¾åº¦è­¦å‘Š
        self._fix_mixed_precision_warnings(solver)
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        episode_rewards = []
        success_episodes = []
        completion_rates = []
        losses = []
        exploration_rates = []
        
        # å¥–åŠ±åˆ†æç»Ÿè®¡
        episode_reward_details = []
        collaboration_ratios = []
        
        # æ—©åœæœºåˆ¶
        best_reward = float('-inf')
        patience_counter = 0
        patience = early_stopping_config['patience']
        
        # ç¯å¢ƒæ­¥æ•°è®¡æ•°
        total_env_steps = 0
        
        start_time = time.time()
        min_episodes = dynamic_episodes['min']
        max_episodes = dynamic_episodes['max']
        
        episode = 0
        early_stop_triggered = False
        early_stop_info = {}
        
        print(f"è®­ç»ƒé…ç½®:")
        print(f"  è½®æ¬¡èŒƒå›´: {min_episodes}-{max_episodes}")
        print(f"  æ—©åœç›‘æ§: æ¯{self.early_stopping_config['monitor_interval']}æ­¥")
        print(f"  å®Œæˆç‡çª—å£: {self.early_stopping_config['completion_window']}")
        print(f"  ç›‘æ§ç‚¹è€å¿ƒ: {self.early_stopping_config['patience_points']}")
        
        while episode < max_episodes and not early_stop_triggered:
            # é‡ç½®ç¯å¢ƒ
            state = solver.env.reset()
            episode_reward = 0
            step_count = 0
            max_steps = 50
            
            # å½“å‰episodeçš„å¥–åŠ±è¯¦æƒ…
            current_episode_rewards = []
            
            while step_count < max_steps:
                # éªŒè¯å’Œå½’ä¸€åŒ–è¾“å…¥
                state_tensor = solver._prepare_state_tensor(state)
                normalized_state = self._validate_and_normalize_input(state_tensor)
                
                # é€‰æ‹©åŠ¨ä½œ
                action = solver.select_action(normalized_state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, truncated, info = solver.env.step(action.item())
                
                # å¥–åŠ±è£å‰ªå’ŒéªŒè¯
                reward_info = {
                    'step': step_count,
                    'value': reward,
                    'type': info.get('reward_type', 'unknown'),
                    'clipped': False
                }
                reward = self._clip_and_validate_rewards(reward, reward_info)
                current_episode_rewards.append(reward_info)
                
                episode_reward += reward
                step_count += 1
                total_env_steps += 1
                
                state = next_state
                
                if done or truncated:
                    break
            
            # è®°å½•episodeæ•°æ®
            episode_rewards.append(episode_reward)
            episode_reward_details.append(current_episode_rewards)
            
            # åˆ†æå¥–åŠ±åˆ†å¸ƒ
            reward_stats = self._analyze_reward_distribution(current_episode_rewards, episode)
            
            # æ£€æŸ¥æˆåŠŸ
            is_success = episode_reward >= 1000
            if is_success:
                success_episodes.append(episode)
            
            # è®¡ç®—å®Œæˆç‡
            total_remaining = sum(np.sum(t.remaining_resources) for t in solver.env.targets)
            total_original = sum(np.sum(t.resources) for t in solver.env.targets)
            completion_rate = 1.0 - (total_remaining / (total_original + 1e-6))
            completion_rates.append(completion_rate)
            
            # è®­ç»ƒç½‘ç»œ
            batch_size = getattr(solver, 'batch_size', self.config.BATCH_SIZE)
            if len(solver.memory) > batch_size:
                loss = solver.optimize_model()
                if loss is not None:
                    losses.append(loss)
            
            exploration_rates.append(solver.epsilon)
            
            # ä¼˜åŒ–çš„æ§åˆ¶å°è¾“å‡º (æ¯è½®æ¬¡)
            if episode % 1 == 0:  # æ¯è½®éƒ½è¾“å‡º
                collab_ratio = reward_stats.get('collaboration_ratio', 0)
                penalty_ratio = reward_stats.get('penalty_ratio', 0)
                
                print(f"Episode {episode+1:4d}: "
                      f"æ­¥æ•°={step_count:2d}, "
                      f"æ€»å¥–åŠ±={episode_reward:7.1f}, "
                      f"ååŒå æ¯”={collab_ratio:.2%}, "
                      f"æƒ©ç½šå æ¯”={penalty_ratio:.2%}, "
                      f"å®Œæˆç‡={completion_rate:.2%}")
                
                # è¯¦ç»†å¥–åŠ±ç±»å‹ç»Ÿè®¡ (æ¯10è½®è¾“å‡ºä¸€æ¬¡)
                if episode % 10 == 0 and reward_stats['reward_types']:
                    print(f"  å¥–åŠ±æ„æˆ: ", end="")
                    for reward_type, value in reward_stats['reward_types'].items():
                        count = reward_stats['reward_counts'][reward_type]
                        print(f"{reward_type}={value:.1f}({count}æ¬¡) ", end="")
                    print()
            
            # æ£€æŸ¥ä¼˜åŒ–çš„æ—©åœæ¡ä»¶
            if episode >= min_episodes:
                early_stop_triggered, early_stop_info = self._optimized_early_stopping_monitor(
                    completion_rates, episode, total_env_steps
                )
                
                if early_stop_triggered:
                    print(f"\nâ¹ï¸ ä¼˜åŒ–æ—©åœè§¦å‘: {early_stop_info['reason']}")
                    print(f"   æ”¹è¿›å¹…åº¦: {early_stop_info['improvement']:.4f} < {early_stop_info['required_improvement']:.4f}")
                    print(f"   ç›‘æ§ç‚¹æ•°: {early_stop_info['monitor_points']}")
                    print(f"   ç¯å¢ƒæ­¥æ•°: {early_stop_info['env_steps']}")
                    break
            
            # ä¼ ç»Ÿæ—©åœæ£€æŸ¥
            if episode_reward > best_reward:
                best_reward = episode_reward
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience and episode >= min_episodes * 0.5:
                print(f"\nâ¹ï¸ ä¼ ç»Ÿæ—©åœè§¦å‘äºç¬¬ {episode + 1} è½®")
                break
            
            episode += 1
        
        training_time = time.time() - start_time
        actual_episodes = episode + 1
        
        # ååŒå¥–åŠ±å æ¯”åˆ†æ
        avg_collaboration_ratio = np.mean(self.training_stats['collaboration_ratios'][-actual_episodes:]) if self.training_stats['collaboration_ratios'] else 0
        
        print(f"\n{level_name} ä¼˜åŒ–è®­ç»ƒå®Œæˆ:")
        print(f"  å®é™…è½®æ¬¡: {actual_episodes}")
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        print(f"  æ€»ç¯å¢ƒæ­¥æ•°: {total_env_steps}")
        print(f"  å¹³å‡ååŒå¥–åŠ±å æ¯”: {avg_collaboration_ratio:.2%}")
        print(f"  è¾“å…¥éªŒè¯é”™è¯¯: {self.training_stats['input_validation_errors']}")
        
        # ååŒå¥–åŠ±ä¼˜åŒ–å»ºè®®
        if avg_collaboration_ratio > self.reward_tracking['collaboration_threshold']:
            print(f"âš ï¸ ååŒå¥–åŠ±å æ¯”è¿‡é«˜ ({avg_collaboration_ratio:.2%} > {self.reward_tracking['collaboration_threshold']:.2%})")
            print(f"   å»ºè®®: é™ä½ååŒå¥–åŠ±æƒé‡æˆ–å¢åŠ å…¶ä»–å¥–åŠ±ç±»å‹çš„æƒé‡")
        
        performance = {
            'episodes': actual_episodes,
            'training_time': training_time,
            'total_env_steps': total_env_steps,
            'total_successes': len(success_episodes),
            'final_success_rate': len(success_episodes) / actual_episodes,
            'final_avg_reward': np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards),
            'final_completion_rate': np.mean(completion_rates[-50:]) if len(completion_rates) >= 50 else np.mean(completion_rates),
            'episode_rewards': episode_rewards,
            'completion_rates': completion_rates,
            'losses': losses,
            'exploration_rates': exploration_rates,
            'best_reward': best_reward,
            'early_stop_info': early_stop_info,
            'collaboration_analysis': {
                'avg_ratio': avg_collaboration_ratio,
                'ratios_history': self.training_stats['collaboration_ratios'][-actual_episodes:],
                'threshold_exceeded': avg_collaboration_ratio > self.reward_tracking['collaboration_threshold']
            },
            'input_validation': {
                'errors': self.training_stats['input_validation_errors'],
                'normalization_enabled': self.normalization_config['enable_input_validation']
            }
        }
        
        return performance
    
    def train_optimized_curriculum(self, base_episodes_per_level=200, success_threshold=0.8):
        """æ‰§è¡Œä¼˜åŒ–çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        print("=" * 80)
        print("å¼€å§‹ä¼˜åŒ–ç‰ˆé«˜çº§è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
        print("=" * 80)
        
        set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
        curriculum_scenarios = get_curriculum_scenarios()
        
        solver = None
        previous_model_path = None
        
        for level_idx, (scenario_func, level_name, description) in enumerate(curriculum_scenarios):
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒçº§åˆ« {level_idx + 1}: {level_name}")
            print(f"åœºæ™¯æè¿°: {description}")
            print(f"{'='*60}")
            
            # è·å–åœºæ™¯
            uavs, targets, obstacles = scenario_func(self.config.OBSTACLE_TOLERANCE)
            
            # åˆ›å»ºæ–°æ±‚è§£å™¨
            solver = self._create_optimized_solver(uavs, targets, obstacles, level_idx, level_name, previous_model_path)
            
            # åŠ¨æ€ç¡®å®šè®­ç»ƒè½®æ¬¡
            dynamic_episodes = self._determine_dynamic_episodes(level_idx, level_name, base_episodes_per_level)
            
            # è·å–æ—©åœé…ç½®
            early_stopping_config = self._get_flexible_early_stopping_config(level_name)
            
            # è®­ç»ƒå½“å‰çº§åˆ«
            level_performance = self._train_level_optimized(
                solver, level_name, dynamic_episodes, success_threshold, early_stopping_config
            )
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(self.output_dir, f"model_level_{level_idx + 1}_{level_name}.pth")
            torch.save(solver.policy_net.state_dict(), model_path)
            previous_model_path = model_path
            
            print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        print(f"\n{'='*80}")
        print("ä¼˜åŒ–ç‰ˆè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        print(f"è¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {self.output_dir}")
        print(f"{'='*80}")
        
        return solver
    
    def _create_optimized_solver(self, uavs, targets, obstacles, level_idx, level_name, previous_model_path):
        """åˆ›å»ºä¼˜åŒ–çš„æ±‚è§£å™¨"""
        print(f"ğŸ”„ åˆ›å»ºä¼˜åŒ–æ±‚è§£å™¨...")
        
        graph = DirectedGraph(uavs, targets, len(uavs[0].resources), obstacles, self.config)
        i_dim = len(uavs) * len(targets) * len(uavs[0].resources)
        h_dim = [256, 128]
        o_dim = len(uavs) * len(targets) * len(uavs[0].resources)
        
        solver = GraphRLSolver(
            uavs=uavs, targets=targets, graph=graph, obstacles=obstacles,
            i_dim=i_dim, h_dim=h_dim, o_dim=o_dim, config=self.config,
            obs_mode="graph", network_type="ZeroShotGNN"
        )
        
        # æƒé‡è¿ç§»
        if previous_model_path and os.path.exists(previous_model_path):
            try:
                previous_state_dict = torch.load(previous_model_path, map_location=solver.device)
                current_state_dict = solver.policy_net.state_dict()
                loaded_keys = []
                
                for key, value in previous_state_dict.items():
                    if key in current_state_dict and current_state_dict[key].shape == value.shape:
                        current_state_dict[key] = value
                        loaded_keys.append(key)
                
                solver.policy_net.load_state_dict(current_state_dict)
                solver.target_net.load_state_dict(current_state_dict)
                
                print(f"   âœ… æˆåŠŸåŠ è½½ {len(loaded_keys)} ä¸ªæƒé‡")
                
            except Exception as e:
                print(f"   âš ï¸ æƒé‡åŠ è½½å¤±è´¥: {e}")
        
        print(f"   æ±‚è§£å™¨åˆ›å»ºå®Œæˆ (åŠ¨ä½œç©ºé—´: {o_dim})")
        return solver
    
    def _determine_dynamic_episodes(self, level_idx, level_name, base_episodes):
        """åŠ¨æ€ç¡®å®šè®­ç»ƒè½®æ¬¡"""
        difficulty_multipliers = {
            'easy': 0.7, 'simple': 0.8, 'medium': 1.0, 'hard': 1.3, 'expert': 1.5
        }
        
        difficulty = 'medium'
        for diff_key in difficulty_multipliers.keys():
            if diff_key.lower() in level_name.lower():
                difficulty = diff_key
                break
        
        multiplier = difficulty_multipliers[difficulty]
        
        min_episodes = max(self.dynamic_config['min_episodes_per_level'], int(base_episodes * multiplier * 0.5))
        max_episodes = min(self.dynamic_config['max_episodes_per_level'], int(base_episodes * multiplier * 1.5))
        
        return {
            'min': min_episodes,
            'max': max_episodes,
            'base': base_episodes,
            'difficulty': difficulty,
            'multiplier': multiplier
        }
    
    def _get_flexible_early_stopping_config(self, level_name):
        """è·å–çµæ´»æ—©åœé…ç½®"""
        difficulty = 'medium'
        for diff_key in self.early_stopping_config['patience_multiplier'].keys():
            if diff_key.lower() in level_name.lower():
                difficulty = diff_key
                break
        
        multiplier = self.early_stopping_config['patience_multiplier'][difficulty]
        patience = int(self.early_stopping_config['base_patience'] * multiplier)
        
        return {
            'patience': patience,
            'difficulty': difficulty,
            'multiplier': multiplier,
            'min_improvement': self.early_stopping_config['min_improvement']
        }


def main():
    """ä¸»å‡½æ•°"""
    print("ä¼˜åŒ–ç‰ˆé«˜çº§è¯¾ç¨‹å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    config = Config()
    trainer = OptimizedAdvancedCurriculumTrainer(config)
    
    final_solver = trainer.train_optimized_curriculum(
        base_episodes_per_level=200,
        success_threshold=0.8
    )
    
    print("ä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆï¼")
    return final_solver


if __name__ == "__main__":
    main()
