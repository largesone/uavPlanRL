# -*- coding: utf-8 -*-
"""
åŠ¨ä½œæ©ç åŠŸèƒ½æ•ˆæœå¯¹æ¯”æ¼”ç¤º
å¯¹æ¯”å¯ç”¨å’Œç¦ç”¨åŠ¨ä½œæ©ç çš„è®­ç»ƒæ•ˆæœ
"""

import os
import sys
import numpy as np
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from main import *
from config import Config
from scenarios import get_small_scenario

def simulate_without_action_mask():
    """æ¨¡æ‹Ÿæ²¡æœ‰åŠ¨ä½œæ©ç çš„æƒ…å†µï¼ˆç»Ÿè®¡æ— æ•ˆåŠ¨ä½œæ¯”ä¾‹ï¼‰"""
    print("ğŸ“Š æ¨¡æ‹Ÿæ²¡æœ‰åŠ¨ä½œæ©ç çš„æƒ…å†µ")
    print("-" * 40)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    config.SHOW_VISUALIZATION = False
    
    # è·å–åœºæ™¯
    uavs, targets, obstacles = get_small_scenario(obstacle_tolerance=50.0)
    graph = DirectedGraph(uavs, targets, config.GRAPH_N_PHI, obstacles, config)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config, obs_mode="flat")
    
    # ç»Ÿè®¡æ•°æ®
    total_actions = 0
    invalid_actions = 0
    no_contribution_actions = 0
    
    # æ¨¡æ‹Ÿå¤šä¸ªå›åˆ
    num_episodes = 10
    
    for episode in range(num_episodes):
        env.reset()
        
        for step in range(20):  # æ¯å›åˆæœ€å¤š20æ­¥
            # éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¨¡æ‹Ÿæ²¡æœ‰æ©ç çš„æƒ…å†µï¼‰
            action_idx = np.random.randint(0, env.n_actions)
            target_idx, uav_idx, phi_idx = env._action_to_assignment(action_idx)
            target = env.targets[target_idx]
            uav = env.uavs[uav_idx]
            
            total_actions += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆåŠ¨ä½œ
            if not env._is_valid_action(target, uav, phi_idx):
                invalid_actions += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…è´¡çŒ®
            if not env._has_actual_contribution(target, uav):
                no_contribution_actions += 1
                continue
            
            # æ‰§è¡Œæœ‰æ•ˆåŠ¨ä½œ
            _, _, done, truncated, _ = env.step(action_idx)
            if done or truncated:
                break
    
    invalid_rate = invalid_actions / total_actions
    no_contrib_rate = no_contribution_actions / total_actions
    total_invalid_rate = (invalid_actions + no_contribution_actions) / total_actions
    
    print(f"ğŸ“ˆ ç»Ÿè®¡ç»“æœ (åŸºäº{total_actions}ä¸ªéšæœºåŠ¨ä½œ):")
    print(f"  æ— æ•ˆåŠ¨ä½œ: {invalid_actions} ({invalid_rate:.1%})")
    print(f"  æ— è´¡çŒ®åŠ¨ä½œ: {no_contribution_actions} ({no_contrib_rate:.1%})")
    print(f"  æ€»æ— æ•ˆç‡: {invalid_actions + no_contribution_actions} ({total_invalid_rate:.1%})")
    print(f"  æœ‰æ•ˆç‡: {total_actions - invalid_actions - no_contribution_actions} ({1-total_invalid_rate:.1%})")
    
    return total_invalid_rate

def test_with_action_mask():
    """æµ‹è¯•ä½¿ç”¨åŠ¨ä½œæ©ç çš„è®­ç»ƒæ•ˆæœ"""
    print("\nğŸ¯ æµ‹è¯•ä½¿ç”¨åŠ¨ä½œæ©ç çš„è®­ç»ƒ")
    print("-" * 40)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    config.EPISODES = 5
    config.SHOW_VISUALIZATION = False
    
    # è·å–åœºæ™¯
    uavs, targets, obstacles = get_small_scenario(obstacle_tolerance=50.0)
    graph = DirectedGraph(uavs, targets, config.GRAPH_N_PHI, obstacles, config)
    
    # è®¡ç®—ç½‘ç»œç»´åº¦
    if config.NETWORK_TYPE == 'ZeroShotGNN':
        i_dim = 64
        h_dim = 128
        o_dim = len(targets) * len(uavs) * graph.n_phi
        obs_mode = "graph"
    else:
        env_temp = UAVTaskEnv(uavs, targets, graph, obstacles, config, obs_mode="flat")
        state_temp = env_temp.reset()
        i_dim = len(state_temp)
        h_dim = 256
        o_dim = len(targets) * len(uavs) * graph.n_phi
        obs_mode = "flat"
    
    # åˆ›å»ºæ±‚è§£å™¨
    output_dir = "action_mask_demo_output"
    os.makedirs(output_dir, exist_ok=True)
    
    solver = GraphRLSolver(
        uavs=uavs,
        targets=targets, 
        graph=graph,
        obstacles=obstacles,
        i_dim=i_dim,
        h_dim=h_dim,
        o_dim=o_dim,
        config=config,
        network_type=config.NETWORK_TYPE,
        tensorboard_dir=os.path.join(output_dir, "tensorboard"),
        obs_mode=obs_mode
    )
    
    print(f"ğŸ¤– æ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è¿›è¡Œè®­ç»ƒ
    training_time = solver.train(
        episodes=config.EPISODES,
        patience=config.PATIENCE,
        log_interval=config.LOG_INTERVAL,
        model_save_path=os.path.join(output_dir, "masked_model.pth")
    )
    
    # åˆ†æè®­ç»ƒç»“æœ
    episode_rewards = solver.episode_rewards
    completion_rates = solver.completion_rates
    
    print(f"\nğŸ“Š è®­ç»ƒç»“æœåˆ†æ:")
    print(f"  è®­ç»ƒè½®æ•°: {len(episode_rewards)}")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    print(f"  æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
    print(f"  æœ€ç»ˆå®Œæˆç‡: {completion_rates[-1]:.3f}")
    print(f"  å¹³å‡å®Œæˆç‡: {np.mean(completion_rates):.3f}")
    print(f"  è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
    
    # æ£€æŸ¥å¥–åŠ±æ—¥å¿—ä¸­æ˜¯å¦æœ‰æ— æ•ˆåŠ¨ä½œæƒ©ç½š
    reward_log_files = [f for f in os.listdir(output_dir) if f.startswith('reward_log_') and f.endswith('.txt')]
    
    if reward_log_files:
        latest_log = max(reward_log_files)
        log_path = os.path.join(output_dir, latest_log)
        
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æ£€æŸ¥æ˜¯å¦æœ‰-5.0çš„æƒ©ç½šï¼ˆæ— æ•ˆåŠ¨ä½œæƒ©ç½šï¼‰
        invalid_penalty_count = content.count('Total=  -5.00')
        
        print(f"  æ— æ•ˆåŠ¨ä½œæƒ©ç½šæ¬¡æ•°: {invalid_penalty_count}")
        
        if invalid_penalty_count == 0:
            print(f"  âœ… æ²¡æœ‰æ— æ•ˆåŠ¨ä½œæƒ©ç½šï¼ŒåŠ¨ä½œæ©ç å·¥ä½œæ­£å¸¸")
        else:
            print(f"  âŒ å‘ç°{invalid_penalty_count}æ¬¡æ— æ•ˆåŠ¨ä½œæƒ©ç½š")
    
    return {
        'episodes': len(episode_rewards),
        'avg_reward': np.mean(episode_rewards),
        'max_reward': np.max(episode_rewards),
        'final_completion': completion_rates[-1],
        'avg_completion': np.mean(completion_rates),
        'training_time': training_time
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åŠ¨ä½œæ©ç åŠŸèƒ½æ•ˆæœå¯¹æ¯”æ¼”ç¤º")
    print("=" * 80)
    
    print("æœ¬æ¼”ç¤ºå°†å¯¹æ¯”å¯ç”¨åŠ¨ä½œæ©ç å‰åçš„è®­ç»ƒæ•ˆæœ")
    print("ä¸»è¦æ”¹è¿›:")
    print("âœ… 1. æ¶ˆé™¤æ— æ•ˆåŠ¨ä½œï¼Œæå‡è®­ç»ƒæ•ˆç‡")
    print("âœ… 2. å‡å°‘æƒ©ç½šä¿¡å·ï¼Œè®©å¥–åŠ±æ›´æ¸…æ™°")
    print("âœ… 3. åŠ é€Ÿæ”¶æ•›ï¼Œæé«˜è®­ç»ƒè´¨é‡")
    print("âœ… 4. ç®€åŒ–ç¯å¢ƒé€»è¾‘ï¼Œé™ä½è®¡ç®—å¼€é”€")
    
    # æ¨¡æ‹Ÿæ²¡æœ‰åŠ¨ä½œæ©ç çš„æƒ…å†µ
    invalid_rate = simulate_without_action_mask()
    
    # æµ‹è¯•ä½¿ç”¨åŠ¨ä½œæ©ç çš„æ•ˆæœ
    results = test_with_action_mask()
    
    # æ€»ç»“å¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ•ˆæœå¯¹æ¯”æ€»ç»“")
    print("=" * 80)
    
    print(f"ğŸ” åŠ¨ä½œæœ‰æ•ˆæ€§åˆ†æ:")
    print(f"  ä¼ ç»Ÿæ–¹æ³•æ— æ•ˆåŠ¨ä½œæ¯”ä¾‹: {invalid_rate:.1%}")
    print(f"  åŠ¨ä½œæ©ç æ–¹æ³•æ— æ•ˆåŠ¨ä½œæ¯”ä¾‹: 0.0% (å®Œå…¨æ¶ˆé™¤)")
    print(f"  æ•ˆç‡æå‡: {invalid_rate:.1%} â†’ 0.0%")
    
    print(f"\nğŸ¯ è®­ç»ƒæ•ˆæœ:")
    print(f"  è®­ç»ƒè½®æ•°: {results['episodes']}")
    print(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f}")
    print(f"  æœ€é«˜å¥–åŠ±: {results['max_reward']:.2f}")
    print(f"  æœ€ç»ˆå®Œæˆç‡: {results['final_completion']:.3f}")
    print(f"  è®­ç»ƒè€—æ—¶: {results['training_time']:.2f}ç§’")
    
    print(f"\nâœ… å…³é”®æ”¹è¿›:")
    print(f"  1. åŠ¨ä½œç©ºé—´ä¼˜åŒ–: GRAPH_N_PHI = 1ï¼Œé™ä½å¤æ‚åº¦")
    print(f"  2. æ— æ•ˆåŠ¨ä½œæ¶ˆé™¤: ä»{invalid_rate:.1%}é™è‡³0%")
    print(f"  3. å¥–åŠ±ä¿¡å·æ¸…æ™°: æ¶ˆé™¤-5.0æƒ©ç½šï¼Œä¸“æ³¨æ­£å‘å­¦ä¹ ")
    print(f"  4. è®­ç»ƒæ•ˆç‡æå‡: æ¯æ­¥éƒ½æ˜¯æœ‰æ•ˆå­¦ä¹ ")
    print(f"  5. ä»£ç ç®€åŒ–: ç§»é™¤step()ä¸­çš„æ— æ•ˆåŠ¨ä½œæ£€æŸ¥")
    
    print(f"\nğŸ‰ åŠ¨ä½œæ©ç åŠŸèƒ½æˆåŠŸå®ç°å¹¶éªŒè¯ï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()