#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºè®­ç»ƒä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„è®­ç»ƒåŠŸèƒ½
"""

import os
import sys
from datetime import datetime

from config import Config
from scenarios import get_curriculum_scenarios
from environment import DirectedGraph, UAVTaskEnv
from main import GraphRLSolver
from enhanced_trainer import EnhancedTrainer
from enhanced_training_config import EnhancedTrainingConfig
from baseline_config import BaselineConfig
from model_manager import EnsembleInference

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¢å¼ºè®­ç»ƒç³»ç»Ÿç¤ºä¾‹")
    print("="*60)
    
    # 1. åˆ›å»ºé…ç½®
    config = Config()
    
    # åº”ç”¨åŸºçº¿é…ç½®ï¼ˆå¯é€‰ï¼‰
    # BaselineConfig.apply_to_config(config)
    
    # æˆ–è€…æ‰‹åŠ¨è®¾ç½®å¢å¼ºé…ç½®
    config.NETWORK_TYPE = 'ZeroShotGNN'
    config.ENABLE_PBRS = True
    config.ENABLE_REWARD_DEBUG = False  # è®­ç»ƒæ—¶å…³é—­ä»¥æé«˜æ€§èƒ½
    
    # 2. åˆ›å»ºåœºæ™¯å’Œç¯å¢ƒ
    curriculum_scenarios = get_curriculum_scenarios()
    scenario_func, level_name, description = curriculum_scenarios[0]
    scenario = scenario_func()
    
    if isinstance(scenario, tuple):
        uavs, targets, obstacles = scenario
    else:
        uavs = scenario['uavs']
        targets = scenario['targets']
        obstacles = scenario.get('obstacles', [])
    
    graph = DirectedGraph(uavs, targets, config.GRAPH_N_PHI, obstacles, config)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config, obs_mode="graph")
    
    # 3. åˆ›å»ºæ±‚è§£å™¨
    i_dim = len(uavs) + len(targets)
    h_dim = 128
    o_dim = len(uavs) * len(targets) * len(uavs[0].resources)
    
    solver = GraphRLSolver(
        uavs=uavs, targets=targets, graph=graph, obstacles=obstacles,
        i_dim=i_dim, h_dim=[h_dim], o_dim=o_dim, config=config,
        obs_mode="graph"
    )
    
    # 4. åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"enhanced_training_output_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # 5. åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
    trainer = EnhancedTrainer(solver, config, output_dir)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ¯ è®­ç»ƒåœºæ™¯: {level_name}")
    
    # 6. é€‰æ‹©è®­ç»ƒæ¨¡å¼
    print(f"\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print(f"1. åŸºçº¿è®­ç»ƒ (ç¨³å®šç‰ˆæœ¬)")
    print(f"2. å¢å¼ºè®­ç»ƒ (ä¼˜åŒ–ç‰ˆæœ¬)")
    print(f"3. å¯¹æ¯”è®­ç»ƒ (ä¸¤ç§æ¨¡å¼)")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice == "1":
        # åŸºçº¿è®­ç»ƒ
        print(f"\nğŸ”§ æ‰§è¡ŒåŸºçº¿è®­ç»ƒ...")
        results = trainer.train_enhanced(episodes=500, use_baseline=True)
        
    elif choice == "2":
        # å¢å¼ºè®­ç»ƒ
        print(f"\nğŸš€ æ‰§è¡Œå¢å¼ºè®­ç»ƒ...")
        results = trainer.train_enhanced(episodes=1000, use_baseline=False)
        
        # æ¼”ç¤ºé›†æˆæ¨ç†
        if results['model_saves_count'] > 0:
            print(f"\nğŸ¯ æ¼”ç¤ºé›†æˆæ¨ç†...")
            
            ensemble = EnsembleInference(
                trainer.model_manager,
                type(solver.policy_net),
                temperature=EnhancedTrainingConfig.SOFTMAX_TEMPERATURE
            )
            
            ensemble.load_ensemble_models(
                top_n=min(EnhancedTrainingConfig.ENSEMBLE_SIZE, results['model_saves_count']),
                i_dim=i_dim, h_dim=[h_dim], o_dim=o_dim
            )
            
            # æµ‹è¯•å‡ æ­¥æ¨ç†
            state = env.reset()
            for step in range(5):
                state_tensor = solver._prepare_state_tensor(state)
                action = ensemble.predict(state_tensor, method='weighted_softmax')
                
                next_state, reward, done, truncated, info = env.step(action)
                print(f"  Step {step+1}: Action={action}, Reward={reward:.2f}")
                
                if done or truncated:
                    break
                state = next_state
        
    elif choice == "3":
        # å¯¹æ¯”è®­ç»ƒ
        print(f"\nğŸ“Š æ‰§è¡Œå¯¹æ¯”è®­ç»ƒ...")
        
        print(f"é˜¶æ®µ1: åŸºçº¿è®­ç»ƒ")
        baseline_results = trainer.train_enhanced(episodes=300, use_baseline=True)
        
        # é‡ç½®æ±‚è§£å™¨çŠ¶æ€
        solver.epsilon = EnhancedTrainingConfig.EPSILON_START
        
        print(f"\né˜¶æ®µ2: å¢å¼ºè®­ç»ƒ")
        enhanced_results = trainer.train_enhanced(episodes=500, use_baseline=False)
        
        # å¯¹æ¯”ç»“æœ
        print(f"\nğŸ“ˆ å¯¹æ¯”ç»“æœ:")
        print(f"åŸºçº¿è®­ç»ƒ - æœ€ä½³åˆ†æ•°: {baseline_results['best_score']:.1f}, "
              f"æœ€ç»ˆEpsilon: {baseline_results['final_epsilon']:.6f}")
        print(f"å¢å¼ºè®­ç»ƒ - æœ€ä½³åˆ†æ•°: {enhanced_results['best_score']:.1f}, "
              f"æœ€ç»ˆEpsilon: {enhanced_results['final_epsilon']:.6f}, "
              f"ä¿å­˜æ¨¡å‹: {enhanced_results['model_saves_count']}")
    
    else:
        print(f"æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤å¢å¼ºè®­ç»ƒ")
        results = trainer.train_enhanced(episodes=500, use_baseline=False)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"\nğŸ“‹ ä½¿ç”¨çš„é…ç½®:")
    if hasattr(results, 'config_type'):
        if results['config_type'] == 'baseline':
            print("åŸºçº¿é…ç½®å‚æ•°:")
            baseline_config = BaselineConfig.get_baseline_config()
            for key, value in baseline_config.items():
                print(f"  {key}: {value}")
        else:
            print("å¢å¼ºé…ç½®å‚æ•°:")
            print(f"  EPSILON_START: {EnhancedTrainingConfig.EPSILON_START}")
            print(f"  EPSILON_END: {EnhancedTrainingConfig.EPSILON_END}")
            print(f"  EPSILON_DECAY: {EnhancedTrainingConfig.EPSILON_DECAY}")
            print(f"  SAVE_TOP_N_MODELS: {EnhancedTrainingConfig.SAVE_TOP_N_MODELS}")
            print(f"  ENSEMBLE_SIZE: {EnhancedTrainingConfig.ENSEMBLE_SIZE}")

if __name__ == "__main__":
    main()