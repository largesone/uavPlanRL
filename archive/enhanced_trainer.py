#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºè®­ç»ƒå™¨
å®ç°ä¼˜åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬åŠ¨æ€epsilonè°ƒåº¦å’Œæ¨¡å‹ç®¡ç†
"""

import numpy as np
import torch
import os
import time
from datetime import datetime
import matplotlib.pyplot as plt

from enhanced_training_config import EnhancedTrainingConfig
from model_manager import ModelManager
from baseline_config import BaselineConfig

class EnhancedTrainer:
    """å¢å¼ºè®­ç»ƒå™¨"""
    
    def __init__(self, solver, config, output_dir):
        """
        åˆå§‹åŒ–å¢å¼ºè®­ç»ƒå™¨
        
        Args:
            solver: æ±‚è§£å™¨å¯¹è±¡
            config: é…ç½®å¯¹è±¡
            output_dir: è¾“å‡ºç›®å½•
        """
        self.solver = solver
        self.config = config
        self.output_dir = output_dir
        
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        model_save_dir = os.path.join(output_dir, "saved_models")
        self.model_manager = ModelManager(
            save_dir=model_save_dir,
            max_models=EnhancedTrainingConfig.SAVE_TOP_N_MODELS
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode_rewards': [],
            'episode_scores': [],
            'epsilon_history': [],
            'loss_history': [],
            'model_saves': [],
            'phase_transitions': []
        }
        
        print(f"å¢å¼ºè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir}")
    
    def train_enhanced(self, episodes, use_baseline=False):
        """
        å¢å¼ºè®­ç»ƒä¸»å‡½æ•°
        
        Args:
            episodes: è®­ç»ƒè½®æ¬¡
            use_baseline: æ˜¯å¦ä½¿ç”¨åŸºçº¿é…ç½®
            
        Returns:
            dict: è®­ç»ƒç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ")
        print(f"è®­ç»ƒè½®æ¬¡: {episodes}")
        print(f"ä½¿ç”¨é…ç½®: {'åŸºçº¿é…ç½®' if use_baseline else 'å¢å¼ºé…ç½®'}")
        
        # é€‰æ‹©é…ç½®
        config_class = BaselineConfig if use_baseline else EnhancedTrainingConfig
        
        start_time = time.time()
        best_score = float('-inf')
        
        # è®¡ç®—é˜¶æ®µè½¬æ¢ç‚¹
        exploration_episodes = int(episodes * config_class.EXPLORATION_PHASE_RATIO)
        
        print(f"æ¢ç´¢é˜¶æ®µ: 0-{exploration_episodes} è½®")
        print(f"åˆ©ç”¨é˜¶æ®µ: {exploration_episodes}-{episodes} è½®")
        
        for episode in range(episodes):
            # åŠ¨æ€è°ƒæ•´epsilon
            if not use_baseline:
                epsilon = config_class.get_epsilon_schedule(episode, episodes)
                self.solver.epsilon = epsilon
            else:
                # åŸºçº¿é…ç½®ä½¿ç”¨åŸæœ‰çš„epsilonè¡°å‡
                self.solver.epsilon = max(
                    config_class.EPSILON_END,
                    self.solver.epsilon * config_class.EPSILON_DECAY
                )
            
            # è®°å½•é˜¶æ®µè½¬æ¢
            if episode == exploration_episodes and not use_baseline:
                self.training_stats['phase_transitions'].append({
                    'episode': episode,
                    'phase': 'exploitation',
                    'epsilon': self.solver.epsilon
                })
                print(f"\nğŸ¯ è¿›å…¥åˆ©ç”¨é˜¶æ®µ (Episode {episode})")
                print(f"   å½“å‰Epsilon: {self.solver.epsilon:.6f}")
            
            # æ‰§è¡Œä¸€ä¸ªepisode
            episode_reward, episode_score = self._run_episode(episode)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.training_stats['episode_rewards'].append(episode_reward)
            self.training_stats['episode_scores'].append(episode_score)
            self.training_stats['epsilon_history'].append(self.solver.epsilon)
            
            # è®­ç»ƒç½‘ç»œ
            if len(self.solver.memory) > self.config.BATCH_SIZE:
                loss = self.solver.optimize_model()
                if loss is not None:
                    self.training_stats['loss_history'].append(loss)
            
            # æ›´æ–°æœ€ä½³åˆ†æ•°
            if episode_score > best_score:
                best_score = episode_score
            
            # æ¨¡å‹ä¿å­˜æ£€æŸ¥
            if not use_baseline:  # åªåœ¨å¢å¼ºæ¨¡å¼ä¸‹ä¿å­˜å¤šä¸ªæ¨¡å‹
                if self.model_manager.should_save_model(
                    episode, episode_score,
                    min_episodes=config_class.MIN_EPISODES_FOR_SAVE,
                    save_interval=config_class.MODEL_SAVE_INTERVAL
                ):
                    filepath = self.model_manager.save_model(
                        self.solver.policy_net, episode, episode_score,
                        additional_info={
                            'epsilon': self.solver.epsilon,
                            'total_episodes': episodes,
                            'config_type': 'enhanced'
                        }
                    )
                    self.training_stats['model_saves'].append({
                        'episode': episode,
                        'score': episode_score,
                        'filepath': filepath
                    })
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if (episode + 1) % 100 == 0:
                self._print_progress(episode + 1, episodes, episode_reward, episode_score)
        
        training_time = time.time() - start_time
        
        # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        results = self._generate_training_report(episodes, training_time, use_baseline)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåŸºçº¿æ¨¡å¼ï¼‰
        if use_baseline:
            final_model_path = os.path.join(self.output_dir, "final_baseline_model.pth")
            torch.save({
                'model_state_dict': self.solver.policy_net.state_dict(),
                'episode': episodes,
                'score': best_score,
                'config_type': 'baseline',
                'training_time': training_time
            }, final_model_path)
            print(f"åŸºçº¿æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        return results
    
    def _run_episode(self, episode):
        """
        è¿è¡Œå•ä¸ªepisode
        
        Args:
            episode: å½“å‰è½®æ¬¡
            
        Returns:
            tuple: (episode_reward, episode_score)
        """
        state = self.solver.env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 50
        
        while step_count < max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            state_tensor = self.solver._prepare_state_tensor(state)
            action = self.solver.select_action(state_tensor)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, info = self.solver.env.step(action.item())
            
            episode_reward += reward
            step_count += 1
            
            # å­˜å‚¨ç»éªŒ
            self.solver.memory.push(state, action, reward, next_state, done or truncated)
            
            state = next_state
            
            if done or truncated:
                break
        
        # è®¡ç®—episodeåˆ†æ•°ï¼ˆå¯ä»¥æ˜¯å¥–åŠ±ã€å®Œæˆç‡ç­‰çš„ç»„åˆï¼‰
        episode_score = self._calculate_episode_score(episode_reward, info, step_count)
        
        return episode_reward, episode_score
    
    def _calculate_episode_score(self, reward, info, steps):
        """
        è®¡ç®—episodeåˆ†æ•°
        
        Args:
            reward: æ€»å¥–åŠ±
            info: æœ€åä¸€æ­¥çš„ä¿¡æ¯
            steps: æ­¥æ•°
            
        Returns:
            float: episodeåˆ†æ•°
        """
        # ç»¼åˆè€ƒè™‘å¥–åŠ±ã€æ•ˆç‡ç­‰å› ç´ 
        base_score = reward
        
        # æ•ˆç‡å¥–åŠ±ï¼ˆæ­¥æ•°è¶Šå°‘è¶Šå¥½ï¼‰
        efficiency_bonus = max(0, (50 - steps) * 2)
        
        # æˆåŠŸå®Œæˆå¥–åŠ±
        success_bonus = 100 if info.get('done', False) else 0
        
        total_score = base_score + efficiency_bonus + success_bonus
        return total_score
    
    def _print_progress(self, episode, total_episodes, reward, score):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        recent_rewards = self.training_stats['episode_rewards'][-100:]
        recent_scores = self.training_stats['episode_scores'][-100:]
        
        avg_reward = np.mean(recent_rewards)
        avg_score = np.mean(recent_scores)
        
        print(f"Episode {episode}/{total_episodes}: "
              f"Reward={reward:.1f}, Score={score:.1f}, "
              f"Avg_Reward={avg_reward:.1f}, Avg_Score={avg_score:.1f}, "
              f"Epsilon={self.solver.epsilon:.6f}")
    
    def _generate_training_report(self, episodes, training_time, use_baseline):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        results = {
            'episodes': episodes,
            'training_time': training_time,
            'config_type': 'baseline' if use_baseline else 'enhanced',
            'final_epsilon': self.solver.epsilon,
            'best_score': max(self.training_stats['episode_scores']),
            'avg_final_score': np.mean(self.training_stats['episode_scores'][-100:]),
            'model_saves_count': len(self.training_stats['model_saves']),
            'training_stats': self.training_stats
        }
        
        print(f"\nğŸ“Š è®­ç»ƒå®ŒæˆæŠ¥å‘Š:")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        print(f"   æœ€ä½³åˆ†æ•°: {results['best_score']:.1f}")
        print(f"   æœ€ç»ˆ100è½®å¹³å‡åˆ†æ•°: {results['avg_final_score']:.1f}")
        print(f"   æœ€ç»ˆEpsilon: {results['final_epsilon']:.6f}")
        
        if not use_baseline:
            print(f"   ä¿å­˜çš„æ¨¡å‹æ•°é‡: {results['model_saves_count']}")
            print(self.model_manager.get_model_summary())
        
        # ä¿å­˜è®­ç»ƒæ›²çº¿
        self._save_training_curves(use_baseline)
        
        return results
    
    def _save_training_curves(self, use_baseline):
        """ä¿å­˜è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.training_stats['episode_rewards'])
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        
        # åˆ†æ•°æ›²çº¿
        axes[0, 1].plot(self.training_stats['episode_scores'])
        axes[0, 1].set_title('Episode Scores')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Score')
        
        # Epsilonæ›²çº¿
        axes[1, 0].plot(self.training_stats['epsilon_history'])
        axes[1, 0].set_title('Epsilon Decay')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Epsilon')
        axes[1, 0].set_yscale('log')
        
        # æŸå¤±æ›²çº¿
        if self.training_stats['loss_history']:
            axes[1, 1].plot(self.training_stats['loss_history'])
            axes[1, 1].set_title('Training Loss')
            axes[1, 1].set_xlabel('Training Step')
            axes[1, 1].set_ylabel('Loss')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        config_type = 'baseline' if use_baseline else 'enhanced'
        curve_path = os.path.join(self.output_dir, f"training_curves_{config_type}.png")
        plt.savefig(curve_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")