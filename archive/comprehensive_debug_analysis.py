#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆè°ƒè¯•åˆ†æè„šæœ¬ - ä¸“é—¨æ£€æŸ¥å¥–åŠ±æ”¶æ•›å’Œèµ„æºåˆ†é…é—®é¢˜
é‡ç‚¹åˆ†æï¼šæ— äººæœºæœ‰èµ„æºã€ç›®æ ‡æœ‰éœ€æ±‚ï¼Œä½†æœªå‚ä¸ä»»åŠ¡åˆ†é…çš„æƒ…å†µ
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
import pickle
import os
from collections import defaultdict, deque
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import seaborn as sns
from pathlib import Path

# å¯¼å…¥å­—ä½“è®¾ç½®å‡½æ•°
from main import set_chinese_font

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…å­—ä½“è­¦å‘Š
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# æŠ‘åˆ¶matplotlibå­—ä½“è­¦å‘Š
import logging
import warnings
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# å½»åº•è§£å†³å­—ä½“è­¦å‘Šé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # å…³é”®ï¼šç¦ç”¨Unicodeå‡å·
plt.rcParams['font.family'] = 'sans-serif'

# è°ƒç”¨å­—ä½“è®¾ç½®å‡½æ•°
set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')

class ComprehensiveDebugAnalyzer:
    """ç»¼åˆè°ƒè¯•åˆ†æå™¨ - æ·±åº¦åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§é—®é¢˜"""
    
    def __init__(self, output_dir="debug_analysis_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.episode_data = []
        self.step_data = []
        self.resource_allocation_data = []
        self.reward_breakdown_data = []
        self.convergence_data = []
        
        # é—®é¢˜æ£€æµ‹è®¡æ•°å™¨
        self.issue_counters = {
            'unused_resources': 0,
            'unmet_demands': 0,
            'inefficient_allocation': 0,
            'reward_anomalies': 0,
            'convergence_issues': 0
        }
        
        # åˆ†æç»“æœå­˜å‚¨
        self.analysis_results = {}
        
    def log_episode_data(self, episode: int, episode_info: Dict[str, Any]):
        """è®°å½•æ¯ä¸ªepisodeçš„è¯¦ç»†æ•°æ®"""
        episode_record = {
            'episode': episode,
            'timestamp': datetime.now().isoformat(),
            'total_reward': episode_info.get('total_reward', 0),
            'completion_rate': episode_info.get('completion_rate', 0),
            'resource_utilization': episode_info.get('resource_utilization', 0),
            'num_actions': episode_info.get('num_actions', 0),
            'invalid_actions': episode_info.get('invalid_actions', 0),
            'final_success': episode_info.get('final_success', False),
            'synergy_attacks': episode_info.get('synergy_attacks', 0),
            'exploration_rate': episode_info.get('exploration_rate', 0),
            'learning_rate': episode_info.get('learning_rate', 0),
            'loss': episode_info.get('loss', 0)
        }
        
        self.episode_data.append(episode_record)
        
        # å®æ—¶é—®é¢˜æ£€æµ‹
        self._detect_episode_issues(episode_record)
    
    def log_step_data(self, episode: int, step: int, step_info: Dict[str, Any]):
        """è®°å½•æ¯ä¸ªstepçš„è¯¦ç»†æ•°æ®"""
        step_record = {
            'episode': episode,
            'step': step,
            'action': step_info.get('action', None),
            'reward': step_info.get('reward', 0),
            'state_before': step_info.get('state_before', None),
            'state_after': step_info.get('state_after', None),
            'q_values': step_info.get('q_values', None),
            'target_id': step_info.get('target_id', None),
            'uav_id': step_info.get('uav_id', None),
            'contribution': step_info.get('contribution', 0),
            'path_length': step_info.get('path_length', 0),
            'is_valid': step_info.get('is_valid', True)
        }
        
        self.step_data.append(step_record)
    
    def log_resource_allocation(self, episode: int, uavs: List, targets: List):
        """è®°å½•èµ„æºåˆ†é…çŠ¶æ€"""
        allocation_record = {
            'episode': episode,
            'timestamp': datetime.now().isoformat(),
            'uav_states': [],
            'target_states': [],
            'allocation_matrix': [],
            'efficiency_metrics': {}
        }
        
        # è®°å½•UAVçŠ¶æ€
        for uav in uavs:
            uav_state = {
                'id': uav.id,
                'position': list(uav.current_position),
                'resources': list(uav.resources),
                'max_resources': getattr(uav, 'initial_resources', list(uav.resources)),
                'is_active': np.any(uav.resources > 0),
                'task_count': len(getattr(uav, 'task_sequence', [])),
                'utilization_rate': self._calculate_uav_utilization(uav)
            }
            allocation_record['uav_states'].append(uav_state)
        
        # è®°å½•ç›®æ ‡çŠ¶æ€
        for target in targets:
            target_state = {
                'id': target.id,
                'position': list(target.position),
                'resources': list(target.resources),
                'remaining_resources': list(target.remaining_resources),
                'allocated_uavs': list(getattr(target, 'allocated_uavs', [])),
                'is_satisfied': np.all(target.remaining_resources <= 0),
                'satisfaction_rate': self._calculate_target_satisfaction(target)
            }
            allocation_record['target_states'].append(target_state)
        
        # è®¡ç®—åˆ†é…çŸ©é˜µ
        allocation_record['allocation_matrix'] = self._build_allocation_matrix(uavs, targets)
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        allocation_record['efficiency_metrics'] = self._calculate_efficiency_metrics(uavs, targets)
        
        self.resource_allocation_data.append(allocation_record)
        
        # æ£€æµ‹èµ„æºåˆ†é…é—®é¢˜
        self._detect_allocation_issues(allocation_record)
    
    def _calculate_uav_utilization(self, uav) -> float:
        """è®¡ç®—UAVèµ„æºåˆ©ç”¨ç‡"""
        initial_resources = getattr(uav, 'initial_resources', uav.resources)
        if np.sum(initial_resources) == 0:
            return 0.0
        
        used_resources = np.array(initial_resources) - np.array(uav.resources)
        return np.sum(used_resources) / np.sum(initial_resources)
    
    def _calculate_target_satisfaction(self, target) -> float:
        """è®¡ç®—ç›®æ ‡æ»¡è¶³ç‡"""
        if np.sum(target.resources) == 0:
            return 1.0
        
        satisfied_resources = np.array(target.resources) - np.array(target.remaining_resources)
        return np.sum(satisfied_resources) / np.sum(target.resources)
    
    def _build_allocation_matrix(self, uavs: List, targets: List) -> List[List[float]]:
        """æ„å»ºåˆ†é…çŸ©é˜µ"""
        matrix = []
        for uav in uavs:
            uav_row = []
            for target in targets:
                # æ£€æŸ¥UAVæ˜¯å¦åˆ†é…ç»™äº†è¿™ä¸ªç›®æ ‡
                allocation_score = 0.0
                if hasattr(target, 'allocated_uavs'):
                    for uav_id, phi_idx in target.allocated_uavs:
                        if uav_id == uav.id:
                            allocation_score = 1.0
                            break
                uav_row.append(allocation_score)
            matrix.append(uav_row)
        return matrix
    
    def _calculate_efficiency_metrics(self, uavs: List, targets: List) -> Dict[str, float]:
        """è®¡ç®—æ•ˆç‡æŒ‡æ ‡"""
        metrics = {}
        
        # æ€»ä½“èµ„æºåˆ©ç”¨ç‡
        total_initial_resources = sum(np.sum(getattr(uav, 'initial_resources', uav.resources)) for uav in uavs)
        total_remaining_resources = sum(np.sum(uav.resources) for uav in uavs)
        metrics['overall_utilization'] = (total_initial_resources - total_remaining_resources) / max(total_initial_resources, 1e-6)
        
        # ç›®æ ‡å®Œæˆç‡
        completed_targets = sum(1 for target in targets if np.all(target.remaining_resources <= 0))
        metrics['completion_rate'] = completed_targets / len(targets)
        
        # èµ„æºåŒ¹é…æ•ˆç‡
        total_demand = sum(np.sum(target.remaining_resources) for target in targets)
        total_supply = sum(np.sum(uav.resources) for uav in uavs)
        metrics['supply_demand_ratio'] = total_supply / max(total_demand, 1e-6)
        
        # è´Ÿè½½å‡è¡¡åº¦
        uav_utilizations = [self._calculate_uav_utilization(uav) for uav in uavs]
        if len(uav_utilizations) > 1:
            metrics['load_balance'] = 1.0 - np.std(uav_utilizations) / max(np.mean(uav_utilizations), 1e-6)
        else:
            metrics['load_balance'] = 1.0
        
        return metrics
    
    def _detect_episode_issues(self, episode_record: Dict[str, Any]):
        """æ£€æµ‹episodeçº§åˆ«çš„é—®é¢˜"""
        # æ£€æµ‹å¥–åŠ±å¼‚å¸¸
        if abs(episode_record['total_reward']) > 10000 or np.isnan(episode_record['total_reward']):
            self.issue_counters['reward_anomalies'] += 1
        
        # æ£€æµ‹æ”¶æ•›é—®é¢˜
        if len(self.episode_data) > 50:
            recent_rewards = [ep['total_reward'] for ep in self.episode_data[-50:]]
            if np.std(recent_rewards) > np.mean(recent_rewards) * 2:
                self.issue_counters['convergence_issues'] += 1
    
    def _detect_allocation_issues(self, allocation_record: Dict[str, Any]):
        """æ£€æµ‹èµ„æºåˆ†é…é—®é¢˜"""
        # æ£€æµ‹æœªä½¿ç”¨çš„èµ„æº
        active_uavs = [uav for uav in allocation_record['uav_states'] if uav['is_active']]
        unallocated_uavs = [uav for uav in active_uavs if uav['task_count'] == 0]
        
        if unallocated_uavs:
            self.issue_counters['unused_resources'] += len(unallocated_uavs)
        
        # æ£€æµ‹æœªæ»¡è¶³çš„éœ€æ±‚
        unsatisfied_targets = [target for target in allocation_record['target_states'] 
                             if not target['is_satisfied'] and target['satisfaction_rate'] < 0.1]
        
        if unsatisfied_targets and active_uavs:
            self.issue_counters['unmet_demands'] += len(unsatisfied_targets)
        
        # æ£€æµ‹ä½æ•ˆåˆ†é…
        if allocation_record['efficiency_metrics']['overall_utilization'] < 0.3:
            self.issue_counters['inefficient_allocation'] += 1
    
    def analyze_reward_convergence(self) -> Dict[str, Any]:
        """åˆ†æå¥–åŠ±æ”¶æ•›æƒ…å†µ"""
        if not self.episode_data:
            return {"error": "æ²¡æœ‰episodeæ•°æ®"}
        
        rewards = [ep['total_reward'] for ep in self.episode_data]
        episodes = [ep['episode'] for ep in self.episode_data]
        
        analysis = {
            'total_episodes': len(rewards),
            'reward_statistics': {
                'mean': np.mean(rewards),
                'std': np.std(rewards),
                'min': np.min(rewards),
                'max': np.max(rewards),
                'median': np.median(rewards)
            },
            'convergence_metrics': {},
            'trend_analysis': {},
            'stability_analysis': {}
        }
        
        # æ”¶æ•›æ€§åˆ†æ - ä¿®å¤ç‰ˆæœ¬
        if len(rewards) > 10:  # é™ä½é˜ˆå€¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ†æ
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            window_size = min(20, max(5, len(rewards) // 4))  # ç¡®ä¿çª—å£å¤§å°åˆç†
            moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
            
            # è¶‹åŠ¿åˆ†æ - æ€»æ˜¯æ‰§è¡Œ
            if len(moving_avg) > 2:  # åªéœ€è¦è‡³å°‘3ä¸ªç‚¹å°±èƒ½è®¡ç®—è¶‹åŠ¿
                trend_slope = np.polyfit(range(len(moving_avg)), moving_avg, 1)[0]
                analysis['trend_analysis'] = {
                    'slope': trend_slope,
                    'is_improving': trend_slope > 0,
                    'improvement_rate': trend_slope * 100  # æ¯100è½®çš„æ”¹è¿›é‡
                }
            else:
                # å¦‚æœç§»åŠ¨å¹³å‡ç‚¹å¤ªå°‘ï¼Œç›´æ¥ç”¨åŸå§‹æ•°æ®
                trend_slope = np.polyfit(range(len(rewards)), rewards, 1)[0]
                analysis['trend_analysis'] = {
                    'slope': trend_slope,
                    'is_improving': trend_slope > 0,
                    'improvement_rate': trend_slope * 100
                }
            
            # ç¨³å®šæ€§åˆ†æ
            if len(rewards) > 20:
                recent_size = min(50, len(rewards) // 2)
                early_size = min(50, len(rewards) // 2)
                recent_rewards = rewards[-recent_size:]
                early_rewards = rewards[:early_size]
            else:
                # æ•°æ®è¾ƒå°‘æ—¶ï¼Œç®€å•åˆ†ä¸ºå‰åä¸¤åŠ
                mid = len(rewards) // 2
                recent_rewards = rewards[mid:]
                early_rewards = rewards[:mid] if mid > 0 else rewards
            
            analysis['stability_analysis'] = {
                'recent_std': np.std(recent_rewards),
                'early_std': np.std(early_rewards),
                'stability_improvement': np.std(early_rewards) - np.std(recent_rewards),
                'coefficient_of_variation': np.std(recent_rewards) / max(abs(np.mean(recent_rewards)), 1e-6)
            }
        else:
            # æ•°æ®å¤ªå°‘æ—¶çš„é»˜è®¤å€¼
            analysis['trend_analysis'] = {
                'slope': 0.0,
                'is_improving': False,
                'improvement_rate': 0.0
            }
            analysis['stability_analysis'] = {
                'recent_std': np.std(rewards),
                'early_std': np.std(rewards),
                'stability_improvement': 0.0,
                'coefficient_of_variation': np.std(rewards) / max(abs(np.mean(rewards)), 1e-6)
            }
        
        # 1000åˆ†å¥–åŠ±åˆ†æ
        high_reward_episodes = [ep for ep in self.episode_data if ep.get('final_success', False)]
        analysis['high_reward_analysis'] = {
            'count': len(high_reward_episodes),
            'rate': len(high_reward_episodes) / len(self.episode_data),
            'episodes': [ep['episode'] for ep in high_reward_episodes[:10]]  # å‰10ä¸ª
        }
        
        return analysis
    
    def analyze_resource_allocation_patterns(self) -> Dict[str, Any]:
        """åˆ†æèµ„æºåˆ†é…æ¨¡å¼"""
        if not self.resource_allocation_data:
            return {"error": "æ²¡æœ‰èµ„æºåˆ†é…æ•°æ®"}
        
        analysis = {
            'allocation_efficiency': {},
            'resource_utilization_trends': {},
            'problematic_patterns': {},
            'recommendations': []
        }
        
        # æ•ˆç‡è¶‹åŠ¿åˆ†æ
        efficiency_metrics = [record['efficiency_metrics'] for record in self.resource_allocation_data]
        
        if efficiency_metrics:
            for metric_name in efficiency_metrics[0].keys():
                values = [metrics[metric_name] for metrics in efficiency_metrics]
                analysis['allocation_efficiency'][metric_name] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'trend': np.polyfit(range(len(values)), values, 1)[0] if len(values) > 1 else 0,
                    'latest': values[-1] if values else 0
                }
        
        # é—®é¢˜æ¨¡å¼æ£€æµ‹
        unused_resource_episodes = []
        unmet_demand_episodes = []
        
        for record in self.resource_allocation_data:
            episode = record['episode']
            
            # æ£€æµ‹æœ‰èµ„æºä½†æœªåˆ†é…çš„æƒ…å†µ
            active_uavs = [uav for uav in record['uav_states'] if uav['is_active']]
            unallocated_uavs = [uav for uav in active_uavs if uav['task_count'] == 0]
            unsatisfied_targets = [target for target in record['target_states'] 
                                 if not target['is_satisfied']]
            
            if unallocated_uavs and unsatisfied_targets:
                unused_resource_episodes.append({
                    'episode': episode,
                    'unused_uavs': len(unallocated_uavs),
                    'unmet_targets': len(unsatisfied_targets),
                    'efficiency': record['efficiency_metrics']['overall_utilization']
                })
        
        analysis['problematic_patterns'] = {
            'unused_resources_with_unmet_demands': {
                'count': len(unused_resource_episodes),
                'rate': len(unused_resource_episodes) / len(self.resource_allocation_data),
                'examples': unused_resource_episodes[:5]  # å‰5ä¸ªä¾‹å­
            }
        }
        
        # ç”Ÿæˆå»ºè®®
        if len(unused_resource_episodes) > len(self.resource_allocation_data) * 0.3:
            analysis['recommendations'].append("æ£€æµ‹åˆ°å¤§é‡æœªä½¿ç”¨èµ„æºä¸æœªæ»¡è¶³éœ€æ±‚å¹¶å­˜çš„æƒ…å†µï¼Œå»ºè®®ä¼˜åŒ–åŠ¨ä½œé€‰æ‹©ç­–ç•¥")
        
        if analysis['allocation_efficiency'].get('overall_utilization', {}).get('mean', 0) < 0.5:
            analysis['recommendations'].append("æ•´ä½“èµ„æºåˆ©ç”¨ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ èµ„æºè´¡çŒ®å¥–åŠ±æƒé‡")
        
        return analysis
    
    def generate_comprehensive_plots(self):
        """ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨"""
        if not self.episode_data:
            print("æ²¡æœ‰æ•°æ®å¯ä¾›ç»˜å›¾")
            return
        
        # ç¡®ä¿å­—ä½“è®¾ç½®ç”Ÿæ•ˆ
        set_chinese_font(manual_font_path='C:/Windows/Fonts/simhei.ttf')
        
        # åˆ›å»ºå¤§å›¾è¡¨
        fig = plt.figure(figsize=(20, 16))
        
        # 1. å¥–åŠ±æ”¶æ•›æ›²çº¿
        ax1 = plt.subplot(3, 3, 1)
        episodes = [ep['episode'] for ep in self.episode_data]
        rewards = [ep['total_reward'] for ep in self.episode_data]
        
        plt.plot(episodes, rewards, alpha=0.6, linewidth=1, label='åŸå§‹å¥–åŠ±')
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡
        if len(rewards) > 20:
            window = min(50, len(rewards) // 5)
            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
            plt.plot(episodes[window-1:], moving_avg, 'r-', linewidth=2, label=f'{window}è½®ç§»åŠ¨å¹³å‡')
        
        plt.title('å¥–åŠ±æ”¶æ•›æ›²çº¿')
        plt.xlabel('Episode')
        plt.ylabel('æ€»å¥–åŠ±')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. 1000åˆ†å¥–åŠ±è·å¾—æƒ…å†µ
        ax2 = plt.subplot(3, 3, 2)
        final_success_episodes = [ep['episode'] for ep in self.episode_data if ep.get('final_success', False)]
        final_success_rewards = [1000] * len(final_success_episodes)
        
        if final_success_episodes:
            plt.scatter(final_success_episodes, final_success_rewards, color='gold', s=50, alpha=0.8)
            plt.title(f'1000åˆ†å¥–åŠ±è·å¾—æƒ…å†µ (å…±{len(final_success_episodes)}æ¬¡)')
        else:
            plt.text(0.5, 0.5, 'æœªè·å¾—1000åˆ†å¥–åŠ±', ha='center', va='center', transform=ax2.transAxes)
            plt.title('1000åˆ†å¥–åŠ±è·å¾—æƒ…å†µ (0æ¬¡)')
        
        plt.xlabel('Episode')
        plt.ylabel('å¥–åŠ±å€¼')
        plt.grid(True, alpha=0.3)
        
        # 3. å®Œæˆç‡è¶‹åŠ¿
        ax3 = plt.subplot(3, 3, 3)
        completion_rates = [ep.get('completion_rate', 0) for ep in self.episode_data]
        plt.plot(episodes, completion_rates, 'g-', alpha=0.7)
        plt.title('ä»»åŠ¡å®Œæˆç‡è¶‹åŠ¿')
        plt.xlabel('Episode')
        plt.ylabel('å®Œæˆç‡')
        plt.grid(True, alpha=0.3)
        
        # 4. èµ„æºåˆ©ç”¨ç‡åˆ†æ
        if self.resource_allocation_data:
            ax4 = plt.subplot(3, 3, 4)
            allocation_episodes = [record['episode'] for record in self.resource_allocation_data]
            utilization_rates = [record['efficiency_metrics']['overall_utilization'] 
                               for record in self.resource_allocation_data]
            
            plt.plot(allocation_episodes, utilization_rates, 'b-', alpha=0.7)
            plt.title('èµ„æºåˆ©ç”¨ç‡è¶‹åŠ¿')
            plt.xlabel('Episode')
            plt.ylabel('åˆ©ç”¨ç‡')
            plt.grid(True, alpha=0.3)
        
        # 5. æ¢ç´¢ç‡è¡°å‡
        ax5 = plt.subplot(3, 3, 5)
        exploration_rates = [ep.get('exploration_rate', 0) for ep in self.episode_data]
        if any(rate > 0 for rate in exploration_rates):
            plt.plot(episodes, exploration_rates, 'm-', alpha=0.7)
            plt.title('æ¢ç´¢ç‡è¡°å‡')
            plt.xlabel('Episode')
            plt.ylabel('æ¢ç´¢ç‡')
            plt.grid(True, alpha=0.3)
        
        # 6. å­¦ä¹ ç‡å˜åŒ–
        ax6 = plt.subplot(3, 3, 6)
        learning_rates = [ep.get('learning_rate', 0) for ep in self.episode_data]
        if any(rate > 0 for rate in learning_rates):
            plt.semilogy(episodes, learning_rates, 'c-', alpha=0.7)
            plt.title('å­¦ä¹ ç‡å˜åŒ–')
            plt.xlabel('Episode')
            plt.ylabel('å­¦ä¹ ç‡ (log scale)')
            plt.grid(True, alpha=0.3)
        
        # 7. æŸå¤±å‡½æ•°å˜åŒ–
        ax7 = plt.subplot(3, 3, 7)
        losses = [ep.get('loss', 0) for ep in self.episode_data if ep.get('loss', 0) > 0]
        loss_episodes = [ep['episode'] for ep in self.episode_data if ep.get('loss', 0) > 0]
        if losses:
            plt.semilogy(loss_episodes, losses, 'orange', alpha=0.7)
            plt.title('è®­ç»ƒæŸå¤±å˜åŒ–')
            plt.xlabel('Episode')
            plt.ylabel('æŸå¤±å€¼ (log scale)')
            plt.grid(True, alpha=0.3)
        
        # 8. é—®é¢˜ç»Ÿè®¡
        ax8 = plt.subplot(3, 3, 8)
        issue_names = list(self.issue_counters.keys())
        issue_counts = list(self.issue_counters.values())
        
        bars = plt.bar(range(len(issue_names)), issue_counts, alpha=0.7)
        plt.title('æ£€æµ‹åˆ°çš„é—®é¢˜ç»Ÿè®¡')
        plt.xlabel('é—®é¢˜ç±»å‹')
        plt.ylabel('æ¬¡æ•°')
        plt.xticks(range(len(issue_names)), [name.replace('_', '\n') for name in issue_names], rotation=45)
        
        # ä¸ºæ¯ä¸ªæŸ±å­æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, issue_counts):
            if count > 0:
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                        str(count), ha='center', va='bottom')
        
        # 9. ååŒæ”»å‡»ç»Ÿè®¡
        ax9 = plt.subplot(3, 3, 9)
        synergy_attacks = [ep.get('synergy_attacks', 0) for ep in self.episode_data]
        if any(attacks > 0 for attacks in synergy_attacks):
            plt.plot(episodes, synergy_attacks, 'purple', alpha=0.7, marker='o', markersize=3)
            plt.title('ååŒæ”»å‡»æ¬¡æ•°')
            plt.xlabel('Episode')
            plt.ylabel('ååŒæ”»å‡»æ¬¡æ•°')
            plt.grid(True, alpha=0.3)
        else:
            plt.text(0.5, 0.5, 'æ— ååŒæ”»å‡»è®°å½•', ha='center', va='center', transform=ax9.transAxes)
            plt.title('ååŒæ”»å‡»æ¬¡æ•° (0æ¬¡)')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / f"comprehensive_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»¼åˆåˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
        
        return plot_path
    
    def generate_detailed_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ç»¼åˆè°ƒè¯•åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"åˆ†ææ•°æ®: {len(self.episode_data)} episodes, {len(self.step_data)} steps")
        report_lines.append("")
        
        # 1. å¥–åŠ±æ”¶æ•›åˆ†æ
        reward_analysis = self.analyze_reward_convergence()
        report_lines.append("1. å¥–åŠ±æ”¶æ•›åˆ†æ")
        report_lines.append("-" * 40)
        
        if 'error' not in reward_analysis:
            stats = reward_analysis['reward_statistics']
            report_lines.append(f"   æ€»episodeæ•°: {reward_analysis['total_episodes']}")
            report_lines.append(f"   å¥–åŠ±ç»Ÿè®¡: å‡å€¼={stats['mean']:.2f}, æ ‡å‡†å·®={stats['std']:.2f}")
            report_lines.append(f"   å¥–åŠ±èŒƒå›´: [{stats['min']:.2f}, {stats['max']:.2f}]")
            
            high_reward = reward_analysis['high_reward_analysis']
            report_lines.append(f"   1000åˆ†å¥–åŠ±: {high_reward['count']}æ¬¡ ({high_reward['rate']:.1%})")
            
            if 'trend_analysis' in reward_analysis:
                trend = reward_analysis['trend_analysis']
                trend_desc = "ä¸Šå‡" if trend['is_improving'] else "ä¸‹é™"
                report_lines.append(f"   æ”¶æ•›è¶‹åŠ¿: {trend_desc} (æ–œç‡={trend['slope']:.4f})")
            
            if 'stability_analysis' in reward_analysis:
                stability = reward_analysis['stability_analysis']
                report_lines.append(f"   ç¨³å®šæ€§: å˜å¼‚ç³»æ•°={stability['coefficient_of_variation']:.3f}")
        
        report_lines.append("")
        
        # 2. èµ„æºåˆ†é…åˆ†æ
        allocation_analysis = self.analyze_resource_allocation_patterns()
        report_lines.append("2. èµ„æºåˆ†é…åˆ†æ")
        report_lines.append("-" * 40)
        
        if 'error' not in allocation_analysis:
            if 'allocation_efficiency' in allocation_analysis:
                efficiency = allocation_analysis['allocation_efficiency']
                if 'overall_utilization' in efficiency:
                    util = efficiency['overall_utilization']
                    report_lines.append(f"   æ•´ä½“èµ„æºåˆ©ç”¨ç‡: {util['mean']:.3f} Â± {util['std']:.3f}")
                
                if 'completion_rate' in efficiency:
                    comp = efficiency['completion_rate']
                    report_lines.append(f"   ä»»åŠ¡å®Œæˆç‡: {comp['mean']:.3f} Â± {comp['std']:.3f}")
            
            # é—®é¢˜æ¨¡å¼
            if 'problematic_patterns' in allocation_analysis:
                patterns = allocation_analysis['problematic_patterns']
                unused_pattern = patterns.get('unused_resources_with_unmet_demands', {})
                if unused_pattern.get('count', 0) > 0:
                    report_lines.append(f"   âš ï¸  èµ„æºæµªè´¹é—®é¢˜: {unused_pattern['count']}æ¬¡ ({unused_pattern['rate']:.1%})")
                    report_lines.append("      -> å­˜åœ¨æœ‰èµ„æºçš„UAVæœªåˆ†é…ï¼ŒåŒæ—¶æœ‰ç›®æ ‡æœªæ»¡è¶³çš„æƒ…å†µ")
            
            # å»ºè®®
            recommendations = allocation_analysis.get('recommendations', [])
            if recommendations:
                report_lines.append("   å»ºè®®:")
                for rec in recommendations:
                    report_lines.append(f"      â€¢ {rec}")
        
        report_lines.append("")
        
        # 3. é—®é¢˜ç»Ÿè®¡
        report_lines.append("3. é—®é¢˜æ£€æµ‹ç»Ÿè®¡")
        report_lines.append("-" * 40)
        
        total_issues = sum(self.issue_counters.values())
        if total_issues > 0:
            for issue_type, count in self.issue_counters.items():
                if count > 0:
                    percentage = count / len(self.episode_data) * 100 if self.episode_data else 0
                    report_lines.append(f"   {issue_type.replace('_', ' ').title()}: {count}æ¬¡ ({percentage:.1f}%)")
        else:
            report_lines.append("   æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜")
        
        report_lines.append("")
        
        # 4. å…³é”®å‘ç°å’Œå»ºè®®
        report_lines.append("4. å…³é”®å‘ç°å’Œå»ºè®®")
        report_lines.append("-" * 40)
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if 'error' not in reward_analysis:
            high_reward_rate = reward_analysis['high_reward_analysis']['rate']
            if high_reward_rate < 0.1:
                report_lines.append("   ğŸ” å‘ç°: 1000åˆ†å¥–åŠ±è·å¾—ç‡è¿‡ä½")
                report_lines.append("      å»ºè®®: 1) å¢åŠ æˆåŠŸå¥–åŠ±æƒé‡ 2) ä¼˜åŒ–æ¢ç´¢ç­–ç•¥ 3) æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
            
            if reward_analysis['reward_statistics']['std'] > reward_analysis['reward_statistics']['mean']:
                report_lines.append("   ğŸ” å‘ç°: å¥–åŠ±æ–¹å·®è¿‡å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š")
                report_lines.append("      å»ºè®®: 1) å¯ç”¨å¥–åŠ±æ ‡å‡†åŒ– 2) è°ƒæ•´å­¦ä¹ ç‡ 3) å¢åŠ æ¢¯åº¦è£å‰ª")
        
        if self.issue_counters['unused_resources'] > len(self.episode_data) * 0.2:
            report_lines.append("   ğŸ” å‘ç°: å¤§é‡èµ„æºæœªè¢«æœ‰æ•ˆåˆ©ç”¨")
            report_lines.append("      å»ºè®®: 1) ä¼˜åŒ–åŠ¨ä½œé€‰æ‹©ç­–ç•¥ 2) å¢åŠ èµ„æºåˆ©ç”¨å¥–åŠ± 3) æ”¹è¿›çŠ¶æ€è¡¨ç¤º")
        
        report_lines.append("")
        report_lines.append("=" * 80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report_lines)
        report_path = self.output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return report_content
    
    def export_data(self):
        """å¯¼å‡ºæ‰€æœ‰åˆ†ææ•°æ®"""
        export_data = {
            'episode_data': self.episode_data,
            'step_data': self.step_data[-1000:],  # åªä¿å­˜æœ€è¿‘1000æ­¥
            'resource_allocation_data': self.resource_allocation_data,
            'issue_counters': self.issue_counters,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜ä¸ºJSON
        json_path = self.output_dir / f"debug_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ä¸ºpickle
        pickle_path = self.output_dir / f"debug_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        with open(pickle_path, 'wb') as f:
            pickle.dump(export_data, f)
        
        print(f"è°ƒè¯•æ•°æ®å·²å¯¼å‡ºè‡³:")
        print(f"  JSON: {json_path}")
        print(f"  Pickle: {pickle_path}")
        
        return json_path, pickle_path

def integrate_with_training_example():
    """å±•ç¤ºå¦‚ä½•é›†æˆåˆ°è®­ç»ƒå¾ªç¯ä¸­çš„ç¤ºä¾‹"""
    example_code = '''
# åœ¨è®­ç»ƒå¼€å§‹å‰åˆå§‹åŒ–åˆ†æå™¨
analyzer = ComprehensiveDebugAnalyzer("debug_analysis_output")

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    episode_info = {
        'synergy_attacks': 0,
        'invalid_actions': 0,
        'num_actions': 0
    }
    
    while not done:
        action = agent.select_action(state)
        next_state, reward, done, truncated, info = env.step(action)
        
        # è®°å½•stepæ•°æ®
        analyzer.log_step_data(episode, step, {
            'action': action,
            'reward': reward,
            'state_before': state,
            'state_after': next_state,
            'target_id': info.get('target_id'),
            'uav_id': info.get('uav_id'),
            'contribution': info.get('actual_contribution', 0),
            'path_length': info.get('path_length', 0),
            'is_valid': not info.get('invalid_action', False)
        })
        
        episode_reward += reward
        episode_info['num_actions'] += 1
        if info.get('invalid_action'):
            episode_info['invalid_actions'] += 1
        if 'synergy_attacks' in info:
            episode_info['synergy_attacks'] += 1
        
        state = next_state
        step += 1
    
    # è®°å½•episodeæ•°æ®
    episode_info.update({
        'total_reward': episode_reward,
        'completion_rate': calculate_completion_rate(env),
        'resource_utilization': calculate_resource_utilization(env),
        'final_success': episode_reward > 900,
        'exploration_rate': agent.epsilon,
        'learning_rate': agent.optimizer.param_groups[0]['lr'],
        'loss': last_loss
    })
    
    analyzer.log_episode_data(episode, episode_info)
    
    # è®°å½•èµ„æºåˆ†é…çŠ¶æ€
    analyzer.log_resource_allocation(episode, env.uavs, env.targets)
    
    # å®šæœŸç”Ÿæˆåˆ†ææŠ¥å‘Š
    if episode % 100 == 0 and episode > 0:
        analyzer.generate_comprehensive_plots()
        analyzer.generate_detailed_report()
        analyzer.export_data()

# è®­ç»ƒç»“æŸåç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
analyzer.generate_comprehensive_plots()
final_report = analyzer.generate_detailed_report()
analyzer.export_data()

print("\\n" + "="*60)
print("æœ€ç»ˆåˆ†ææŠ¥å‘Š:")
print("="*60)
print(final_report)
'''
    
    return example_code

if __name__ == "__main__":
    print("=" * 60)
    print("ç»¼åˆè°ƒè¯•åˆ†æå™¨")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹åˆ†æå™¨
    analyzer = ComprehensiveDebugAnalyzer()
    
    # ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º
    print("ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º...")
    
    np.random.seed(42)
    for episode in range(200):
        # æ¨¡æ‹Ÿepisodeæ•°æ®
        base_reward = np.random.normal(150, 80)
        
        # æ¨¡æ‹Ÿ1000åˆ†å¥–åŠ±ï¼ˆ10%æ¦‚ç‡ï¼‰
        final_success = False
        if episode > 50 and np.random.random() < 0.1:
            base_reward = 1000
            final_success = True
        
        episode_info = {
            'total_reward': base_reward,
            'completion_rate': np.random.beta(2, 3),
            'resource_utilization': np.random.beta(3, 2),
            'num_actions': np.random.randint(10, 50),
            'invalid_actions': np.random.randint(0, 5),
            'final_success': final_success,
            'synergy_attacks': np.random.randint(0, 3) if final_success else 0,
            'exploration_rate': 0.9 * (0.995 ** episode),
            'learning_rate': 1e-4 * (0.99 ** (episode // 10)),
            'loss': np.random.exponential(0.01)
        }
        
        analyzer.log_episode_data(episode, episode_info)
        
        # æ¨¡æ‹Ÿèµ„æºåˆ†é…æ•°æ®
        if episode % 10 == 0:
            # åˆ›å»ºæ¨¡æ‹Ÿçš„UAVå’Œç›®æ ‡å¯¹è±¡
            class MockUAV:
                def __init__(self, uav_id):
                    self.id = uav_id
                    self.current_position = np.random.rand(2) * 1000
                    self.resources = np.random.rand(2) * 100
                    self.initial_resources = self.resources + np.random.rand(2) * 50
                    self.task_sequence = []
                    if np.random.random() < 0.7:  # 70%æ¦‚ç‡æœ‰ä»»åŠ¡
                        self.task_sequence = list(range(np.random.randint(1, 4)))
            
            class MockTarget:
                def __init__(self, target_id):
                    self.id = target_id
                    self.position = np.random.rand(2) * 1000
                    self.resources = np.random.rand(2) * 100 + 50
                    self.remaining_resources = self.resources * np.random.rand(2)
                    self.allocated_uavs = []
                    if np.random.random() < 0.6:  # 60%æ¦‚ç‡æœ‰åˆ†é…
                        self.allocated_uavs = [(np.random.randint(0, 4), np.random.randint(0, 6))]
            
            mock_uavs = [MockUAV(i) for i in range(4)]
            mock_targets = [MockTarget(i) for i in range(2)]
            
            analyzer.log_resource_allocation(episode, mock_uavs, mock_targets)
    
    # ç”Ÿæˆåˆ†æç»“æœ
    print("\nç”Ÿæˆç»¼åˆåˆ†æ...")
    analyzer.generate_comprehensive_plots()
    report = analyzer.generate_detailed_report()
    analyzer.export_data()
    
    print("\n" + "="*60)
    print("ç¤ºä¾‹åˆ†ææŠ¥å‘Š:")
    print("="*60)
    print(report)
    
    print("\n" + "="*60)
    print("é›†æˆç¤ºä¾‹:")
    print("="*60)
    print(integrate_with_training_example())
