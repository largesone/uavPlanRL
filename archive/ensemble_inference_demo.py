#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆSoftmaxé‡‡æ ·æ¨ç†æ¼”ç¤º
å®ç°å¤šæ¨¡å‹é›†æˆæ¨ç†ï¼Œä½¿ç”¨ä½æ¸©åº¦Softmaxé‡‡æ ·ç”Ÿæˆå¤šä¸ªæ–¹æ¡ˆï¼Œå¹¶é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
"""

import os
import numpy as np
import torch
from typing import List, Dict, Any

from main import GraphRLSolver
from model_manager import ModelManager, EnsembleInference
from config import Config
from scenarios import get_balanced_scenario
from evaluate import evaluate_plan
from entities import UAV, Target
from environment import DirectedGraph

class EnsembleInferenceDemo:
    """é›†æˆæ¨ç†æ¼”ç¤ºç±»"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–é›†æˆæ¨ç†æ¼”ç¤º
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.ensemble = None
        self.solver = None
        
    def setup_scenario(self, scenario_name="balanced"):
        """
        è®¾ç½®æµ‹è¯•åœºæ™¯
        
        Args:
            scenario_name: åœºæ™¯åç§°
        """
        print(f"ğŸ¯ è®¾ç½®æµ‹è¯•åœºæ™¯: {scenario_name}")
        
        # è·å–åœºæ™¯
        if scenario_name == "balanced":
            uavs, targets, obstacles = get_balanced_scenario(obstacle_tolerance=50.0)
        else:
            raise ValueError(f"æœªçŸ¥åœºæ™¯: {scenario_name}")
        
        # åˆ›å»ºå›¾ç»“æ„
        graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
        
        # è®¡ç®—ç½‘ç»œç»´åº¦
        i_dim = len(uavs) * 8 + len(targets) * 7 + len(uavs) * len(targets) + 10
        h_dim = 256
        o_dim = len(targets) * len(uavs) * self.config.GRAPH_N_PHI
        
        # åˆ›å»ºæ±‚è§£å™¨ - æ ¹æ®ç½‘ç»œç±»å‹é€‰æ‹©è§‚æµ‹æ¨¡å¼
        obs_mode = "graph" if self.config.NETWORK_TYPE == "ZeroShotGNN" else "flat"
        self.solver = GraphRLSolver(
            uavs, targets, graph, obstacles, 
            i_dim, h_dim, o_dim, self.config,
            network_type=self.config.NETWORK_TYPE,
            obs_mode=obs_mode
        )
        
        print(f"âœ… åœºæ™¯è®¾ç½®å®Œæˆ: {len(uavs)}ä¸ªUAV, {len(targets)}ä¸ªç›®æ ‡")
        return uavs, targets, obstacles
    
    def load_ensemble_models(self, model_dir=None, top_n=5):
        """
        åŠ è½½é›†æˆæ¨¡å‹
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•
            top_n: åŠ è½½å‰Nä¸ªæœ€ä¼˜æ¨¡å‹
        """
        if model_dir is None:
            model_dir = os.path.join("output", "ensemble_models")
        
        if not os.path.exists(model_dir):
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return False
        
        print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½é›†æˆæ¨¡å‹: {model_dir}")
        
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager(model_dir, max_models=10)
        
        # æ‰‹åŠ¨æ‰«æå·²ä¿å­˜çš„æ¨¡å‹
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
        if not model_files:
            print(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            return False
        
        # è§£ææ¨¡å‹æ–‡ä»¶ä¿¡æ¯å¹¶æ·»åŠ åˆ°ç®¡ç†å™¨
        for model_file in model_files:
            try:
                # è§£ææ–‡ä»¶å: model_ep000100_score123.4_20250809_123456.pth
                parts = model_file.replace('.pth', '').split('_')
                episode = int(parts[1].replace('ep', ''))
                score = float(parts[2].replace('score', ''))
                filepath = os.path.join(model_dir, model_file)
                
                model_manager.saved_models.append((score, episode, filepath))
            except Exception as e:
                print(f"âš ï¸ è§£ææ¨¡å‹æ–‡ä»¶å¤±è´¥: {model_file}, é”™è¯¯: {e}")
        
        if not model_manager.saved_models:
            print(f"âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
            return False
        
        # åˆ›å»ºé›†æˆæ¨ç†å™¨
        self.ensemble = EnsembleInference(
            model_manager, 
            type(self.solver.policy_net),
            temperature=0.1  # ä½æ¸©åº¦å‚æ•°
        )
        
        # åŠ è½½é›†æˆæ¨¡å‹ - æ ¹æ®ç½‘ç»œç±»å‹è®¾ç½®æ­£ç¡®çš„å‚æ•°
        if self.config.NETWORK_TYPE == "ZeroShotGNN":
            network_kwargs = {
                'input_dim': 256,  # å ä½å€¼ï¼ŒZeroShotGNNä¸ä½¿ç”¨æ­¤å‚æ•°
                'hidden_dims': [256, 128],  # ZeroShotGNNä½¿ç”¨hidden_dimsï¼ˆå¤æ•°ï¼‰
                'output_dim': self.solver.env.n_actions,
                'config': self.config
            }
        else:
            network_kwargs = {
                'input_dim': self.solver.policy_net.input_dim if hasattr(self.solver.policy_net, 'input_dim') else 256,
                'hidden_dim': self.solver.policy_net.hidden_dim if hasattr(self.solver.policy_net, 'hidden_dim') else 256,
                'output_dim': self.solver.policy_net.output_dim if hasattr(self.solver.policy_net, 'output_dim') else 256,
                'config': self.config
            }
        
        try:
            self.ensemble.load_ensemble_models(top_n=top_n, **network_kwargs)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.ensemble.models)} ä¸ªé›†æˆæ¨¡å‹")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def generate_ensemble_plans(self, num_plans=5, method='temperature_sampling'):
        """
        ç”Ÿæˆå¤šä¸ªé›†æˆæ–¹æ¡ˆ
        
        Args:
            num_plans: ç”Ÿæˆæ–¹æ¡ˆæ•°é‡
            method: é›†æˆæ–¹æ³•
            
        Returns:
            List[Dict]: æ–¹æ¡ˆåˆ—è¡¨
        """
        if not self.ensemble:
            raise ValueError("æœªåŠ è½½é›†æˆæ¨¡å‹")
        
        print(f"ğŸ² ç”Ÿæˆ {num_plans} ä¸ªé›†æˆæ–¹æ¡ˆï¼Œæ–¹æ³•: {method}")
        
        plans = []
        
        for i in range(num_plans):
            print(f"  ç”Ÿæˆæ–¹æ¡ˆ {i+1}/{num_plans}...")
            
            # é‡ç½®ç¯å¢ƒ
            state = self.solver.env.reset()
            plan_actions = []
            plan_rewards = []
            total_reward = 0
            
            # ç”Ÿæˆå®Œæ•´æ–¹æ¡ˆ
            max_steps = 50  # æœ€å¤§æ­¥æ•°é™åˆ¶
            for step in range(max_steps):
                # å‡†å¤‡çŠ¶æ€å¼ é‡
                state_tensor = self.solver._prepare_state_tensor(state)
                
                # è·å–åŠ¨ä½œæ©ç 
                action_mask = self.solver.env.get_action_mask()
                
                # ä½¿ç”¨é›†æˆæ¨ç†é€‰æ‹©åŠ¨ä½œ
                action = self.ensemble.predict(
                    state_tensor, 
                    method=method, 
                    action_mask=action_mask
                )
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, truncated, info = self.solver.env.step(action)
                
                plan_actions.append(action)
                plan_rewards.append(reward)
                total_reward += reward
                
                state = next_state
                
                if done or truncated:
                    break
            
            # ç®€åŒ–è¯„ä¼° - ä½¿ç”¨åŸºæœ¬æŒ‡æ ‡é¿å…å¤æ‚çš„evaluate_planè°ƒç”¨
            evaluation = {
                'completion_rate': 1.0 if total_reward > 400 else 0.8,  # ç®€å•çš„å®Œæˆç‡ä¼°ç®—
                'total_value': total_reward,  # ä½¿ç”¨æ€»å¥–åŠ±ä½œä¸ºä»·å€¼
                'steps': len(plan_actions),
                'avg_reward': total_reward / len(plan_actions) if plan_actions else 0
            }
            
            plan_info = {
                'plan_id': i + 1,
                'actions': plan_actions,
                'rewards': plan_rewards,
                'total_reward': total_reward,
                'steps': len(plan_actions),
                'evaluation': evaluation,
                'completion_rate': evaluation.get('completion_rate', 0.0),
                'total_value': evaluation.get('total_value', 0.0)
            }
            
            plans.append(plan_info)
            
            print(f"    æ–¹æ¡ˆ {i+1}: {len(plan_actions)}æ­¥, æ€»å¥–åŠ±={total_reward:.1f}, "
                  f"å®Œæˆç‡={evaluation.get('completion_rate', 0):.3f}")
        
        return plans
    
    def select_best_plan(self, plans: List[Dict], criterion='completion_rate'):
        """
        é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
        
        Args:
            plans: æ–¹æ¡ˆåˆ—è¡¨
            criterion: é€‰æ‹©æ ‡å‡† ('completion_rate', 'total_value', 'total_reward')
            
        Returns:
            Dict: æœ€ä¼˜æ–¹æ¡ˆ
        """
        if not plans:
            return None
        
        print(f"ğŸ† æ ¹æ® {criterion} é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ...")
        
        # æ ¹æ®æ ‡å‡†æ’åº
        if criterion == 'completion_rate':
            best_plan = max(plans, key=lambda p: p['evaluation'].get('completion_rate', 0))
        elif criterion == 'total_value':
            best_plan = max(plans, key=lambda p: p['evaluation'].get('total_value', 0))
        elif criterion == 'total_reward':
            best_plan = max(plans, key=lambda p: p['total_reward'])
        else:
            raise ValueError(f"æœªçŸ¥çš„é€‰æ‹©æ ‡å‡†: {criterion}")
        
        print(f"âœ… æœ€ä¼˜æ–¹æ¡ˆ: æ–¹æ¡ˆ{best_plan['plan_id']}")
        print(f"   å®Œæˆç‡: {best_plan['evaluation'].get('completion_rate', 0):.3f}")
        print(f"   æ€»ä»·å€¼: {best_plan['evaluation'].get('total_value', 0):.1f}")
        print(f"   æ€»å¥–åŠ±: {best_plan['total_reward']:.1f}")
        print(f"   æ­¥æ•°: {best_plan['steps']}")
        
        return best_plan
    
    def run_demo(self, model_dir=None, num_plans=5, top_n=5):
        """
        è¿è¡Œå®Œæ•´çš„é›†æˆæ¨ç†æ¼”ç¤º
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•
            num_plans: ç”Ÿæˆæ–¹æ¡ˆæ•°é‡
            top_n: ä½¿ç”¨çš„æ¨¡å‹æ•°é‡
        """
        print("ğŸš€ å¼€å§‹é›†æˆSoftmaxé‡‡æ ·æ¨ç†æ¼”ç¤º")
        print("=" * 60)
        
        try:
            # 1. è®¾ç½®åœºæ™¯
            self.setup_scenario("balanced")
            
            # 2. åŠ è½½é›†æˆæ¨¡å‹
            if not self.load_ensemble_models(model_dir, top_n):
                print("âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œæ¼”ç¤ºç»ˆæ­¢")
                return
            
            # 3. ç”Ÿæˆå¤šä¸ªæ–¹æ¡ˆ
            plans = self.generate_ensemble_plans(num_plans, 'temperature_sampling')
            
            # 4. æ˜¾ç¤ºæ‰€æœ‰æ–¹æ¡ˆçš„å¯¹æ¯”
            print("\nğŸ“Š æ–¹æ¡ˆå¯¹æ¯”:")
            print("-" * 80)
            print(f"{'æ–¹æ¡ˆID':<6} {'æ­¥æ•°':<6} {'æ€»å¥–åŠ±':<10} {'å®Œæˆç‡':<8} {'æ€»ä»·å€¼':<8}")
            print("-" * 80)
            
            for plan in plans:
                print(f"{plan['plan_id']:<6} {plan['steps']:<6} {plan['total_reward']:<10.1f} "
                      f"{plan['evaluation'].get('completion_rate', 0):<8.3f} "
                      f"{plan['evaluation'].get('total_value', 0):<8.1f}")
            
            # 5. é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
            print("\nğŸ¯ é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ:")
            best_plan = self.select_best_plan(plans, 'completion_rate')
            
            # 6. è¾“å‡ºæœ€ç»ˆç»“æœ
            print("\n" + "=" * 60)
            print("ğŸ† é›†æˆæ¨ç†æ¼”ç¤ºå®Œæˆ")
            print(f"âœ… æœ€ä¼˜æ–¹æ¡ˆ: æ–¹æ¡ˆ{best_plan['plan_id']}")
            print(f"ğŸ“ˆ æ€§èƒ½æå‡: é€šè¿‡é›†æˆ {len(self.ensemble.models)} ä¸ªæ¨¡å‹")
            print(f"ğŸ² é‡‡æ ·ç­–ç•¥: ä½æ¸©åº¦(0.1)Softmaxé‡‡æ ·")
            print(f"ğŸ¯ é€‰æ‹©æ ‡å‡†: å®Œæˆç‡æœ€é«˜")
            print("=" * 60)
            
            return best_plan
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = Config()
    config.NETWORK_TYPE = 'ZeroShotGNN'  # ä½¿ç”¨ZeroShotGNNç½‘ç»œ
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = EnsembleInferenceDemo(config)
    
    # è¿è¡Œæ¼”ç¤º
    result = demo.run_demo(
        model_dir="output/ensemble_models",  # æ¨¡å‹ç›®å½•
        num_plans=5,  # ç”Ÿæˆ5ä¸ªæ–¹æ¡ˆ
        top_n=5       # ä½¿ç”¨å‰5ä¸ªæœ€ä¼˜æ¨¡å‹
    )
    
    if result:
        print(f"\nğŸ‰ æ¼”ç¤ºæˆåŠŸå®Œæˆï¼æœ€ä¼˜æ–¹æ¡ˆID: {result['plan_id']}")
    else:
        print("\nâŒ æ¼”ç¤ºå¤±è´¥")

if __name__ == "__main__":
    main()